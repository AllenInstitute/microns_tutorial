@article{the_microns_consortium_functional_2025,
	title = {Functional connectomics spanning multiple areas of mouse visual cortex},
	volume = {640},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-025-08790-w},
	doi = {10.1038/s41586-025-08790-w},
	abstract = {Abstract
            
              Understanding the brain requires understanding neurons’ functional responses to the circuit architecture shaping them. Here we introduce the {MICrONS} functional connectomics dataset with dense calcium imaging of around 75,000 neurons in primary visual cortex ({VISp}) and higher visual areas ({VISrl}, {VISal} and {VISlm}) in an awake mouse that is viewing natural and synthetic stimuli. These data are co-registered with an electron microscopy reconstruction containing more than 200,000 cells and 0.5 billion synapses. Proofreading of a subset of neurons yielded reconstructions that include complete dendritic trees as well the local and inter-areal axonal projections that map up to thousands of cell-to-cell connections per neuron. Released as an open-access resource, this dataset includes the tools for data retrieval and analysis
              1,2
              . Accompanying studies describe its use for comprehensive characterization of cell types
              3–6
              , a synaptic level connectivity diagram of a cortical column
              4
              , and uncovering cell-type-specific inhibitory connectivity that can be linked to gene expression data
              4,7
              . Functionally, we identify new computational principles of how information is integrated across visual space
              8
              , characterize novel types of neuronal invariances
              9
              and bring structure and function together to uncover a general principle for connectivity between excitatory neurons within and across areas
              10,11
              .},
	pages = {435--447},
	number = {8058},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {{The MICrONS Consortium} and Bae, J. Alexander and Baptiste, Mahaly and Baptiste, Maya R. and Bishop, Caitlyn A. and Bodor, Agnes L. and Brittain, Derrick and Brooks, Victoria and Buchanan, JoAnn and Bumbarger, Daniel J. and Castro, Manuel A. and Celii, Brendan and Cobos, Erick and Collman, Forrest and Da Costa, Nuno Maçarico and Danskin, Bethanny and Dorkenwald, Sven and Elabbady, Leila and Fahey, Paul G. and Fliss, Tim and Froudarakis, Emmanouil and Gager, Jay and Gamlin, Clare and Gray-Roncal, William and Halageri, Akhilesh and Hebditch, James and Jia, Zhen and Joyce, Emily and Ellis-Joyce, Justin and Jordan, Chris and Kapner, Daniel and Kemnitz, Nico and Kinn, Sam and Kitchell, Lindsey M. and Koolman, Selden and Kuehner, Kai and Lee, Kisuk and Li, Kai and Lu, Ran and Macrina, Thomas and Mahalingam, Gayathri and Matelsky, Jordan and McReynolds, Sarah and Miranda, Elanine and Mitchell, Eric and Mondal, Shanka Subhra and Moore, Merlin and Mu, Shang and Muhammad, Taliah and Nehoran, Barak and Neace, Erika and Ogedengbe, Oluwaseun and Papadopoulos, Christos and Papadopoulos, Stelios and Patel, Saumil and Vega, Guadalupe Jovita Yasmin Perez and Pitkow, Xaq and Popovych, Sergiy and Ramos, Anthony and Reid, R. Clay and Reimer, Jacob and Rivlin, Patricia K. and Rose, Victoria and Sauter, Zachary M. and Schneider-Mizell, Casey M. and Seung, H. Sebastian and Silverman, Ben and Silversmith, William and Sterling, Amy and Sinz, Fabian H. and Smith, Cameron L. and Swanstrom, Rachael and Suckow, Shelby and Takeno, Marc and Tan, Zheng H. and Tolias, Andreas S. and Torres, Russel and Turner, Nicholas L. and Walker, Edgar Y. and Wang, Tianyu and Wanner, Adrian and Wester, Brock A. and Williams, Grace and Williams, Sarah and Willie, Kyle and Willie, Ryan and Wong, William and Wu, Jingpeng and Xu, Chris and Yang, Runzhe and Yatsenko, Dimitri and Ye, Fei and Yin, Wenjing and Young, Rob and Yu, Szi-chieh and Xenes, Daniel and Zhang, Chi},
	urldate = {2025-04-09},
	date = {2025-04-10},
	langid = {english},
	file = {The MICrONS Consortium et al. - 2025 - Functional connectomics spanning multiple areas of.pdf:C\:\\Users\\bethanny.danskin\\Zotero\\storage\\U4IV5VC2\\The MICrONS Consortium et al. - 2025 - Functional connectomics spanning multiple areas of.pdf:application/pdf},
}

@article{dorkenwald_multi-layered_2023,
	title = {Multi-layered maps of neuropil with segmentation-guided contrastive learning},
	volume = {20},
	issn = {1548-7091, 1548-7105},
	url = {https://www.nature.com/articles/s41592-023-02059-8},
	doi = {10.1038/s41592-023-02059-8},
	abstract = {Abstract
            Maps of the nervous system that identify individual cells along with their type, subcellular components and connectivity have the potential to elucidate fundamental organizational principles of neural circuits. Nanometer-resolution imaging of brain tissue provides the necessary raw data, but inferring cellular and subcellular annotation layers is challenging. We present segmentation-guided contrastive learning of representations ({SegCLR}), a self-supervised machine learning technique that produces representations of cells directly from 3D imagery and segmentations. When applied to volumes of human and mouse cortex, {SegCLR} enables accurate classification of cellular subcompartments and achieves performance equivalent to a supervised approach while requiring 400-fold fewer labeled examples. {SegCLR} also enables inference of cell types from fragments as small as 10 μm, which enhances the utility of volumes in which many neurites are truncated at boundaries. Finally, {SegCLR} enables exploration of layer 5 pyramidal cell subtypes and automated large-scale analysis of synaptic partners in mouse visual cortex.},
	pages = {2011--2020},
	number = {12},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Dorkenwald, Sven and Li, Peter H. and Januszewski, Michał and Berger, Daniel R. and Maitin-Shepard, Jeremy and Bodor, Agnes L. and Collman, Forrest and Schneider-Mizell, Casey M. and Da Costa, Nuno Maçarico and Lichtman, Jeff W. and Jain, Viren},
	urldate = {2023-12-18},
	date = {2023-12},
	langid = {english},
}

@article{dorkenwald_binary_2022,
	title = {Binary and analog variation of synapses between cortical pyramidal neurons},
	volume = {11},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/76120},
	doi = {10.7554/eLife.76120},
	abstract = {Learning from experience depends at least in part on changes in neuronal connections. We present the largest map of connectivity to date between cortical neurons of a defined type (layer 2/3 [L2/3] pyramidal cells in mouse primary visual cortex), which was enabled by automated analysis of serial section electron microscopy images with improved handling of image defects (250 × 140 × 90 μm3 volume). We used the map to identify constraints on the learning algorithms employed by the cortex. Previous cortical studies modeled a continuum of synapse sizes by a log-normal distribution. A continuum is consistent with most neural network models of learning, in which synaptic strength is a continuously graded analog variable. Here, we show that synapse size, when restricted to synapses between L2/3 pyramidal cells, is well modeled by the sum of a binary variable and an analog variable drawn from a log-normal distribution. Two synapses sharing the same presynaptic and postsynaptic cells are known to be correlated in size. We show that the binary variables of the two synapses are highly correlated, while the analog variables are not. Binary variation could be the outcome of a Hebbian or other synaptic plasticity rule depending on activity signals that are relatively uniform across neuronal arbors, while analog variation may be dominated by other influences such as spontaneous dynamical fluctuations. We discuss the implications for the longstanding hypothesis that activity-dependent plasticity switches synapses between bistable states.},
	pages = {e76120},
	journaltitle = {{eLife}},
	author = {Dorkenwald, Sven and Turner, Nicholas L and Macrina, Thomas and Lee, Kisuk and Lu, Ran and Wu, Jingpeng and Bodor, Agnes L and Bleckert, Adam A and Brittain, Derrick and Kemnitz, Nico and Silversmith, William M and Ih, Dodam and Zung, Jonathan and Zlateski, Aleksandar and Tartavull, Ignacio and Yu, Szi-Chieh and Popovych, Sergiy and Wong, William and Castro, Manuel and Jordan, Chris S and Wilson, Alyssa M and Froudarakis, Emmanouil and Buchanan, {JoAnn} and Takeno, Marc M and Torres, Russel and Mahalingam, Gayathri and Collman, Forrest and Schneider-Mizell, Casey M and Bumbarger, Daniel J and Li, Yang and Becker, Lynne and Suckow, Shelby and Reimer, Jacob and Tolias, Andreas S and Macarico Da Costa, Nuno and Reid, R Clay and Seung, H Sebastian},
	urldate = {2024-01-23},
	date = {2022-11-16},
	langid = {english},
}

@misc{ding_bipartite_2023,
	title = {Bipartite invariance in mouse primary visual cortex},
	url = {http://biorxiv.org/lookup/doi/10.1101/2023.03.15.532836},
	doi = {10.1101/2023.03.15.532836},
	abstract = {A defining characteristic of intelligent systems, whether natural or artificial, is the ability to generalize and infer behaviorally relevant latent causes from high-dimensional sensory input, despite significant variations in the environment. To understand how brains achieve generalization, it is crucial to identify the features to which neurons respond selectively and invariantly. However, the high-dimensional nature of visual inputs, the non-linearity of information processing in the brain, and limited experimental time make it challenging to systematically characterize neuronal tuning and invariances, especially for natural stimuli. Here, we extended “inception loops” — a paradigm that iterates between large-scale recordings, neural predictive models, and
            in silico
            experiments followed by
            in vivo
            verification — to systematically characterize single neuron invariances in the mouse primary visual cortex. Using the predictive model we synthesized Diverse Exciting Inputs ({DEIs}), a set of inputs that differ substantially from each other while each driving a target neuron strongly, and verified these {DEIs}’ efficacy
            in vivo
            . We discovered a novel bipartite invariance: one portion of the receptive field encoded phase-invariant texturelike patterns, while the other portion encoded a fixed spatial pattern. Our analysis revealed that the division between the fixed and invariant portions of the receptive fields aligns with object boundaries defined by spatial frequency differences present in highly activating natural images. These findings suggest that bipartite invariance might play a role in segmentation by detecting texture-defined object boundaries, independent of the phase of the texture. We also replicated these bipartite {DEIs} in the functional connectomics {MICrONs} data set, which opens the way towards a circuit-level mechanistic understanding of this novel type of invariance. Our study demonstrates the power of using a data-driven deep learning approach to systematically characterize neuronal invariances. By applying this method across the visual hierarchy, cell types, and sensory modalities, we can decipher how latent variables are robustly extracted from natural scenes, leading to a deeper understanding of generalization.},
	author = {Ding, Zhiwei and Tran, Dat T. and Ponder, Kayla and Cobos, Erick and Ding, Zhuokun and Fahey, Paul G. and Wang, Eric and Muhammad, Taliah and Fu, Jiakun and Cadena, Santiago A. and Papadopoulos, Stelios and Patel, Saumil and Franke, Katrin and Reimer, Jacob and Sinz, Fabian H. and Ecker, Alexander S. and Pitkow, Xaq and Tolias, Andreas S.},
	urldate = {2025-01-07},
	date = {2023-03-16},
	langid = {english},
}

@misc{fu_pattern_2023,
	title = {Pattern completion and disruption characterize contextual modulation in the visual cortex},
	url = {http://biorxiv.org/lookup/doi/10.1101/2023.03.13.532473},
	doi = {10.1101/2023.03.13.532473},
	abstract = {Vision is fundamentally context-dependent, with neuronal responses influenced not just by local features but also by surrounding contextual information. In the visual cortex, studies using simple grating stimuli indicate that congruent stimuli - where the center and surround share the same orientation - are more inhibitory than when orientations are orthogonal, potentially serving redundancy reduction and predictive coding. Understanding these center-surround interactions in relation to natural image statistics is challenging due to the high dimensionality of the stimulus space, yet crucial for deciphering the neuronal code of real-world sensory processing. Utilizing large-scale recordings from mouse V1, we trained convolutional neural networks ({CNNs}) to predict and synthesize surround patterns that either optimally suppressed or enhanced responses to center stimuli, confirmed by in vivo experiments. Contrary to the notion that congruent stimuli are suppressive, we found that surrounds that completed patterns based on natural image statistics were facilitatory, while disruptive surrounds were suppressive. Applying our {CNN} image synthesis method in macaque V1, we discovered that pattern completion within the near surround occurred more frequently with excitatory than with inhibitory surrounds, suggesting that our results in mice are conserved in macaques. Further, experiments and model analyses confirmed previous studies reporting the opposite effect with grating stimuli in both species. Using the {MICrONS} functional connectomics dataset, we observed that neurons with similar feature selectivity formed excitatory connections regardless of their receptive field overlap, aligning with the pattern completion phenomenon observed for excitatory surrounds. Finally, our empirical results emerged in a normative model of perception implementing Bayesian inference, where neuronal responses are modulated by prior knowledge of natural scene statistics. In summary, our findings identify a novel relationship between contextual information and natural scene statistics and provide evidence for a role of contextual modulation in hierarchical inference},
	author = {Fu, Jiakun and Shrinivasan, Suhas and Baroni, Luca and Ding, Zhuokun and Fahey, Paul G. and Pierzchlewicz, Paweł and Ponder, Kayla and Froebe, Rachel and Ntanavara, Lydia and Muhammad, Taliah and Willeke, Konstantin F and Wang, Eric and Ding, Zhiwei and Tran, Dat T. and Papadopoulos, Stelios and Patel, Saumil and Reimer, Jacob and Ecker, Alexander S. and Pitkow, Xaq and Antolik, Jan and Sinz, Fabian H. and Haefner, Ralf M. and Tolias, Andreas S. and Franke, Katrin},
	urldate = {2025-01-07},
	date = {2023-03-14},
	langid = {english},
}

@misc{fahey_global_2019,
	title = {A global map of orientation tuning in mouse visual cortex},
	url = {http://biorxiv.org/lookup/doi/10.1101/745323},
	doi = {10.1101/745323},
	abstract = {In primates and most carnivores, neurons in primary visual cortex are spatially organized by their functional properties. For example, neurons with similar orientation preferences are grouped together in iso-orientation domains that smoothly vary over the cortical sheet. In rodents, on the other hand, neurons with different orientation preferences are thought to be spatially intermingled, a feature which has been termed “salt-and-pepper” organization. The apparent absence of any systematic structure in orientation tuning has been considered a defining feature of the rodent visual system for more than a decade, with broad implications for brain development, visual processing, and comparative neurophysiology. Here, we revisited this question using new techniques for wide-field two-photon calcium imaging that enabled us to collect nearly complete population tuning preferences in layers 2-4 across a large fraction of the mouse visual hierarchy. Examining the orientation tuning of these hundreds of thousands of neurons, we found a global map spanning multiple visual cortical areas in which orientation bias was organized around a single pinwheel centered in V1. This pattern was consistent across animals and cortical depth. The existence of this global organization in rodents has implications for our understanding of visual processing and the principles governing the ontogeny and phylogeny of the visual cortex of mammals.},
	author = {Fahey, Paul G. and Muhammad, Taliah and Smith, Cameron and Froudarakis, Emmanouil and Cobos, Erick and Fu, Jiakun and Walker, Edgar Y. and Yatsenko, Dimitri and Sinz, Fabian H. and Reimer, Jacob and Tolias, Andreas S.},
	urldate = {2025-01-07},
	date = {2019-08-23},
	langid = {english},
}

@misc{bodor_agnes_l_synaptic_2023,
	title = {The Synaptic Architecture of Layer 5 Thick Tufted Excitatory Neurons in the Visual Cortex of Mice},
	url = {https://www.biorxiv.org/content/10.1101/2023.10.18.562531v1},
	doi = {10.1101/2023.10.18.562531},
	abstract = {The neocortex is one of the most critical structures that makes us human, and it is involved in a variety of cognitive functions from perception to sensory integration and motor control. Composed of repeated modules, or microcircuits, the neocortex relies on distinct cell types as its fundamental building blocks. Despite significant progress in characterizing these cell types 1–5, an understanding of the complete synaptic partners associated with individual excitatory cell types remain elusive.},
	publisher = {{bioRxiv}},
	author = {{Bodor, Agnes L.} and {Schneider-Mizell, Casey M.} and {Zhang, Chi} and {Elabbady, Leila} and {Mallen, Alex} and {Bergeson, Andi} and {Brittain, Derrick} and {Buchannan, JoAnn} and {The MICrONS Consortium} and {Reimer, Jacob} and {Seung, Sebastian H} and {Reid, R. Clay} and {Collman, Forrest} and {da Costa, Nuno Macarico}},
	date = {2023-10-18},
	langid = {english},
}

@article{wang_foundation_2025,
	title = {Foundation model of neural activity predicts response to new stimulus types},
	volume = {640},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-025-08829-y},
	doi = {10.1038/s41586-025-08829-y},
	abstract = {Abstract
            
              The complexity of neural circuits makes it challenging to decipher the brain’s algorithms of intelligence. Recent breakthroughs in deep learning have produced models that accurately simulate brain activity, enhancing our understanding of the brain’s computational objectives and neural coding. However, it is difficult for such models to generalize beyond their training distribution, limiting their utility. The emergence of foundation models
              1
              trained on vast datasets has introduced a new artificial intelligence paradigm with remarkable generalization capabilities. Here we collected large amounts of neural activity from visual cortices of multiple mice and trained a foundation model to accurately predict neuronal responses to arbitrary natural videos. This model generalized to new mice with minimal training and successfully predicted responses across various new stimulus domains, such as coherent motion and noise patterns. Beyond neural response prediction, the model also accurately predicted anatomical cell types, dendritic features and neuronal connectivity within the {MICrONS} functional connectomics dataset
              2
              . Our work is a crucial step towards building foundation models of the brain. As neuroscience accumulates larger, multimodal datasets, foundation models will reveal statistical regularities, enable rapid adaptation to new tasks and accelerate research.},
	pages = {470--477},
	number = {8058},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Wang, Eric Y. and Fahey, Paul G. and Ding, Zhuokun and Papadopoulos, Stelios and Ponder, Kayla and Weis, Marissa A. and Chang, Andersen and Muhammad, Taliah and Patel, Saumil and Ding, Zhiwei and Tran, Dat and Fu, Jiakun and Schneider-Mizell, Casey M. and {MICrONS Consortium} and Da Costa, Nuno Maçarico and Reid, R. Clay and Collman, Forrest and Da Costa, Nuno Maçarico and Franke, Katrin and Ecker, Alexander S. and Reimer, Jacob and Pitkow, Xaq and Sinz, Fabian H. and Tolias, Andreas S.},
	urldate = {2025-04-09},
	date = {2025-04-10},
	langid = {english},
	file = {Wang et al. - 2025 - Foundation model of neural activity predicts respo.pdf:C\:\\Users\\bethanny.danskin\\Zotero\\storage\\S5SPBY4Y\\Wang et al. - 2025 - Foundation model of neural activity predicts respo.pdf:application/pdf},
}

@misc{weis_unsupervised_2022,
	title = {An unsupervised map of excitatory neurons’ dendritic morphology in the mouse visual cortex},
	url = {http://biorxiv.org/lookup/doi/10.1101/2022.12.22.521541},
	doi = {10.1101/2022.12.22.521541},
	abstract = {Neurons in the neocortex exhibit astonishing morphological diversity which is critical for properly wiring neural circuits and giving neurons their functional properties. However, the organizational principles underlying this morphological diversity remain an open question. Here, we took a data-driven approach using graph-based machine learning methods to obtain a low-dimensional morphological “bar code” describing more than 30,000 excitatory neurons in mouse visual areas V1, {AL} and {RL} that were reconstructed from the millimeter scale {MICrONS} serial-section electron microscopy volume. Contrary to previous classiﬁcations into discrete morphological types (m-types), our data-driven approach suggests that the morphological landscape of cortical excitatory neurons is better described as a continuum, with a few notable exceptions in layers 5 and 6. Dendritic morphologies in layers 2–3 exhibited a trend towards a decreasing width of the dendritic arbor and a smaller tuft with increasing cortical depth. Inter-area differences were most evident in layer 4, where V1 contained more atufted neurons than higher visual areas. Moreover, we discovered neurons in V1 on the border to layer 5 which avoided deeper layers with their dendrites. In summary, we suggest that excitatory neurons’ morphological diversity is better understood by considering axes of variation than using distinct m-types.},
	author = {Weis, Marissa A. and Papadopoulos, Stelios and Hansel, Laura and Lüddecke, Timo and Celii, Brendan and Fahey, Paul G. and Wang, Eric Y. and Bae, J. Alexander and Bodor, Agnes L. and Brittain, Derrick and Buchanan, {JoAnn} and Bumbarger, Daniel J. and Castro, Manuel A. and Collman, Forrest and Da Costa, Nuno Maçarico and Dorkenwald, Sven and Elabbady, Leila and Halageri, Akhilesh and Jia, Zhen and Jordan, Chris and Kapner, Dan and Kemnitz, Nico and Kinn, Sam and Lee, Kisuk and Li, Kai and Lu, Ran and Macrina, Thomas and Mahalingam, Gayathri and Mitchell, Eric and Mondal, Shanka Subhra and Mu, Shang and Nehoran, Barak and Popovych, Sergiy and Reid, R. Clay and Schneider-Mizell, Casey M. and Seung, H. Sebastian and Silversmith, William and Takeno, Marc and Torres, Russel and Turner, Nicholas L. and Wong, William and Wu, Jingpeng and Yin, Wenjing and Yu, Szi-chieh and Reimer, Jacob and Berens, Philipp and Tolias, Andreas S. and Ecker, Alexander S.},
	urldate = {2025-01-07},
	date = {2022-12-22},
	langid = {english},
}

@article{celii_neurd_2025,
	title = {{NEURD} offers automated proofreading and feature extraction for connectomics},
	volume = {640},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-025-08660-5},
	doi = {10.1038/s41586-025-08660-5},
	abstract = {Abstract
            
              We are in the era of millimetre-scale electron microscopy volumes collected at nanometre resolution
              1,2
              . Dense reconstruction of cellular compartments in these electron microscopy volumes has been enabled by recent advances in machine learning
              3–6
              . Automated segmentation methods produce exceptionally accurate reconstructions of cells, but post hoc proofreading is still required to generate large connectomes that are free of merge and split errors. The elaborate 3D meshes of neurons in these volumes contain detailed morphological information at multiple scales, from the diameter, shape and branching patterns of axons and dendrites, down to the fine-scale structure of dendritic spines. However, extracting these features can require substantial effort to piece together existing tools into custom workflows. Here, building on existing open source software for mesh manipulation, we present Neural Decomposition ({NEURD}), a software package that decomposes meshed neurons into compact and extensively annotated graph representations. With these feature-rich graphs, we automate a variety of tasks such as state-of-the-art automated proofreading of merge errors, cell classification, spine detection, axonal-dendritic proximities and other annotations. These features enable many downstream analyses of neural morphology and connectivity, making these massive and complex datasets more accessible to neuroscience researchers.},
	pages = {487--496},
	number = {8058},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Celii, Brendan and Papadopoulos, Stelios and Ding, Zhuokun and Fahey, Paul G. and Wang, Eric and Papadopoulos, Christos and Kunin, Alexander B. and Patel, Saumil and Bae, J. Alexander and Bodor, Agnes L. and Brittain, Derrick and Buchanan, {JoAnn} and Bumbarger, Daniel J. and Castro, Manuel A. and Cobos, Erick and Dorkenwald, Sven and Elabbady, Leila and Halageri, Akhilesh and Jia, Zhen and Jordan, Chris and Kapner, Dan and Kemnitz, Nico and Kinn, Sam and Lee, Kisuk and Li, Kai and Lu, Ran and Macrina, Thomas and Mahalingam, Gayathri and Mitchell, Eric and Mondal, Shanka Subhra and Mu, Shang and Nehoran, Barak and Popovych, Sergiy and Schneider-Mizell, Casey M. and Silversmith, William and Takeno, Marc and Torres, Russel and Turner, Nicholas L. and Wong, William and Wu, Jingpeng and Yu, Szi-chieh and Yin, Wenjing and Xenes, Daniel and Kitchell, Lindsey M. and Rivlin, Patricia K. and Rose, Victoria A. and Bishop, Caitlyn A. and Wester, Brock and Froudarakis, Emmanouil and Walker, Edgar Y. and Sinz, Fabian and Seung, H. Sebastian and Collman, Forrest and Da Costa, Nuno Maçarico and Reid, R. Clay and Pitkow, Xaq and Tolias, Andreas S. and Reimer, Jacob},
	urldate = {2025-04-09},
	date = {2025-04-10},
	langid = {english},
	file = {Celii et al. - 2025 - NEURD offers automated proofreading and feature ex.pdf:C\:\\Users\\bethanny.danskin\\Zotero\\storage\\4ESAI7LV\\Celii et al. - 2025 - NEURD offers automated proofreading and feature ex.pdf:application/pdf},
}

@article{gamlin_connectomics_2025,
	title = {Connectomics of predicted Sst transcriptomic types in mouse visual cortex},
	volume = {640},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-025-08805-6},
	doi = {10.1038/s41586-025-08805-6},
	pages = {497--505},
	number = {8058},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Gamlin, Clare R. and Schneider-Mizell, Casey M. and Mallory, Matthew and Elabbady, Leila and Gouwens, Nathan and Williams, Grace and Mukora, Alice and Dalley, Rachel and Bodor, Agnes L. and Brittain, Derrick and Buchanan, {JoAnn} and Bumbarger, Daniel J. and Joyce, Emily and Kapner, Daniel and Kinn, Sam and Mahalingam, Gayathri and Seshamani, Sharmishtaa and Takeno, Marc and Torres, Russel and Yin, Wenjing and Nicovich, Philip R. and Bae, J. Alexander and Castro, Manuel A. and Dorkenwald, Sven and Halageri, Akhilesh and Jia, Zhen and Jordan, Chris and Kemnitz, Nico and Lee, Kisuk and Li, Kai and Lu, Ran and Macrina, Thomas and Mitchell, Eric and Mondal, Shanka Subhra and Mu, Shang and Nehoran, Barak and Popovych, Sergiy and Silversmith, William and Turner, Nicholas L. and Wong, William and Wu, Jingpeng and Yu, Szi-chieh and Berg, Jim and Jarsky, Tim and Lee, Brian and Seung, H. Sebastian and Zeng, Hongkui and Reid, R. Clay and Collman, Forrest and Da Costa, Nuno Maçarico and Sorensen, Staci A.},
	urldate = {2025-04-09},
	date = {2025-04-10},
	langid = {english},
	file = {Gamlin et al. - 2025 - Connectomics of predicted Sst transcriptomic types.pdf:C\:\\Users\\bethanny.danskin\\Zotero\\storage\\F6PBBNU9\\Gamlin et al. - 2025 - Connectomics of predicted Sst transcriptomic types.pdf:application/pdf},
}


@article{dorkenwald_cave_2025,
	title = {{CAVE}: Connectome Annotation Versioning Engine},
	issn = {1548-7091, 1548-7105},
	url = {https://www.nature.com/articles/s41592-024-02426-z},
	doi = {10.1038/s41592-024-02426-z},
	shorttitle = {{CAVE}},
	abstract = {Abstract
            
              Advances in electron microscopy, image segmentation and computational infrastructure have given rise to large-scale and richly annotated connectomic datasets, which are increasingly shared across communities. To enable collaboration, users need to be able to concurrently create annotations and correct errors in the automated segmentation by proofreading. In large datasets, every proofreading edit relabels cell identities of millions of voxels and thousands of annotations like synapses. For analysis, users require immediate and reproducible access to this changing and expanding data landscape. Here we present the Connectome Annotation Versioning Engine ({CAVE}), a computational infrastructure that provides scalable solutions for proofreading and flexible annotation support for fast analysis queries at arbitrary time points. Deployed as a suite of web services, {CAVE} empowers distributed communities to perform reproducible connectome analysis in up to petascale datasets ({\textasciitilde}1 mm
              3
              ) while proofreading and annotating is ongoing.},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Dorkenwald, Sven and Schneider-Mizell, Casey M. and Brittain, Derrick and Halageri, Akhilesh and Jordan, Chris and Kemnitz, Nico and Castro, Manual A. and Silversmith, William and Maitin-Shephard, Jeremy and Troidl, Jakob and Pfister, Hanspeter and Gillet, Valentin and Xenes, Daniel and Bae, J. Alexander and Bodor, Agnes L. and Buchanan, {JoAnn} and Bumbarger, Daniel J. and Elabbady, Leila and Jia, Zhen and Kapner, Daniel and Kinn, Sam and Lee, Kisuk and Li, Kai and Lu, Ran and Macrina, Thomas and Mahalingam, Gayathri and Mitchell, Eric and Mondal, Shanka Subhra and Mu, Shang and Nehoran, Barak and Popovych, Sergiy and Takeno, Marc and Torres, Russel and Turner, Nicholas L. and Wong, William and Wu, Jingpeng and Yin, Wenjing and Yu, Szi-chieh and Reid, R. Clay and Da Costa, Nuno Maçarico and Seung, H. Sebastian and Collman, Forrest},
	urldate = {2025-04-09},
	date = {2025-04-09},
	langid = {english},
	file = {Dorkenwald et al. - 2025 - CAVE Connectome Annotation Versioning Engine.pdf:C\:\\Users\\bethanny.danskin\\Zotero\\storage\\35GUZ2W5\\Dorkenwald et al. - 2025 - CAVE Connectome Annotation Versioning Engine.pdf:application/pdf},
}

@misc{macrina_petascale_2021,
	title = {Petascale neural circuit reconstruction: automated methods},
	url = {http://biorxiv.org/lookup/doi/10.1101/2021.08.04.455162},
	doi = {10.1101/2021.08.04.455162},
	shorttitle = {Petascale neural circuit reconstruction},
	abstract = {Abstract
          3D electron microscopy ({EM}) has been successful at mapping invertebrate nervous systems, but the approach has been limited to small chunks of mammalian brains. To scale up to larger volumes, we have built a computational pipeline for processing petascale image datasets acquired by serial section {EM}, a popular form of 3D {EM}. The pipeline employs convolutional nets to compute the nonsmooth transformations required to align images of serial sections containing numerous cracks and folds, detect neuronal boundaries, label voxels as axon, dendrite, soma, and other semantic categories, and detect synapses and assign them to presynaptic and postsynaptic segments. The output of neuronal boundary detection is segmented by mean affinity agglomeration with semantic and size constraints. Pipeline operations are implemented by leveraging distributed and cloud computing. Intermediate results of the pipeline are held in cloud storage, and can be effortlessly viewed as images, which aids debugging. We applied the pipeline to create an automated reconstruction of an {EM} image volume spanning four visual cortical areas of a mouse brain. Code for the pipeline is publicly available, as is the reconstructed volume.},
	author = {Macrina, Thomas and Lee, Kisuk and Lu, Ran and Turner, Nicholas L. and Wu, Jingpeng and Popovych, Sergiy and Silversmith, William and Kemnitz, Nico and Bae, J. Alexander and Castro, Manuel A. and Dorkenwald, Sven and Halageri, Akhilesh and Jia, Zhen and Jordan, Chris and Li, Kai and Mitchell, Eric and Mondal, Shanka Subhra and Mu, Shang and Nehoran, Barak and Wong, William and Yu, Szi-chieh and Bodor, Agnes L. and Brittain, Derrick and Buchanan, {JoAnn} and Bumbarger, Daniel J. and Cobos, Erick and Collman, Forrest and Elabbady, Leila and Fahey, Paul G. and Froudarakis, Emmanouil and Kapner, Daniel and Kinn, Sam and Mahalingam, Gayathri and Papadopoulos, Stelios and Patel, Saumil and Schneider-Mizell, Casey M. and Sinz, Fabian H. and Takeno, Marc and Torres, Russel and Yin, Wenjing and Pitkow, Xaq and Reimer, Jacob and Tolias, Andreas S. and Reid, R. Clay and Costa, Nuno Maçarico Da and Seung, H. Sebastian},
	urldate = {2025-01-07},
	date = {2021-08-05},
	langid = {english},
}

@article{elabbady_perisomatic_2025,
	title = {Perisomatic ultrastructure efficiently classifies cells in mouse cortex},
	volume = {640},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07765-7},
	doi = {10.1038/s41586-024-07765-7},
	abstract = {Abstract
            
              Mammalian neocortex contains a highly diverse set of cell types. These cell types have been mapped systematically using a variety of molecular, electrophysiological and morphological approaches
              1–4
              . Each modality offers new perspectives on the variation of biological processes underlying cell-type specialization. Cellular-scale electron microscopy provides dense ultrastructural examination and an unbiased perspective on the subcellular organization of brain cells, including their synaptic connectivity and nanometre-scale morphology. In data that contain tens of thousands of neurons, most of which have incomplete reconstructions, identifying cell types becomes a clear challenge for analysis
              5
              . Here, to address this challenge, we present a systematic survey of the somatic region of all cells in a cubic millimetre of cortex using quantitative features obtained from electron microscopy. This analysis demonstrates that the perisomatic region is sufficient to identify cell types, including types defined primarily on the basis of their connectivity patterns. We then describe how this classification facilitates cell-type-specific connectivity characterization and locating cells with rare connectivity patterns in the dataset.},
	pages = {478--486},
	number = {8058},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Elabbady, Leila and Seshamani, Sharmishtaa and Mu, Shang and Mahalingam, Gayathri and Schneider-Mizell, Casey M. and Bodor, Agnes L. and Bae, J. Alexander and Brittain, Derrick and Buchanan, {JoAnn} and Bumbarger, Daniel J. and Castro, Manuel A. and Dorkenwald, Sven and Halageri, Akhilesh and Jia, Zhen and Jordan, Chris and Kapner, Dan and Kemnitz, Nico and Kinn, Sam and Lee, Kisuk and Li, Kai and Lu, Ran and Macrina, Thomas and Mitchell, Eric and Mondal, Shanka Subhra and Nehoran, Barak and Popovych, Sergiy and Silversmith, William and Takeno, Marc and Torres, Russel and Turner, Nicholas L. and Wong, William and Wu, Jingpeng and Yin, Wenjing and Yu, Szi-chieh and Seung, H. Sebastian and Reid, R. Clay and Da Costa, Nuno Maçarico and Collman, Forrest},
	urldate = {2025-04-09},
	date = {2025-04-10},
	langid = {english},
	file = {Elabbady et al. - 2025 - Perisomatic ultrastructure efficiently classifies .pdf:C\:\\Users\\bethanny.danskin\\Zotero\\storage\\2NGJ6QV3\\Elabbady et al. - 2025 - Perisomatic ultrastructure efficiently classifies .pdf:application/pdf},
}

@article{ding_functional_2025,
	title = {Functional connectomics reveals general wiring rule in mouse visual cortex},
	volume = {640},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-025-08840-3},
	doi = {10.1038/s41586-025-08840-3},
	abstract = {Abstract
            
              Understanding the relationship between circuit connectivity and function is crucial for uncovering how the brain computes. In mouse primary visual cortex, excitatory neurons with similar response properties are more likely to be synaptically connected
              1–8
              ; however, broader connectivity rules remain unknown. Here we leverage the millimetre-scale {MICrONS} dataset to analyse synaptic connectivity and functional properties of neurons across cortical layers and areas. Our results reveal that neurons with similar response properties are preferentially connected within and across layers and areas—including feedback connections—supporting the universality of ‘like-to-like’ connectivity across the visual hierarchy. Using a validated digital twin model, we separated neuronal tuning into feature (what neurons respond to) and spatial (receptive field location) components. We found that only the feature component predicts fine-scale synaptic connections beyond what could be explained by the proximity of axons and dendrites. We also discovered a higher-order rule whereby postsynaptic neuron cohorts downstream of presynaptic cells show greater functional similarity than predicted by a pairwise like-to-like rule. Recurrent neural networks trained on a simple classification task develop connectivity patterns that mirror both pairwise and higher-order rules, with magnitudes similar to those in {MICrONS} data. Ablation studies in these recurrent neural networks reveal that disrupting like-to-like connections impairs performance more than disrupting random connections. These findings suggest that these connectivity principles may have a functional role in sensory processing and learning, highlighting shared principles between biological and artificial systems.},
	pages = {459--469},
	number = {8058},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Ding, Zhuokun and Fahey, Paul G. and Papadopoulos, Stelios and Wang, Eric Y. and Celii, Brendan and Papadopoulos, Christos and Chang, Andersen and Kunin, Alexander B. and Tran, Dat and Fu, Jiakun and Ding, Zhiwei and Patel, Saumil and Ntanavara, Lydia and Froebe, Rachel and Ponder, Kayla and Muhammad, Taliah and Bae, J. Alexander and Bodor, Agnes L. and Brittain, Derrick and Buchanan, {JoAnn} and Bumbarger, Daniel J. and Castro, Manuel A. and Cobos, Erick and Dorkenwald, Sven and Elabbady, Leila and Halageri, Akhilesh and Jia, Zhen and Jordan, Chris and Kapner, Dan and Kemnitz, Nico and Kinn, Sam and Lee, Kisuk and Li, Kai and Lu, Ran and Macrina, Thomas and Mahalingam, Gayathri and Mitchell, Eric and Mondal, Shanka Subhra and Mu, Shang and Nehoran, Barak and Popovych, Sergiy and Schneider-Mizell, Casey M. and Silversmith, William and Takeno, Marc and Torres, Russel and Turner, Nicholas L. and Wong, William and Wu, Jingpeng and Yin, Wenjing and Yu, Szi-chieh and Yatsenko, Dimitri and Froudarakis, Emmanouil and Sinz, Fabian and Josić, Krešimir and Rosenbaum, Robert and Seung, H. Sebastian and Collman, Forrest and Da Costa, Nuno Maçarico and Reid, R. Clay and Walker, Edgar Y. and Pitkow, Xaq and Reimer, Jacob and Tolias, Andreas S.},
	urldate = {2025-04-09},
	date = {2025-04-10},
	langid = {english},
	file = {Ding et al. - 2025 - Functional connectomics reveals general wiring rul.pdf:C\:\\Users\\bethanny.danskin\\Zotero\\storage\\UPL87BBG\\s41586-025-08840-3.pdf:application/pdf},
}

@misc{turner_synaptic_2019,
	title = {Synaptic Partner Assignment Using Attentional Voxel Association Networks},
	url = {http://arxiv.org/abs/1904.09947},
	doi = {10.48550/arXiv.1904.09947},
	abstract = {Connectomics aims to recover a complete set of synaptic connections within a dataset imaged by volume electron microscopy. Many systems have been proposed for locating synapses, and recent research has included a way to identify the synaptic partners that communicate at a synaptic cleft. We re-frame the problem of identifying synaptic partners as directly generating the mask of the synaptic partners from a given cleft. We train a convolutional network to perform this task. The network takes the local image context and a binary mask representing a single cleft as input. It is trained to produce two binary output masks: one which labels the voxels of the presynaptic partner within the input image, and another similar labeling for the postsynaptic partner. The cleft mask acts as an attentional gating signal for the network. We ﬁnd that an implementation of this approach performs well on a dataset of mouse somatosensory cortex, and evaluate it as part of a combined system to predict both clefts and connections.},
	number = {{arXiv}:1904.09947},
	publisher = {{arXiv}},
	author = {Turner, Nicholas and Lee, Kisuk and Lu, Ran and Wu, Jingpeng and Ih, Dodam and Seung, H. Sebastian},
	urldate = {2025-01-07},
	date = {2019-11-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.09947 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhou_ease_2020,
	title = {{EASE}: {EM}-Assisted Source Extraction from calcium imaging data},
	rights = {https://www.biorxiv.org/about/{FAQ}\#license},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.03.25.007468},
	doi = {10.1101/2020.03.25.007468},
	shorttitle = {{EASE}},
	abstract = {Combining two-photon calcium imaging (2PCI) and electron microscopy ({EM}) provides arguably the most powerful current approach for connecting function to structure in neural circuits. Recent years have seen dramatic advances in obtaining and processing {CI} and {EM} data separately. In addition, several joint {CI}-{EM} datasets (with {CI} performed in vivo, followed by {EM} reconstruction of the same volume) have been collected. However, no automated analysis tools yet exist that can match each signal extracted from the {CI} data to a cell segment extracted from {EM}; previous eﬀorts have been largely manual and focused on analyzing calcium activity in cell bodies, neglecting potentially rich functional information from axons and dendrites. There are two major roadblocks to solving this matching problem: ﬁrst, dense {EM} reconstruction extracts orders of magnitude more segments than are visible in the corresponding {CI} ﬁeld of view, and second, due to optical constraints and non-uniform brightness of the calcium indicator in each cell, direct matching of {EM} and {CI} spatial components is nontrivial. In this work we develop a pipeline for fusing {CI} and densely-reconstructed {EM} data. We model the observed {CI} data using a constrained nonnegative matrix factorization ({CNMF}) framework, in which segments extracted from the {EM} reconstruction serve to initialize and constrain the spatial components of the matrix factorization. We develop an eﬃcient iterative procedure for solving the resulting combined matching and matrix factorization problem and apply this procedure to joint {CI}-{EM} data from mouse visual cortex. The method recovers hundreds of dendritic components from the {CI} data, visible across multiple functional scans at diﬀerent depths, matched with densely-reconstructed three-dimensional neural segments recovered from the {EM} volume. We publicly release the output of this analysis as a new gold standard dataset that can be used to score algorithms for demixing signals from 2PCI data. Finally, we show that this database can be exploited to (1) learn a mapping from 3d {EM} segmentations to predict the corresponding 2d spatial components estimated from {CI} data, and (2) train a neural network to denoise these estimated spatial components. This neural network denoiser is a stand-alone module that can be dropped in to enhance any existing 2PCI analysis pipeline.},
	author = {Zhou, Pengcheng and Reimer, Jacob and Zhou, Ding and Pasarkar, Amol and Kinsella, Ian and Froudarakis, Emmanouil and Yatsenko, Dimitri V and Fahey, Paul G and Bodor, Agnes and Buchanan, {JoAnn} and Bumbarger, Dan and Mahalingam, Gayathri and Torres, Russel and Dorkenwald, Sven and Ih, Dodam and Lee, Kisuk and Lu, Ran and Macrina, Thomas and Wu, Jingpeng and Da Costa, Nuno and Reid, R. Clay and Tolias, Andreas S and Paninski, Liam},
	urldate = {2025-01-07},
	date = {2020-03-25},
	langid = {english},
}

@article{schneider-mizell_inhibitory_2025,
	title = {Inhibitory specificity from a connectomic census of mouse visual cortex},
	volume = {640},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07780-8},
	doi = {10.1038/s41586-024-07780-8},
	pages = {448--458},
	number = {8058},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Schneider-Mizell, Casey M. and Bodor, Agnes L. and Brittain, Derrick and Buchanan, {JoAnn} and Bumbarger, Daniel J. and Elabbady, Leila and Gamlin, Clare and Kapner, Daniel and Kinn, Sam and Mahalingam, Gayathri and Seshamani, Sharmishtaa and Suckow, Shelby and Takeno, Marc and Torres, Russel and Yin, Wenjing and Dorkenwald, Sven and Bae, J. Alexander and Castro, Manuel A. and Halageri, Akhilesh and Jia, Zhen and Jordan, Chris and Kemnitz, Nico and Lee, Kisuk and Li, Kai and Lu, Ran and Macrina, Thomas and Mitchell, Eric and Mondal, Shanka Subhra and Mu, Shang and Nehoran, Barak and Popovych, Sergiy and Silversmith, William and Turner, Nicholas L. and Wong, William and Wu, Jingpeng and Reimer, Jacob and Tolias, Andreas S. and Seung, H. Sebastian and Reid, R. Clay and Collman, Forrest and Da Costa, Nuno Maçarico},
	urldate = {2025-04-09},
	date = {2025-04-10},
	langid = {english},
	file = {Schneider-Mizell et al. - 2025 - Inhibitory specificity from a connectomic census o.pdf:C\:\\Users\\bethanny.danskin\\Zotero\\storage\\SETMKP2G\\Schneider-Mizell et al. - 2025 - Inhibitory specificity from a connectomic census o.pdf:application/pdf},
}

@article{buchanan_oligodendrocyte_2022,
	title = {Oligodendrocyte precursor cells ingest axons in the mouse neocortex},
	volume = {119},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2202580119},
	doi = {10.1073/pnas.2202580119},
	abstract = {Neurons in the developing brain undergo extensive structural refinement as nascent circuits adopt their mature form. This physical transformation of neurons is facilitated by the engulfment and degradation of axonal branches and synapses by surrounding glial cells, including microglia and astrocytes. However, the small size of phagocytic organelles and the complex, highly ramified morphology of glia have made it difficult to define the contribution of these and other glial cell types to this crucial process. Here, we used large-scale, serial section transmission electron microscopy ({TEM}) with computational volume segmentation to reconstruct the complete 3D morphologies of distinct glial types in the mouse visual cortex, providing unprecedented resolution of their morphology and composition. Unexpectedly, we discovered that the fine processes of oligodendrocyte precursor cells ({OPCs}), a population of abundant, highly dynamic glial progenitors, frequently surrounded small branches of axons. Numerous phagosomes and phagolysosomes ({PLs}) containing fragments of axons and vesicular structures were present inside their processes, suggesting that {OPCs} engage in axon pruning. Single-nucleus {RNA} sequencing from the developing mouse cortex revealed that {OPCs} express key phagocytic genes at this stage, as well as neuronal transcripts, consistent with active axon engulfment. Although microglia are thought to be responsible for the majority of synaptic pruning and structural refinement, {PLs} were ten times more abundant in {OPCs} than in microglia at this stage, and these structures were markedly less abundant in newly generated oligodendrocytes, suggesting that {OPCs} contribute substantially to the refinement of neuronal circuits during cortical development.},
	pages = {e2202580119},
	number = {48},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Buchanan, {JoAnn} and Elabbady, Leila and Collman, Forrest and Jorstad, Nikolas L. and Bakken, Trygve E. and Ott, Carolyn and Glatzer, Jenna and Bleckert, Adam A. and Bodor, Agnes L. and Brittain, Derrick and Bumbarger, Daniel J. and Mahalingam, Gayathri and Seshamani, Sharmishtaa and Schneider-Mizell, Casey and Takeno, Marc M. and Torres, Russel and Yin, Wenjing and Hodge, Rebecca D. and Castro, Manuel and Dorkenwald, Sven and Ih, Dodam and Jordan, Chris S. and Kemnitz, Nico and Lee, Kisuk and Lu, Ran and Macrina, Thomas and Mu, Shang and Popovych, Sergiy and Silversmith, William M. and Tartavull, Ignacio and Turner, Nicholas L. and Wilson, Alyssa M. and Wong, William and Wu, Jingpeng and Zlateski, Aleksandar and Zung, Jonathan and Lippincott-Schwartz, Jennifer and Lein, Ed S. and Seung, H. Sebastian and Bergles, Dwight E. and Reid, R. Clay and Da Costa, Nuno Maçarico},
	urldate = {2025-01-07},
	date = {2022-11-29},
	langid = {english},
}

@misc{ott_nanometer-scale_2023,
	title = {Nanometer-scale views of visual cortex reveal anatomical features of primary cilia poised to detect synaptic spillover},
	url = {http://biorxiv.org/lookup/doi/10.1101/2023.10.31.564838},
	doi = {10.1101/2023.10.31.564838},
	abstract = {A primary cilium is a thin membrane-bound extension off a cell surface that contains receptors for perceiving and transmitting signals that modulate cell state and activity. While many cell types have a primary cilium, little is known about primary cilia in the brain, where they are less accessible than cilia on cultured cells or epithelial tissues and protrude from cell bodies into a deep, dense network of glial and neuronal processes. Here, we investigated cilia frequency, internal structure, shape, and position in large, high-resolution transmission electron microscopy volumes of mouse primary visual cortex. Cilia extended from the cell bodies of nearly all excitatory and inhibitory neurons, astrocytes, and oligodendrocyte precursor cells ({OPCs}), but were absent from oligodendrocytes and microglia. Structural comparisons revealed that the membrane structure at the base of the cilium and the microtubule organization differed between neurons and glia. {OPC} cilia were distinct in that they were the shortest and contained pervasive internal vesicles only occasionally observed in neuron and astrocyte cilia. Investigating cilia-proximal features revealed that many cilia were directly adjacent to synapses, suggesting cilia are well poised to encounter locally released signaling molecules. Cilia proximity to synapses was random, not enriched, in the synapse-rich neuropil. The internal anatomy, including microtubule changes and centriole location, defined key structural features including cilium placement and shape. Together, the anatomical insights both within and around neuron and glia cilia provide new insights into cilia formation and function across cell types in the brain.},
	author = {Ott, Carolyn M. and Torres, Russel and Kuan, Tung-Sheng and Kuan, Aaron and Buchanan, {JoAnn} and Elabbady, Leila and Seshamani, Sharmishtaa and Bodor, Agnes L. and Collman, Forrest and Bock, Davi D. and Lee, Wei Chung and Maçarico Da Costa, Nuno and Lippincott-Schwartz, Jennifer},
	urldate = {2025-01-07},
	date = {2023-11-01},
	langid = {english},
}

@article{turner_reconstruction_2022,
	title = {Reconstruction of neocortex: Organelles, compartments, cells, circuits, and activity},
	volume = {185},
	issn = {00928674},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0092867422001349},
	doi = {10.1016/j.cell.2022.01.023},
	shorttitle = {Reconstruction of neocortex},
	abstract = {We assembled a semi-automated reconstruction of L2/3 mouse primary visual cortex from \$250 3 140 3 90 mm3 of electron microscopic images, including pyramidal and non-pyramidal neurons, astrocytes, microglia, oligodendrocytes and precursors, pericytes, vasculature, nuclei, mitochondria, and synapses. Visual responses of a subset of pyramidal cells are included. The data are publicly available, along with tools for programmatic and three-dimensional interactive access. Brief vignettes illustrate the breadth of potential applications relating structure to function in cortical circuits and neuronal cell biology. Mitochondria and synapse organization are characterized as a function of path length from the soma. Pyramidal connectivity motif frequencies are predicted accurately using a conﬁguration model of random graphs. Pyramidal cells receiving more connections from nearby cells exhibit stronger and more reliable visual responses. Sample code shows data access and analysis.},
	pages = {1082--1100.e24},
	number = {6},
	journaltitle = {Cell},
	shortjournal = {Cell},
	author = {Turner, Nicholas L. and Macrina, Thomas and Bae, J. Alexander and Yang, Runzhe and Wilson, Alyssa M. and Schneider-Mizell, Casey and Lee, Kisuk and Lu, Ran and Wu, Jingpeng and Bodor, Agnes L. and Bleckert, Adam A. and Brittain, Derrick and Froudarakis, Emmanouil and Dorkenwald, Sven and Collman, Forrest and Kemnitz, Nico and Ih, Dodam and Silversmith, William M. and Zung, Jonathan and Zlateski, Aleksandar and Tartavull, Ignacio and Yu, Szi-chieh and Popovych, Sergiy and Mu, Shang and Wong, William and Jordan, Chris S. and Castro, Manuel and Buchanan, {JoAnn} and Bumbarger, Daniel J. and Takeno, Marc and Torres, Russel and Mahalingam, Gayathri and Elabbady, Leila and Li, Yang and Cobos, Erick and Zhou, Pengcheng and Suckow, Shelby and Becker, Lynne and Paninski, Liam and Polleux, Franck and Reimer, Jacob and Tolias, Andreas S. and Reid, R. Clay and Da Costa, Nuno Maçarico and Seung, H. Sebastian},
	urldate = {2025-01-07},
	date = {2022-03},
	langid = {english},
}


@article{glickfeld_higher-order_2017,
  title = {Higher-{{Order Areas}} of the {{Mouse Visual Cortex}}},
  author = {Glickfeld, Lindsey L. and Olsen, Shawn R.},
  year = {2017},
  journal = {Annual Review of Vision Science},
  volume = {3},
  pages = {251--273},
  issn = {2374-4650},
  doi = {10.1146/annurev-vision-102016-061331},
  pmid = {28746815},
  keywords = {Animal,Animals,Behavior,Brain Mapping,connectivity,Connectome,functional specialization,hierarchical and parallel processing,higher visual area,Mice,mouse,visual cortex,Visual Cortex,Visual Pathways,Visual Perception}
}