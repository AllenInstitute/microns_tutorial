{"title":"MICrONS Functional Data","markdown":{"yaml":{"jupytext":{"formats":"md:myst","text_representation":{"extension":".md","format_name":"myst","format_version":0.13,"jupytext_version":"1.11.5"}},"kernelspec":{"display_name":"Python 3","language":"python","name":"allensdk"}},"headingText":"MICrONS Functional Data","containsRefs":false,"markdown":"\n\n(em:functional-data)=\n\nThe MICrONs mouse went through a battery of functional imaging experiments before be prepared for electron microscopy.\nExcitatory neurons were imaged with 2-photon calcium imaging using GCaMP6s.\nBecause of the size of the volume, different populations of neurons were imaged in different sessions, spanning several days.\nEach neuronal {term}`ROI`` from the functional data is uniquely determined based on the combination of image session, scan index, and ROI unit id.\nDuring each imaging session, the mouse was head-fixed and presented a variety of visual stimuli to the left visual field, including both natural movies and synthetic parametric movies.\nIn addition to functional 2p imaging, the treadmill rotation, left eye position, and left pupil diameter were recorded to provide behavioral context.\n\n## Stimuli and \"Digital twin\"\n\nThe visual stimuli presented to the mouse included many natural movies, as well as parametric stimuli that were designed to emphasize specific visual features, such as orientation and spatial frequency.\nBecause imaging sessions were performed over several days and imaging time in any one location was limited, comparisons between cells in different sessions are not straightforward.\nTo get around this, the principle goal of this collection of stimuli was to be used as training data for deep neural networks that were trained to predict the response of each cell to an arbitrary stimulus.\nThese so-called \"digital twins\" were designed to be used to probe functional responses to cells to stimuli that were outside the original training set.\nMore details about this digital twin approach can be found in [Wang et al. 2023](https://www.biorxiv.org/content/10.1101/2023.03.21.533548v2).\n\nAs a measure of the reliability of visual responses, a collection of six movies totalling one minute were presented to the mouse ten times per imaging session.\nAn **oracle score** was computed based on the signal correlation of a given cell to the oracle stimuli across the imaging session, where higher numbers indicate more reliable responses.\nSpecifically, the oracle score is the mean signal correlation of the response of each presentation to the average of the other nine presentations.\n\n\n## Coregistration\n\nThe process of aligning the location of cells from the functional imaging with the same cells in the EM imaging is called **coregistration**.\nIt is a challenging problem due to the need for micron-scale alignment of image volumes, despite signifiant differences in the imaging modalities, tissue deformations under different conditions and potential distortions introduced by sample preparation.\n\nThe dataset contains two approaches to coregistration, one semi-manual and one fully automatic.\nIn the semi-manual approach, a transform between the EM data and the 2p data was generated based on fiducual points such as blood vessels and cell bodies.\nThis transform was then applied to the functional data to identify a location in the EM space, and a human annotator then identified the cell body in the EM data that was a best match to the functional ROI based on location and context.\nIn the fully automatic approach, blood vessels were segmented in both 2p and EM volumes, and a transform was generated based on matching this 3d structure.\nMore details can be found in the [MICrONS dataset preprint](https://www.biorxiv.org/content/10.1101/2021.07.28.454025v3).\n\n### Coregistration Quality Metrics\n\n```{figure} img/coreg-metrics.png\n---\nalign: center\n---\nCartoon illustrating the coregistration metrics.\n```\n\nTwo values are available to help assess the quality of the coregistration.\nThe **residual** indicates the distance between the location of an ROI after transformation to the EM space and the location of the matched cell body in the EM data.\nThe **score** (or separation score) is the distance between the matched cell body and the nearest other cell body in the EM data.\nThis attempts to measure how the residual compares with the distance to other potential matches in the data.\nLarger values indicate fewer potential matches and therefore a more confident match, in general.\n\nWhen using the automated coregistration, it is important to filter the data based on assignment confidence.\nA guide for this can be found by comparing the subset of cells matched by both the automated and manual coregistration methods.\n\n```{figure} img/coreg-agreement.png\n---\nalign: center\n---\nRelationship between separation threshold (left) and residual (right) and the accuracy of automated coregistration compared to manual coregistration. Orange curves depict the fraction of cells that remain after filtering out those matches beyond the threshold indicated on the x-axis.\n```\n\n### Matching EM to Function\n\nThe combination of session index, scan index, and ROI unit id uniquely identifies each ROI in the functional data.\nThe annotation database contains tables with the results of each of the coregistration methods.\nEach row in each table contains the nucleus id, centroid, and root ID of an EM cell as well as the scan/session/unit indices required to match it.\nIn additoin, the residual and score metrics for each match are provided to filter by quality.\nFor manual coregistration, the table is called `coregistration_manual_v3` and for automated coregistration, the table is called `apl_functional_coreg_forward_v5`.\n\nFull column definitions can be [found on Annotation Tables page](em:functional-coreg).\n\n## Functional data\n\nA collection of *in silico* model responses of neurons to a variety of visual stimuli has been precomputed.\nThe data can be found in a collection of files:\n\n:::{list-table}\n:header-rows: 1\n* - Filename\n  - Format\n  - Info\n* - `nat_movie.npy`\n  - `numpy.ndarray`\n  - dimensions=`[clip, frame, height, width]`\n* - `nat_resp.npy`\n  - `numpy.ndarray`\n  - dimensions=`[unit, bin]`\n* - `nat_unit.csv`\n  - importable into `pandas.DataFrame`\n  - columns=`[animal_id, scan_session, scan_idx, unit_id, row_idx]`\n* - `monet_resp.npy`\n  - `numpy.ndarray`\n  - dimensions=`[unit, direction, trial]`\n* - `monet_dir.npy`\n  - `numpy.ndarray`\n  - dimensions=`[direction]`\n* - `monet_unit.csv`\n  - importable into `pandas.DataFrame`\n  - columns=`[animal_id, scan_session, scan_idx, unit_id]`\n:::\n\nThe model response data is organized into two sets of files, one for natural movies and one for Monet stimuli, a type of parametric stimulus that measures orientation tuning.\nThe `.npy` files can be read with the numpy function `np.load` and the `.csv` files with the pandas function `pd.read_csv`.\n\n```{figure} img/function-stimulus.png\n---\nalign: center\n---\nExample images from a variety fo the stimuli used to probe functional responses. Natural movies include scenes from cinema, POV nature videos, and rendered 3d scenes. Monet stimuli are a parametic textured stimulus that varies in orientation and spatial frequency of correlated motion.\n```\n\nThe natural movie data is organized into three files:\n\n* `nat_movie` contains 250 10-sec clips of natural movies. Movies were shown to the model at a resolution of 72 * 128 pixels at 30 hz. This is downsampled from the *in vivo* presentation resolution of 144 * 256 pixels.\n* `nat_resp` contains 104,171 units' binned responses to natural movies. Responses were binned into 500 msec non-overlapping bins. This results in 250 clips * 10 sec / 500 msec = 5000 bins, with every binned response corresponding to 15 frames of natural movies.\n* `nat_unit` contains the mapping from rows in nat_resp to functional unit keys. Since there is only a single animal for the EM data, use the session, scan, and unit id values to match to the EM data via one of the coregistration tables described above.\n\nThe Monet stimulus data is similarly organized into three files:\n\n* `monet_resp` contains 104,171 units' mean responses to Monet stimuli. 120 trials of 16-direction Monet stimuli were shown to the model. The 120 trials were generated with different random seeds.\n\n* `monet_dir` contains the direction (in units of degrees) for the Monet stimulus shown to the model. The order of directions matches the order in the direction dimension of monet_resp. Here, 0 degrees is vertical feature moving leftwards, with degrees increases in the anti-clockwise direction.\n\n* `monet_unit` contains the functional unit keys of the 104,171 units. The order of the unit keys matches the order in the unit dimension of `monet_resp`.  Since there is only a single animal for the EM data, use the session, scan, and unit id values to match to the EM data via one of the coregistration tables described above.\n","srcMarkdownNoYaml":"\n\n(em:functional-data)=\n# MICrONS Functional Data\n\nThe MICrONs mouse went through a battery of functional imaging experiments before be prepared for electron microscopy.\nExcitatory neurons were imaged with 2-photon calcium imaging using GCaMP6s.\nBecause of the size of the volume, different populations of neurons were imaged in different sessions, spanning several days.\nEach neuronal {term}`ROI`` from the functional data is uniquely determined based on the combination of image session, scan index, and ROI unit id.\nDuring each imaging session, the mouse was head-fixed and presented a variety of visual stimuli to the left visual field, including both natural movies and synthetic parametric movies.\nIn addition to functional 2p imaging, the treadmill rotation, left eye position, and left pupil diameter were recorded to provide behavioral context.\n\n## Stimuli and \"Digital twin\"\n\nThe visual stimuli presented to the mouse included many natural movies, as well as parametric stimuli that were designed to emphasize specific visual features, such as orientation and spatial frequency.\nBecause imaging sessions were performed over several days and imaging time in any one location was limited, comparisons between cells in different sessions are not straightforward.\nTo get around this, the principle goal of this collection of stimuli was to be used as training data for deep neural networks that were trained to predict the response of each cell to an arbitrary stimulus.\nThese so-called \"digital twins\" were designed to be used to probe functional responses to cells to stimuli that were outside the original training set.\nMore details about this digital twin approach can be found in [Wang et al. 2023](https://www.biorxiv.org/content/10.1101/2023.03.21.533548v2).\n\nAs a measure of the reliability of visual responses, a collection of six movies totalling one minute were presented to the mouse ten times per imaging session.\nAn **oracle score** was computed based on the signal correlation of a given cell to the oracle stimuli across the imaging session, where higher numbers indicate more reliable responses.\nSpecifically, the oracle score is the mean signal correlation of the response of each presentation to the average of the other nine presentations.\n\n\n## Coregistration\n\nThe process of aligning the location of cells from the functional imaging with the same cells in the EM imaging is called **coregistration**.\nIt is a challenging problem due to the need for micron-scale alignment of image volumes, despite signifiant differences in the imaging modalities, tissue deformations under different conditions and potential distortions introduced by sample preparation.\n\nThe dataset contains two approaches to coregistration, one semi-manual and one fully automatic.\nIn the semi-manual approach, a transform between the EM data and the 2p data was generated based on fiducual points such as blood vessels and cell bodies.\nThis transform was then applied to the functional data to identify a location in the EM space, and a human annotator then identified the cell body in the EM data that was a best match to the functional ROI based on location and context.\nIn the fully automatic approach, blood vessels were segmented in both 2p and EM volumes, and a transform was generated based on matching this 3d structure.\nMore details can be found in the [MICrONS dataset preprint](https://www.biorxiv.org/content/10.1101/2021.07.28.454025v3).\n\n### Coregistration Quality Metrics\n\n```{figure} img/coreg-metrics.png\n---\nalign: center\n---\nCartoon illustrating the coregistration metrics.\n```\n\nTwo values are available to help assess the quality of the coregistration.\nThe **residual** indicates the distance between the location of an ROI after transformation to the EM space and the location of the matched cell body in the EM data.\nThe **score** (or separation score) is the distance between the matched cell body and the nearest other cell body in the EM data.\nThis attempts to measure how the residual compares with the distance to other potential matches in the data.\nLarger values indicate fewer potential matches and therefore a more confident match, in general.\n\nWhen using the automated coregistration, it is important to filter the data based on assignment confidence.\nA guide for this can be found by comparing the subset of cells matched by both the automated and manual coregistration methods.\n\n```{figure} img/coreg-agreement.png\n---\nalign: center\n---\nRelationship between separation threshold (left) and residual (right) and the accuracy of automated coregistration compared to manual coregistration. Orange curves depict the fraction of cells that remain after filtering out those matches beyond the threshold indicated on the x-axis.\n```\n\n### Matching EM to Function\n\nThe combination of session index, scan index, and ROI unit id uniquely identifies each ROI in the functional data.\nThe annotation database contains tables with the results of each of the coregistration methods.\nEach row in each table contains the nucleus id, centroid, and root ID of an EM cell as well as the scan/session/unit indices required to match it.\nIn additoin, the residual and score metrics for each match are provided to filter by quality.\nFor manual coregistration, the table is called `coregistration_manual_v3` and for automated coregistration, the table is called `apl_functional_coreg_forward_v5`.\n\nFull column definitions can be [found on Annotation Tables page](em:functional-coreg).\n\n## Functional data\n\nA collection of *in silico* model responses of neurons to a variety of visual stimuli has been precomputed.\nThe data can be found in a collection of files:\n\n:::{list-table}\n:header-rows: 1\n* - Filename\n  - Format\n  - Info\n* - `nat_movie.npy`\n  - `numpy.ndarray`\n  - dimensions=`[clip, frame, height, width]`\n* - `nat_resp.npy`\n  - `numpy.ndarray`\n  - dimensions=`[unit, bin]`\n* - `nat_unit.csv`\n  - importable into `pandas.DataFrame`\n  - columns=`[animal_id, scan_session, scan_idx, unit_id, row_idx]`\n* - `monet_resp.npy`\n  - `numpy.ndarray`\n  - dimensions=`[unit, direction, trial]`\n* - `monet_dir.npy`\n  - `numpy.ndarray`\n  - dimensions=`[direction]`\n* - `monet_unit.csv`\n  - importable into `pandas.DataFrame`\n  - columns=`[animal_id, scan_session, scan_idx, unit_id]`\n:::\n\nThe model response data is organized into two sets of files, one for natural movies and one for Monet stimuli, a type of parametric stimulus that measures orientation tuning.\nThe `.npy` files can be read with the numpy function `np.load` and the `.csv` files with the pandas function `pd.read_csv`.\n\n```{figure} img/function-stimulus.png\n---\nalign: center\n---\nExample images from a variety fo the stimuli used to probe functional responses. Natural movies include scenes from cinema, POV nature videos, and rendered 3d scenes. Monet stimuli are a parametic textured stimulus that varies in orientation and spatial frequency of correlated motion.\n```\n\nThe natural movie data is organized into three files:\n\n* `nat_movie` contains 250 10-sec clips of natural movies. Movies were shown to the model at a resolution of 72 * 128 pixels at 30 hz. This is downsampled from the *in vivo* presentation resolution of 144 * 256 pixels.\n* `nat_resp` contains 104,171 units' binned responses to natural movies. Responses were binned into 500 msec non-overlapping bins. This results in 250 clips * 10 sec / 500 msec = 5000 bins, with every binned response corresponding to 15 frames of natural movies.\n* `nat_unit` contains the mapping from rows in nat_resp to functional unit keys. Since there is only a single animal for the EM data, use the session, scan, and unit id values to match to the EM data via one of the coregistration tables described above.\n\nThe Monet stimulus data is similarly organized into three files:\n\n* `monet_resp` contains 104,171 units' mean responses to Monet stimuli. 120 trials of 16-direction Monet stimuli were shown to the model. The 120 trials were generated with different random seeds.\n\n* `monet_dir` contains the direction (in units of degrees) for the Monet stimulus shown to the model. The order of directions matches the order in the direction dimension of monet_resp. Here, 0 degrees is vertical feature moving leftwards, with degrees increases in the anti-clockwise direction.\n\n* `monet_unit` contains the functional unit keys of the 104,171 units. The order of the unit keys matches the order in the unit dimension of `monet_resp`.  Since there is only a single animal for the EM data, use the session, scan, and unit id values to match to the EM data via one of the coregistration tables described above.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"em_functional_intro.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.546","theme":"cosmo","jupytext":{"formats":"md:myst","text_representation":{"extension":".md","format_name":"myst","format_version":0.13,"jupytext_version":"1.11.5"}},"kernelspec":{"display_name":"Python 3","language":"python","name":"allensdk"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}