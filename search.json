[
  {
    "objectID": "programmatic_access/ultrastructure_tutorial/em_ultrastructure.html",
    "href": "programmatic_access/ultrastructure_tutorial/em_ultrastructure.html",
    "title": "Ultrastructure",
    "section": "",
    "text": "Under construction",
    "crumbs": [
      "Introduction",
      "Ultrastructure"
    ]
  },
  {
    "objectID": "programmatic_access/ultrastructure_tutorial/em_ultrastructure.html#ultrastructure-resources",
    "href": "programmatic_access/ultrastructure_tutorial/em_ultrastructure.html#ultrastructure-resources",
    "title": "Ultrastructure",
    "section": "Ultrastructure Resources",
    "text": "Ultrastructure Resources\nElectron microscopy allows one to see very high resolution features called the “ultrastructure” of the cell, such as organelles and microtubules. However, when first seeing EM images, it can be difficult to know the biological meaning of what you are looking at.\nHere are a couple of resources with several examples of particular structures imaged with EM:\n\nSynapseWeb. This website from the Harris lab at UT Austin offers a number of annotated examples of EM images, in particular the Atlas of Ultrastructural Neurocytology.\nBrain Ultrastructure: Putting the Pieces Together. This short paper has some particularly clear pictures of less common structures like neurites undergoing autophagy.\n\nHowever, note that EM imagery can look very different when imaged with different imaging techniques, sectioning methods or staining protocols, and thus the exact appearance can vary between datasets. In addition, the same structure imaged in different planes can look different, particularly in anisotropic approaches like serial section EM where the x/y resolution is much finer than the z resolution.",
    "crumbs": [
      "Introduction",
      "Ultrastructure"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_07_coordinates.html",
    "href": "programmatic_access/em_py_07_coordinates.html",
    "title": "Coordinate System",
    "section": "",
    "text": "Because of the many different data representations, dealing with coordinates in the MICrONs data is not entirely simple and it is easy to make mistakes by converting between coordinate systems incorrectly.\nThere are three main coordinate systems that wind up getting used:\n\nVoxel coordinates are the coordinates of a point in the original image volume. These are the coordinates that are used to index into the volumes you can see in Neuroglancer, but each number has a potentially different unit. In the MICrONs data, a voxel is 4 nm wide in the x and y directions, and 40 nm long in the z direction. This means that a 1x1x1 micron cube would be represented by a 250x250x25 voxel span. Annotations (such as synapses) are stored in voxel coordinates.\nNanometer coordinates are the coordinates of a point in the original image volume, but in nanometers. This is equivalent to the voxel coordinate multiplied by the voxel resolution, with no further transformation applied. Mesh and skeleton vertices are stored in nanometer coordinates.\nTransformed coordinates reflect a trasnsformation that has been applied to the original image volume. This transformation is a rotation to make the pia surface as flat as possible, a translation to move the pial surface to y=0, and a scaling to bring coordinates into microns. Transformed coordinates are convenient for more accurate computations of depth and the pia-to-white-matter axis, but are not stored by default. A python package standard_transform helps convert data to and from transformed coordinates.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that in all of these coordinate systems (including Neuroglancer), the y axis increases with depth. This is a standard definition when working with images, but is the opposite of what you usually think of with points. Because of that, when plotting annotations or neuroanatomy in matplotlib, you will usually have to invert the y axis with ax.invert_yaxis().",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Coordinate System"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_07_coordinates.html#introduction",
    "href": "programmatic_access/em_py_07_coordinates.html#introduction",
    "title": "Coordinate System",
    "section": "",
    "text": "Because of the many different data representations, dealing with coordinates in the MICrONs data is not entirely simple and it is easy to make mistakes by converting between coordinate systems incorrectly.\nThere are three main coordinate systems that wind up getting used:\n\nVoxel coordinates are the coordinates of a point in the original image volume. These are the coordinates that are used to index into the volumes you can see in Neuroglancer, but each number has a potentially different unit. In the MICrONs data, a voxel is 4 nm wide in the x and y directions, and 40 nm long in the z direction. This means that a 1x1x1 micron cube would be represented by a 250x250x25 voxel span. Annotations (such as synapses) are stored in voxel coordinates.\nNanometer coordinates are the coordinates of a point in the original image volume, but in nanometers. This is equivalent to the voxel coordinate multiplied by the voxel resolution, with no further transformation applied. Mesh and skeleton vertices are stored in nanometer coordinates.\nTransformed coordinates reflect a trasnsformation that has been applied to the original image volume. This transformation is a rotation to make the pia surface as flat as possible, a translation to move the pial surface to y=0, and a scaling to bring coordinates into microns. Transformed coordinates are convenient for more accurate computations of depth and the pia-to-white-matter axis, but are not stored by default. A python package standard_transform helps convert data to and from transformed coordinates.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that in all of these coordinate systems (including Neuroglancer), the y axis increases with depth. This is a standard definition when working with images, but is the opposite of what you usually think of with points. Because of that, when plotting annotations or neuroanatomy in matplotlib, you will usually have to invert the y axis with ax.invert_yaxis().",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Coordinate System"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_07_coordinates.html#standard-transform",
    "href": "programmatic_access/em_py_07_coordinates.html#standard-transform",
    "title": "Coordinate System",
    "section": "Standard Transform",
    "text": "Standard Transform\nStandard Transform is a python package designed to convert voxel and nanometer coordinates to transformed coordinates, with particular emphasis on the MICrONs data.\n\nInstallation\nStandard Transform can be installed from pip: pip install standard_transform\n\n\nWhy use Standard Transform?\nLet’s look at the coordinates of every excitatory neuron in the MICrONs data to see why we might want to use transformed coordinates.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom caveclient import CAVEclient\n\nclient = CAVEclient('minnie65_public')\n\nct_df = client.materialize.query_table('aibs_soma_nuc_metamodel_preds_v117', split_positions=True)\n\n# convert to nanometers\nct_df['pt_position_x_nm'] = ct_df['pt_position_x'] * 4\nct_df['pt_position_y_nm'] = ct_df['pt_position_y'] * 4\nct_df['pt_position_z_nm'] = ct_df['pt_position_z'] * 40\n\nfig, ax = plt.subplots(figsize=(5,4))\nsns.scatterplot(\n    x='pt_position_x_nm',\n    y='pt_position_y_nm',\n    s=3,\n    data=ct_df.sample(10_000), # Pick a random sample of 10,000 points, enough to see the shape of the data.\n    ax=ax,\n)\n\n# Important to flip the y axis to have y increase with depth!\nax.invert_yaxis()\nsns.despine(ax=ax)\n\n\n\n\n\n\n\n\nThis is just the raw positions, converted to nanometers. You can see a few aspects that might make this hard to work with.\nFirst, if you look along the top of the data, you can see that the pia surface is not flat. In fact, there’s a roughly 5 degree slope from the left to the right. Second, if you look at the location of the pia surface, it’s at around y = 0.4 * 10^6.\nNot only are these units large, the offset is arbitrary and it would make much more sense to anchor y=0 to the pial surface.\nLet’s see how we can do this with standard_transform.\n\nfrom standard_transform import minnie_ds\nimport numpy as np\n\nX_transformed = minnie_ds.transform_vx.apply_dataframe('pt_position', ct_df)\nX_transformed = np.array(X_transformed)\nct_df['pt_xt'] = X_transformed[:,0]\nct_df['pt_yt'] = X_transformed[:,1]\nct_df['pt_zt'] = X_transformed[:,2] \n\nfig, ax = plt.subplots(figsize=(5,4))\n\nsns.scatterplot(\n    x='pt_xt',\n    y='pt_yt',\n    s=3,\n    data=ct_df.sample(10_000),\n    ax=ax,\n)\n\nax.invert_yaxis()\nsns.despine(ax=ax)\n\n\n\n\n\n\n\n\nNow you can see that the surface is much more aligned with the x-axis, and the cells in layer 1 start just below y=0. In addition, the units have been converted to microns, which is much more readable and more consistent with measurements in other modalities.\n\n\nHow to use Standard Transform.\nA number of examples are available in the standard_transform readme.\n\nTransforming points\nThe main functions when working with point data are transform_vx and transform_nm, which transform from voxel coordinates and from nanometer coordinates respectively. In the above example, you will note that because we were working with annotations, we used the voxel coordinate transform.\nEach of the two transforms has the same functions that can be used:\n\nminnie_ds.{transform}.apply(x): This converts an Nx3 array of points from the original space to an Nx3 array of transformed coordinates.\nminnie_ds.{transform}.apply_project(projection_axis, x): This converts an Nx3 array of points from the original space to an N-length array of transformed coordinates, taking only values along the given axis (x, y, or z). This is typically done along the y axis to get the depth of each point.\nminnie_ds.{transform}.apply_dataframe(column_name, df, Optional[projection_axis]): This takes points from a dataframe column (or collection of three dataframe columns named {column_name}_x, {column_name}_y, and {column_name}_z) and converts them to transformed coordinates. If projection_axis is given, it will only return the values along that axis.\n\nWhere {transform} is either transform_vx or transform_nm.\nYou can also invert a transformation, for example if you want to convert transformed coordinates back to voxel coordinates to view in Neuroglancer.\n\nminnie_ds.{transform}.invert(X): This maps from an Nx3 array in the transformed space back to the original space (voxel or nanometer coordiantes, depending on which transform you used).\n\n\n\nTransforming meshes and skeletons\nStandard transform has the capability to transform MeshParty skeletons, MeshWorks, and MeshWork annotations using these same transforms as above.\nFor example, to transform all the vertices of meshwork object:\n\nnrn_transformed = minnie_ds.transform_nm.apply_meshwork_vertices(nrn)\n\nHowever, Meshwork objects also have annotations in dataframes as well as vertices. In order to transform these points, we need to specify both the name of the dataframe table and the columns to transform as a dictionary. For example, if you want to also remap the ctr_pt_position values from the pre_syn table and post_syn tables (which are in voxels by default), you would use:\n\nanno_dict={'pre_syn': 'ctr_pt_position', 'post_syn': 'ctr_pt_position'}\nnrn_transformed = minnie_ds.transform_vx.apply_meshwork_annotations(nrn_transformed, anno_dict, inplace=True)\n\nNote that without the inplace=True parameter, these meshwork transformations will return a new object, while ifinplace=True the original object is modified.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Coordinate System"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_05_meshes.html",
    "href": "programmatic_access/em_py_05_meshes.html",
    "title": "Meshes",
    "section": "",
    "text": "When trying to understand the fine 3d morphology of a neuron (e.g. features under 1 micron in scale), meshes are a particularly useful representation. More precisely, a mesh is a collection of vertices and faces that define a 3d surface. The exact meshes that one sees in Neuroglancer can also be loaded for analysis and visualization in other tools.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Meshes"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_05_meshes.html#downloading-meshes",
    "href": "programmatic_access/em_py_05_meshes.html#downloading-meshes",
    "title": "Meshes",
    "section": "Downloading Meshes",
    "text": "Downloading Meshes\nThe easiest tool for downloading MICrONs meshes is Meshparty, which is a python module that can be installed with pip install meshparty. Documentation for Meshparty can be found here.\nOnce installed, the typical way of getting meshes is by using a “MeshMeta” client that is told both the internet location of the meshes (cv_path) and a local directory in which to store meshes (disk_cache_path). Once initialized, the MeshMeta client can be used to download meshes for a given segmentation using its root id (seg_id). The following code snippet shows how to download an example mesh using a directory “meshes” as the local storage folder.\n\n\n\n\n\n\nInitial Setup\n\n\n\nBefore using any programmatic access to the data, you first need to set up your CAVEclient token.\n\n\n\nimport os\nfrom meshparty import trimesh_io\nfrom caveclient import CAVEclient\nclient = CAVEclient('minnie65_public')\n\nmm = trimesh_io.MeshMeta(\n  cv_path=client.info.segmentation_source(),\n  disk_cache_path=\"meshes\",\n)\n\nroot_id = 864691135014128278\nmesh = mm.mesh(seg_id=root_id)\n\nWarning: deduplication not currently supported for this layer's variable layered draco meshes\n\n\nOne convenience of using the MeshMeta approach is that if you have already downloaded a mesh for with a given root id, it will be loaded from disk rather than re-downloaded.\nIf you have to download many meshes, it is somewhat faster to use the bulk download_meshes function and use multiple threads via the n_threads argument. If you download them to the same folder used for the MeshMeta object, they can be loaded through the same interface.\n\nroot_ids = [864691135014128278, 864691134940133219]\nmm = trimesh_io.download_meshes(\n    seg_ids=root_ids,\n    target_dir='meshes',\n    cv_path=client.info.segmentation_source(),\n    n_threads=4, # Or whatever value you choose above one but less than the number of cores on your computer\n)\n\n\n\n\n\n\n\nFile size\n\n\n\nMeshes can be hundresds of megabytes in size, so be careful about downloading too many if the internet is not acting well or your computer doesn’t have much disk space!",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Meshes"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_05_meshes.html#healing-mesh-gaps",
    "href": "programmatic_access/em_py_05_meshes.html#healing-mesh-gaps",
    "title": "Meshes",
    "section": "Healing Mesh Gaps",
    "text": "Healing Mesh Gaps\n\n\n\nExample of a continuous neuron whose mesh has a gap\n\n\nMany meshes are not actually fully continuous due to small gaps in the segmentation. However, information collected during proofreading allows one to partially repair these gaps by adding in links where the segmentation was merged across a small gap. If you are just visualizaing a mesh, these gaps are not a problem, but if you want to do analysis on the mesh, you will want to heal these gaps. Conveniently, there’s a function to do this:\n\nmesh.add_link_edges(\n    seg_id=864691134940133219, # This needs to be the same as the root id used to download the mesh\n    client=client.chunkedgraph,\n)",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Meshes"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_05_meshes.html#properties",
    "href": "programmatic_access/em_py_05_meshes.html#properties",
    "title": "Meshes",
    "section": "Properties",
    "text": "Properties\nMeshes have a large number of properties, many of which come from being based on the Trimesh library’s mesh format, and others being specific to MeshParty.\nSeveral of the most important properties are:\n\nmesh.vertices : An N x 3 list of vertices and their 3d location in nanometers, where N is the number of vertices.\nmesh.faces : An P x 3 list of integers, with each row specifying a triangle of connected vertex indices.\nmesh.edges : An M x 2 list of integers, with each row specifying a pair of connected vertex indices based off of faces.\nmesh.edges : An M x 2 list of integers, with each row specifying a pair of connected vertex indices based off of faces.\nmesh.link_edges : An M_l x 2 list of integers, with each row specifying a pair of “link edges” that were used to heal gaps based on proofreading edits.\nmesh.graph_edges : An (M+M_l) x 2 list of integers, with each row specifying a pair of graph edges, which is the collection of both mesh.edges and mesh.link_edges.\nmesh.csgraph : A Scipy Compressed Sparse Graph representation of the mesh as an NxN graph of vertices connected to one another using graph edges and with edge weights being the distance between vertices. This is particularly useful for computing shortest paths between vertices.\n\n\nMICrONs meshes are not generally “watertight”, a property that would enable a number of properties to be computed natively by Trimesh. Because of this, Trimesh-computed properties relating to solid forms or volumes like mesh.volume or mesh.center_mass do not have sensible values and other approaches should be taken. Unfortunately, because of the Trimesh implementation of these properties it is up to the user to be aware of this issue.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Meshes"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_05_meshes.html#visualization",
    "href": "programmatic_access/em_py_05_meshes.html#visualization",
    "title": "Meshes",
    "section": "Visualization",
    "text": "Visualization\nThere are a variety of tools for visualizing meshes in python. MeshParty interfaces with VTK, a powerful but complex data visualization library that does not always work well in python. The basic pattern for MeshParty’s VTK integration is to create one or more “actors” from the data, and then pass those to a renderer that can be displayed in an interactive approach. The following code snippet shows how to visualize a mesh using this approach.\n\nmesh_actor = trimesh_vtk.mesh_actor(\n  mesh,\n  color=(1,0,0),\n  opacity=0.5,\n)\ntrimesh_vtk.render_actors([mesh_actor])\n\nNote that by default, neurons will appear upside down because the coordinate system of the dataset has the y-axis value increasing along the “downward” pia to white matter axis.\nMore documentation on the MeshParty VTK visualization can be found here.\nOther tools worth exploring are:\n\nPyVista\nPolyscope\nVedo\nMeshLab\nIf you have some existing experience, Blender (see Blender integration by our friends behind Navis, a fly-focused framework analyzing connectomics data).",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Meshes"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_05_meshes.html#masking",
    "href": "programmatic_access/em_py_05_meshes.html#masking",
    "title": "Meshes",
    "section": "Masking",
    "text": "Masking\nOne of the most common operations on meshes is to mask them to a particular region of interest. This can be done by “masking” the mesh with a boolean array of length N where N is the number of vertices in the mesh, with True where the vertex should be kept and False where it should be omitted. There are several convenience functions to generate common masks in the Mesh Filters module.\nIn the following example, we will first mask out all vertices that aren’t part of the largest connected component of the mesh (i.e. get rid of floating vertices that might arise due to internal surfaces) and then mask out all vertices that are more than 20,000 nm away from the soma center.\n\nfrom meshparty import mesh_filters\n\nroot_id =864691134940133219 \nroot_point = client.materialize.tables.nucleus_detection_v0(pt_root_id=root_id).query()['pt_position'].values[0] * [4,4,40]  # Convert the nucleus location from voxels to nanometers via the data resolution.\n\nmesh = mm.mesh(seg_id=root_id)\n# Heal gaps in the mesh\nmesh.add_link_edges(\n    seg_id=864691134940133219,\n    client=client.chunkedgraph,\n)\n\n# Generate and use the largest component mask\ncomp_mask = mesh_filters.filter_largest_component(mesh)\nmask_filt = mesh.apply_mask(comp_mask)\n\nsoma_mask = mesh_filters.filter_spatial_distance_from_points(\n    mask_filt,\n    root_point,\n    20_000, # Note that this is in nanometers\n)\nmesh_soma = mesh.apply_mask(soma_mask)\n\nThis resulting mesh is just a small cutout around the soma.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Meshes"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_03_annotation_tables.html",
    "href": "programmatic_access/em_py_03_annotation_tables.html",
    "title": "Annotation Tables",
    "section": "",
    "text": "The minnie65_public data release includes a number of annotation tables that help label the dataset. This section describes the content of each of these tables — see here for instructions for how to query and filter tables.\nUnless otherwise specificied (i.e. via desired_resolution), all positions are in units of 4,4,40 nm/voxel resolution.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_03_annotation_tables.html#common-fields",
    "href": "programmatic_access/em_py_03_annotation_tables.html#common-fields",
    "title": "Annotation Tables",
    "section": "Common Fields",
    "text": "Common Fields\nSeveral fields (or column names) are common to many tables. These fall into two main classes: the spatial point columns that are how we assign annotations to cells via points in the 3d space and book-keeping columns, that are used internally to track the state of the data.\n\nSpatial Point Columns\nMost tables have one or more Bound Spatial Points, which is a location in the 3d space that tells the annotation to remain associated with the root id at that location.\nBound spatial points have will have one prefix, usually pt (i.e. “point”) and three associated columns with different suffixes: _position, _supervoxel_id, and _root_id.\nFor a given prefix {pt}, the three columns are as follows:\n\nThe {pt}_position indicates the location of the point in 3d space.\nThe {pt}_supervoxel_id indicates a unique identifier in the segmentation, and is mostly internal bookkeeping.\nThe {pt}_root_id indicates the root id of the annotation at that location.\n\n\n\nBook-keeping Columns\nSeveral columns are common to many or all tables, and mostly used as internal book-keeping. Rather than describe these for every table, they will just be mentioned briefly here:\n\nCommon columns\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nA unique ID specific to the annotation within that table\n\n\ncreated\nInternal bookkeeping column, should always be t for data you can download\n\n\nvalid\nA unique ID specific to the annotation within that table\n\n\ntarget_id\nSome tables reference other tables, particularly the nucleus table. If present, this column will be the same as id\n\n\ncreated_ref / valid_ref / id_ref (optional)\nFor reference tables, the data shows both the created/valid/id of the reference annotation and the target annotation. The values with the _ref suffix are those of the reference table (usually something like proofreading state or cell type) and the values without a suffix ar ethose of the target table (usually a nucleus)",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_03_annotation_tables.html#synapse-table",
    "href": "programmatic_access/em_py_03_annotation_tables.html#synapse-table",
    "title": "Annotation Tables",
    "section": "Synapse Table",
    "text": "Synapse Table\nTable name: synapses_pni_v2\nThe only synapse table is synapses_pni_v2. This is by far the largest table in the dataset with 337 million entries, one for each synapse. It contains the following columns (in addition to the bookkeeping columns):\n\nSynapse table column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npre_pt_position / pre_pt_supervoxel_id / pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse.\n\n\npost_pt_position / post_pt_supervoxel_id / post_pt_root_id\nIThe bound spatial point data for the postsynaptic side of the synapse.\n\n\nsize\nThe size of the synapse in voxels. This correlates well, but not perfectly, with the surface area of synapse.\n\n\nctr_pt_position\nA position in the center of the detected synaptic junction. Of all points in the synapse table, this is usually the closest point to the surface (and thus mesh) of both neurons. Because it is at the edge of cells, it is not associated with a root id.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_03_annotation_tables.html#nucleus-tables",
    "href": "programmatic_access/em_py_03_annotation_tables.html#nucleus-tables",
    "title": "Annotation Tables",
    "section": "Nucleus tables",
    "text": "Nucleus tables\nThe ‘nucleus centroid’ of a cell is unlikely to change with proofreading, and so is a useful static identifier for a given cell. The results of automatic nucleus segmentation and neuron-detection are avialable in the following tables. These tables are often the ‘reference’ table for other annotations.\n\nNucleus Detection Table\nTable name: nucleus_detection_v0\nNucleus detection has been used to define unique cells in the dataset. Distinct from the neuronal segmentation, a convolutional neural network was trained to segment nuclei. Each nucleus detection was given a unique ID, and the centroid of the nucleus was recorded as well as its volume. Many other tables in the dataset are reference tables on nucleus_detection_v0, meaning they are linked by the same annotation id. The id of the segmented nucelus, a 6-digit integer, is static across data versions and for this reason is the preferred method to identify the same ‘cell’ across time.\nThe key columns of nucleus_detection_v0 are:\n\nNucleus table column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\n6-digit number of the segmentation for that nucleus; ‘nucleus ID’\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\n\nNote that the id column is the nucleus ID, also called the ‘soma ID’ or the ‘cell ID’.\n\n\nNeuron-Nucleus Table\nTable name: nucleus_ref_neuron_svm\nWhile the table of centroids for all nuclei is nucleus_detection_v0, this includes neuronal nuclei, non-neuronal nuclei, and some erroneous detections. The table nucleus_ref_neuron_svm shows the results of a classifier that was trained to distinguish neuronal nuclei from non-neuronal nuclei and errors. For the purposes of analysis, we recommend using the nucleus_ref_neuron_svm table to get the most broad collection of neurons in the dataset.\nThe key columns of nucleus_ref_neuron_svm are:\n\nNucleus table column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\nclassification-system\nDescribes how the classification was done. All values will be is_neuron for this table\n\n\ncell_type\nThe output of the classifier. All values will be either neuron or not-neuron (glia or error) for this table\n\n\n\nNote that the id column is the same as the nucleus id.\n\n\nNucleus brain area assignment\nGiven the nucleus detection table nucleus_detection_v0 and the transformation of 2-photon in vivo imaging to the EM structural space (see functional-coregistration-tables ), each cell in the volume has been assigned to one of four visual cortical areas.\nThe inferred functional brain area based on the position of the nucleus in the EM volume. Area boundaries estimated from the area-membership assignments of the 2P recorded cells, after transformation to EM space.\nThe table nucleus_functional_area_assignment is a reference table on nucleus_detection_v0 and adds the following columns:\n\nFunctional area assignment\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\ntag\nthe brain area label (one of V1, AL, RL, LM)\n\n\nvalue\nthe distance to the area boundary (in um), calculated as the mean-distance of the 10 nearest neighbors in a non-matching brain area. A larger value is further from the area boundaries, and can be interpretted as higher confidence in area assignment.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_03_annotation_tables.html#cell-type-tables",
    "href": "programmatic_access/em_py_03_annotation_tables.html#cell-type-tables",
    "title": "Annotation Tables",
    "section": "Cell Type Tables",
    "text": "Cell Type Tables\nThere are several tables that contain information about the cell type of neurons in the dataset, with each table representing a different method of doing the classificaiton. Because each method requires a different kind of information, not all cells are present in all tables. Each of the cell types tables has the same format and in all cases the id column references the nucleus id of the cell in question.\n\nManual Cell Types (V1 Column)\nTable name: allen_v1_column_types_slanted_ref and aibs_column_nonneuronal_ref\nA subset of nucleus detections in a 100 um column (n=2204) in VISp were manually classified by anatomists at the Allen Institute into categories of cell subclasses, first distinguishing cells into classes of non-neuronal, excitatory and inhibitory.\nThe key columns are:\n\nAIBS Manual Cell Types, V1 COlumn\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nOne of aibs_coarse_excitatory or aibs_coarse_inhibitory for detected neurons, or aibs_coarse_nonneuronal for non-neurons (glia/pericytes).\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\nThis is a reference table on nucleus_detection_v0. The cell types in the table are:\n\n\n\n\n\n\nManual Cell Types (neurons)\n\n\n\n\n\n\nAIBS Manual Cell Type definitions (neurons)\n\n\n\n\n\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\n23P\nExcitatory\nLayer 2/3 cells\n\n\n4P\nExcitatory\nLayer 4 cells\n\n\n5P-IT\nExcitatory\nLayer 5 intratelencephalic cells\n\n\n5P-ET\nExcitatory\nLayer 5 extratelencephalic cells\n\n\n5P-NP\nExcitatory\nLayer 5 near-projecting cells\n\n\n6P-IT\nExcitatory\nLayer 6 intratelencephalic cells\n\n\n6P-CT\nExcitatory\nLayer 6 corticothalamic cells\n\n\nBC\nInhibitory\nBasket cell\n\n\nBPC\nInhibitory\nBipolar cell. In practice, this was used for all cells thought to be VIP cell, not only those with a bipolar dendrite\n\n\nMC\nInhibitory\nMartinotti cell. In practice, this label was used for all inhibitory neurons that appeared to be Somatostatin cell, not only those with a Martinotti cell morphology\n\n\nUnsure\nInhibitory\nUnsure. In practice, this label also is used for all likely-inhibitory neurons that did not match other types\n\n\n\n\n\n\n\n\n\n\n\n\nManual Cell Types (non-neurons)\n\n\n\n\n\n\nAIBS Manual Cell Type definitions (non-neurons)\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\nOPC\nNon-neuronal\nOligodendrocyte precursor cell\n\n\nastrocyte\nNon-neuronal\nAstrocyte\n\n\nmicroglia\nNon-neuronal\nMicroglia\n\n\npericyte\nNon-neuronal\nPericyte\n\n\noligo\nNon-neuronal\nOligodendrocyte\n\n\n\n\n\n\n\n\nPredictions from soma/nucleus features\nTable name: aibs_metamodel_celltypes_v661\nThis table contains the results of a hierarchical classifier trained on features of the cell body and nucleus of cells. This was applied to most cells in the dataset that had complete cell bodies (e.g. not cut off by the edge of the data). For more details, see Elabbady et al. 2022. In general, this does a good job, but sometimes confuses layer 5 inhibitory neurons as being excitatory:\nThe key columns are:\n\nAIBS Soma Nuc Metamodel Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nOne of excitatory_neuron or inhibitory_neuron for detected neurons, or nonneuron for non-neurons (glia/pericytes).\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\nThis is a reference table on nucleus_detection_v0, with small-objects and multi-soma errors removed. The model was run with cell-based features as of version 661 of the dataset. The cell types in the table are:\n\n\n\n\n\n\nSoma Nuc Metamodel Cell types\n\n\n\n\n\n\nAIBS Soma Nuc Metamodel: Cell Type definitions\n\n\n\n\n\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\n23P\nExcitatory\nLayer 2/3 cells\n\n\n4P\nExcitatory\nLayer 4 cells\n\n\n5P-IT\nExcitatory\nLayer 5 intratelencephalic cells\n\n\n5P-ET\nExcitatory\nLayer 5 extratelencephalic cells\n\n\n5P-NP\nExcitatory\nLayer 5 near-projecting cells\n\n\n6P-IT\nExcitatory\nLayer 6 intratelencephalic cells\n\n\n6P-CT\nExcitatory\nLayer 6 corticothalamic cells\n\n\nBC\nInhibitory\nBasket cell\n\n\nBPC\nInhibitory\nBipolar cell. In practice, this was used for all cells thought to be VIP cell, not only those with a bipolar dendrite\n\n\nMC\nInhibitory\nMartinotti cell. In practice, this label was used for all inhibitory neurons that appeared to be Somatostatin cell, not only those with a Martinotti cell morphology\n\n\nNGC\nInhibitory\nNeurogliaform cell. In practice, this label also is used for all inhibitory neurons in layer 1, many of which may not be neurogliaform cells although they might be in the same molecular family\n\n\nOPC\nNon-neuronal\nOligodendrocyte precursor cell\n\n\nastrocyte\nNon-neuronal\nAstrocyte\n\n\nmicroglia\nNon-neuronal\nMicroglia\n\n\npericyte\nNon-neuronal\nPericyte\n\n\noligo\nNon-neuronal\nOligodendrocyte\n\n\n\n\n\n\nPrevious versions of this table include: aibs_soma_nuc_metamodel_preds_v117 (run on a subset of data, the V1 column) and aibs_soma_nuc_exc_mtype_preds_v117 (using training data labeled by another classifier: see mtypes below).\n\n\nCoarse prediction from spine detection\nTable name: baylor_log_reg_cell_type_coarse_v1\nThis table contains the results of a logistic regression classifier trained on properties of neuronal dendrites. This was applied to many cells in the dataset, but required more data than soma and nucleus features alone and thus more cells did not complete the pipeline. It has very good performance on excitatory vs inhibitory neurons because it focuses on dendritic spines, a characteristic property of excitatory neurons. It is a good table to double check E/I classifications if in doubt.\nThe key columns are:\n\nBaylor Dendrite Feature Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nbaylor_log_reg_cell_type_coarse for all entries\n\n\ncell_type\nexcitatory or inhibitory\n\n\n\n\n\nFine prediction from dendritic features\nTable name: aibs_metamodel_mtypes_v661_v2\nThis table contains all detected neurons across the dataset,\nExcitatory neurons and inhibitory neurons were distinguished with the soma_nucleus model above, and subclasses were assigned based on a data-driven clustering of the neuronal features. Inhibitory neurons were classified based on how they distributed they synaptic outputs onto target cells, while exictatory neurons were classified based on a collection of dendritic features.\nFor more details, see the section on the minnie column or read the preprint (Schneider-Mizell et al. 2023).\nNote that all cell-type labels in this table come from a clustering specific to this paper, and while they are intended to align with the broader literature they are not a direct mapping or a well-established convention.\nFor a more conventional set of labels on the same set of cells, look at the manual table allen_v1_column_types_slanted_ref. Cell types in that table align with those in the aibs_metamodel_celltypes_v661 classifier above.\nThe key columns are:\n\nColumn M-type Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nexcitatory or inhibitory\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\nThis is a reference table on nucleus_detection_v0, with non-neuronal objects removed. The model was run with cell-based features as of version 661 of the dataset. The cell types in the table are:\nThe cell types in the table are:\n\n\n\n\n\n\nM-type Cell Type definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\nL2a\nExcitatory\nA cluster of layer 2 (upper layer 2/3) excitatory neurons\n\n\nL2b\nExcitatory\nA cluster of layer 2 (upper layer 2/3) excitatory neurons\n\n\nL3a\nExcitatory\nA cluster of excitatory neurons transitioning between upper and lower layer 2/3\n\n\nL3b\nExcitatory\nA cluster of layer 3 (upper layer 2/3) excitatory neurons\n\n\nL3c\nExcitatory\nA cluster of layer 3 (upper layer 2/3) excitatory neurons\n\n\nL4a\nExcitatory\nThe largest cluster of layer 4 excitatory neurons\n\n\nL4b\nExcitatory\nAnother cluster of layer 4 excitatory neurons\n\n\nL4c\nExcitatory\nA cluster of layer 4 excitatory neurons along the border with layer 5\n\n\nL5a\nExcitatory\nA cluster of layer 5 IT neurons at the top of layer 5\n\n\nL5b\nExcitatory\nA cluster of layer 5 IT neurons throughout layer 5\n\n\nL5ET\nExcitatory\nThe cluster of layer 5 ET neurons\n\n\nL5NP\nExcitatory\nThe cluster of layer 5 NP neurons\n\n\nL6a\nExcitatory\nA cluster of layer 6 IT neurons at the top of layer 6\n\n\nL6b\nExcitatory\nA cluster of layer 6 IT neurons throughout layer 6. Note that this is different than the label “Layer 6b” which refers to a narrow band at the border between layer 6 and white matter\n\n\nL6c\nExcitatory\nA cluster of tall layer 6 cells (unsure if IT or CT)\n\n\nL6CT\nExcitatory\nA cluster of tall layer 6 cells matching manual CT labels\n\n\nL6wm\nExcitatory\nA cluster of layer 6 cells along the border with white matter\n\n\nPTC\nInhibitory\nPerisomatic targeting cells, a cluster of inhibitory neurons that target the soma and proximal dendrites of excitatory neurons. Approximately corresponds to basket cell\n\n\nDTC\nInhibitory\nDendrite targeting cells, a cluster of inhibitory neurons that target the distal dendrites of excitatory neurons. Most SST cells would be DTC\n\n\nSTC\nInhibitory\nSparsely targeting cells, a cluster of inhibitory neurons that don’t concentrate multiple synapses onto the same target neurons. Many neurogliaform cells and layer 1 interneurons fall into this category\n\n\nITC\nInhibitory\nInhibitory targeting cells, a cluster of inhibitory neurons that preferntially target other inhibitory neurons. Most VIP cells would be ITCs\n\n\n\n\n\n\nPrevious versions of this table include: allen_column_mtypes_v1 (run on a subset of data, the V1 column)",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_03_annotation_tables.html#proofreading-tables",
    "href": "programmatic_access/em_py_03_annotation_tables.html#proofreading-tables",
    "title": "Annotation Tables",
    "section": "Proofreading Tables",
    "text": "Proofreading Tables\n\nProofreading Status and Strategy\nTable name: proofreading_status_and_strategy\nThe table proofreading_status_and_strategy describes the status of cells that have undergone manual proofreading.\nBecause of the inherent difference in the difficulty and time required for different kinds of proofreading, we describe the status of axons and dendrites separately.\nEach compartment status may be either:\n\nFALSE: indicates no comprehensive proofreading has been performed, or is not applicable.\n\nTRUE: indicates that false merges have been comprehensively removed, and the compartment is at least ‘clean’. Consult the strategy column if completeness of the compartment is relevant to your research.\n\nAn axon or dendrite labeled as status=TRUE can be trusted to be correct, but may not be complete. The degree of completion can be read from the strategy column. For more information, please see Proofreading and Data Quality; or also the microns-explorer page on proofreading strategies.\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made. NOTE: if this does not match the pt_root_id then the cell has undergone further changes. This is usually and improvement in proofreading, but proceed with caution.\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. May be TRUE or FALSE\n\n\nstatus_axon\nThe status of the axon proofreading. May be TRUE or FALSE\n\n\nstrategy_dendrite\nThe strategy employed to proofread the dendrite. See strategy table below for details\n\n\nstrategy_axon\nThe strategy employed to proofread the axon. See strategy table below for details\n\n\n\nThe specific strategies are as follows (and will update over time):\n\nProofreading Strategies\n\nProofreading Strategy Table\n\n\n\n\n\n\nStrategy\n\n\n\n\n\nnone\nNo cleaning, and no extension. Indicates an entry in proofreading_status that is FALSE for that compartment\n\n\ndendrite_clean\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. The dendrite may be incorrectly truncated by segmentation error. Not all dendrite tips have been checked for extension. No comprehensive attempt was made to re-attach spine heads.\n\n\ndendrite_extended\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. Every tip was identified, manually inspected, and extended if possible. No comprehensive attempt was made to re-attach spine heads.\n\n\naxon_column_truncated\nAThe axon was extended within the V1 cortical column, with a preference for local connections. In some cases the axon was cut at the column boundary and/or the layer boundary, especially the boundary between layers 2/3 and layer 4. Output synapses represent a sampling of potential partners\n\n\naxon_interareal\nThe axon was extended with a preference for branches that projected to other brain areas. Some axon branches were fully extended, but local connections may be incomplete. Output synapses represent a sampling of potential partners.\n\n\naxon_partially_extended\nThe axon was extended outward from the soma, following each branch to its termination. Output synapses represent a sampling of potential partners.\n\n\naxon_fully_extended\nAxon was extended outward from the soma, following each branch to its termination. After initial extension, every endpoint was identified, manually inspected, and extended again if possible. Output synapses represent a largely complete sampling of partners.\n\n\n\nThis table, proofreading_status_and_strategy, supercedes proofreading_status_public_release.\n\n\n\n\n\n\nProofreading status at public release\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis table is out-of-date, and remains here for convenient reference\n\n\nTable name: proofreading_status_public_release\nThe table proofreading_status_public_release describes the status of cells selected for manual proofreading.\nBecause of the inherent difference in the challenge and time required for different kinds of proofreading, we describe the status of axons and dendrites separately. Further, we distinguish three different categories of proofreading:\n\nnon: No proofreading has been comprehensively performed.\nclean: Proofreading has comprehensively removed false merges, but not necessarily added missing parts.\nextended: Proofreading has comprehensively removed false merges and attempted to add all or most missing parts.\n\nThe key columns are:\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. One of the three categories described above\n\n\nstatus_axon\nThe status of the axon proofreading. One of the three categories described above",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_03_annotation_tables.html#functional-coregistration-tables",
    "href": "programmatic_access/em_py_03_annotation_tables.html#functional-coregistration-tables",
    "title": "Annotation Tables",
    "section": "Functional Coregistration Tables",
    "text": "Functional Coregistration Tables\nTo relate the structural data to functional data, cell bodies must be coregistered between the functional imaging and EM volumes. The results of this coregistration are stored in two tables with the same columns:\n\ncoregistration_manual_v4 : The results of manually verified coregistration. This table is well-verified, but contains fewer {term}ROIs (N=15,352 root ids, 19,181 ROIs).\ncoregistration_auto_phase3_fwd_apl_vess_combined : The results of automated functional matching between the EM and 2-p functional data. This table is not manually verified, but contains more {term}ROIs (N=84,233 ROIs). This table reconciles the following two tables that both make a best match of the of registration using different techniques: coregistration_auto_phase3_fwd and apl_functional_coreg_vess_fwd.\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease see the Functional Data section for more information about using this data.\nfix broken link\n\n\nThe column descriptions are:\n\nCoregistration table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nsession\nThe session index from functional imaging\n\n\nscan_idx\nThe scan index from functional imaging\n\n\nunit_id\nThe functional unit index from imaging. Only unique within scan and session\n\n\nfield\nThe field index from functional imaging\n\n\nresidual\nThe residual distance between the functional and the assigned structural points after transformation, in microns\n\n\nscore\nA separation score, measuring the difference between the residual distance to the assigned neuron and the distance to the nearest non-assigned neuron, in microns. This can be negative if the non-assigned neuron is closer than the assigned neuron. Larger values indicate fewer nearby neurons that could be confused with the assigned neuron.\n\n\n\nPrevious versions of this analysis include the following tables:\n\ncoregistration_manual_v3 : The results of manually verified coregistration. This table is well-verified, but contains fewer {term}ROIs (N=12,052 root ids, 13,925 ROIs).\napl_functional_coreg_forward_v5 : The results of automated functional matching between the EM and 2-p functional data. This table is not manually verified, but contains more {term}ROIs (N=36,078 root ids, 68,873 ROIs).\n\n\nFunctional properties\nA summary of the functional properties for each of the coregistered neurons (as of coregistration_manual_v3) are available for convenience.\nThe table functional_properties_v3_bcm is a reference table on the nucleus_detection_v0, and adds the following columns\n\nFuctional properties of coregistered neurons\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\nsession\nThe session index from functional imaging\n\n\nscan_idx\nThe scan index from functional imaging\n\n\nunit_id\nThe functional unit index from imaging. Only unique within scan and session\n\n\npref_ori\npreferred orientation in radians (0 - pi), horizontal bar moving upward is 0 and orientation increases clockwise, extracted from model responses to oriented noise stimuli\n\n\npref_dir\npreferred direction in radians (0 - 2pi), horizontal bar moving upward is 0 and orientation increases clockwise, extracted from model responses to oriented noise stimuli\n\n\ngOSI\nglobal orientation selectivity index\n\n\ngDSI\nglobal direction selectivity index\n\n\ncc_abs\nprediction performance of the model, higher is better",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_03_annotation_tables.html#overview-of-relevant-tables",
    "href": "programmatic_access/em_py_03_annotation_tables.html#overview-of-relevant-tables",
    "title": "Annotation Tables",
    "section": "Overview of relevant tables",
    "text": "Overview of relevant tables\n\nheard you like tables–here’s a table for your tables\n\n\n\n\n\n\n\nTable Name\nNumber of Annotations\nDescription\n\n\n\n\nsynapses_pni_v2\n337,312,429\nThe locations of synapses and the segment ids of the pre and post-synaptic automated synapse detection\n\n\nnucleus_detection_v0\n144,120\nThe locations of nuclei detected via a fully automated method\n\n\nnucleus_alternative_points\n8,388\nA reference annotation table marking alternative segment_id lookup locations for a subset of nuclei in nucleus_detection_v0 that is more accurate than the centroid location listed there\n\n\nnucleus_ref_neuron_svm\n144,120\nreference annotation indicating the output of a model detecting which nucleus detections are neurons versus which are not 1\n\n\ncoregistration_manual_v4\n19,181\nA table indicating the association between individual units in the functional imaging data and nuclei in the structural data, derived from human powered matching. Includes residual and separation scores to help assess confidence\n\n\ncoregistration_auto_phase3_fwd_apl_vess_combined\n84,233\nA table indicating the association between individual units in the functional imaging data and nuclei in the structural data, derived from the automated procedure. Includes residuals and separation scores to help assess confidence\n\n\nproofreading_status_and_strategy\n1421\nA table indicating which neurons have been proofread on their axons or dendrites\n\n\naibs_column_nonneuronal_ref\n542\nCell type reference annotations from a human expert of non-neuronal cells located amongst the Minnie Column\n\n\nallen_v1_column_types_slanted_ref\n1,357\nNeuron cell type reference annotations from human experts of neuronal cells located amongst the Minnie Column\n\n\nallen_column_mtypes_v1\n1,357\nNeuron cell type reference annotations from data driven unsupervised clustering of neuronal cells\n\n\naibs_metamodel_mtypes_v661_v2\n72,158\nReference annotations indicating the output of a model predicting cell types across the dataset based on the labels from allen_column_mtypes_v1.1\n\n\naibs_metamodel_celltypes_v661\n94,014\nReference annotations indicating the output of a model predicting cell classes based on the labels from allen_v1_column_types_slanted_ref and aibs_column_nonneuronal_ref\n\n\nbaylor_log_reg_cell_type_coarse_v1\n55,063\nReference annotations indicated the output of a logistic regression model predicting whether the nucleus is part of an excitatory or inhibitory cell\n\n\nbaylor_gnn_cell_type_fine_model_v2\n49,051\nReference annotations indicated the output of a graph neural network model predicting the cell type based on the human labels in allen_v1_column_types_slanted_ref\n\n\nproofreading_edits\n121,271\nA table containing the number of edits on every segment_id associated with a nucleus in the volume\n\n\nvortex_astrocyte_proofreading_status\n12\nThis table reports the status of a manually selected subset of astrocytes within the VISP column. Astrocyte seelection and proofreading performed as part of VORTEX.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_01_caveclient_setup.html",
    "href": "programmatic_access/em_py_01_caveclient_setup.html",
    "title": "CAVEclient Setup",
    "section": "",
    "text": "The Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data.\nThe CAVEclient is the main way to programmatically access the MICrONS data using Python.\nIn particular, the CAVEclient provides an interface to query the CAVE database for annotations such as synapses, as well as to get a variety of other kinds of information.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVEclient Setup"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_01_caveclient_setup.html#installation",
    "href": "programmatic_access/em_py_01_caveclient_setup.html#installation",
    "title": "CAVEclient Setup",
    "section": "Installation",
    "text": "Installation\nTo install caveclient, use pip: pip install caveclient.\nOnce you have installed caveclient, to use it you need to set up your user token in one of two ways:",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVEclient Setup"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_01_caveclient_setup.html#setting-up-credentials",
    "href": "programmatic_access/em_py_01_caveclient_setup.html#setting-up-credentials",
    "title": "CAVEclient Setup",
    "section": "Setting up credentials",
    "text": "Setting up credentials\nTo access the data programmatically, you need to set up a user token. This token is assigned by the server and functions as a both a username and password to access any aspect of the data. You will need to save this token to your computer using the tools.\n\nScenario 1: New User, No Previous Account\nIf you have never interacted with CAVE before, you will need to both create an account and get a token. Note that you can only have one token at a time, and thus if you create a new token any computer running a previous one will no longer have access.\nStep 1: Log into a CAVE site to set up a new account with a GMail-associated email address. To do this, click this link and acknowledge the terms of service associated with the MICrONS dataset. Once you have done this, your account is automatically created.\nStep 2: Generate a token. To generate a token, run the following code in a Jupyter notebook:\n\nfrom caveclient import CAVEclient\nclient = CAVEclient()\nclient.auth.setup_token(make_new=True)\n\nThis will open a new browser window and ask you to log in.\nYou will show you a web page with an alphanumeric string that is your token.\nCopy your token, and save it to your computer using the following:\n\nclient.auth.save_token(token=YOUR_TOKEN)\n\nNote that the token must be formatted as a string.\nTo check if your setup works, run the following:\n\nclient = CAVEclient('minnie65_public')\n\nIf you don’t get any errors, your setup has worked and you can move on!\n\n\nScenario 2: Existing user, New computer\nIf you have already created an account and token but not set up your computer yet, you can use the same token on a new computer.\nStep 1) Find your token by running the following code in a Jupyter notebook:\n\nfrom caveclient import CAVEclient\nclient = CAVEclient()\nclient.auth.setup_token(make_new=False)\n\nThis will open a new browser window and ask you to log in.\nAfter lgging in, the page will display your current token (the value after the token:).\nCopy this token, and save it to your computer using the following:\n\nclient.auth.save_token(token=YOUR_TOKEN, overwrite=True)\n\nNote that the token must be formatted as a string.\nTo check if your setup works, run the following:\n\nclient = CAVEclient('minnie65_public')\n\nIf you don’t get any errors, your setup has worked and you can move on!\n\n\nSomething went wrong?\nIf you are having trouble with authentication or permissions, it is probably because the token you are trying to use is not the one CAVE expects.\nTo find your current token, run the following code in a Jupyter notebook:\n\nclient = CAVEclient()\nclient.auth.token\n\nThe notebook will print the token your computer is sending to the server.\nIf this token is not the one you find from running the code in Scenario 2, follow the steps there to set up your computer with the correct token.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVEclient Setup"
    ]
  },
  {
    "objectID": "examples/skeleton_load_and_generate.html",
    "href": "examples/skeleton_load_and_generate.html",
    "title": "Neuron Skeletons",
    "section": "",
    "text": "Created 7/2/2024 by Bethanny Danskin\nMICrONS Tutorial for VORTEX\nDepends on the following two packages: CAVEclient and Skeleton-Plot (version&gt;0.0.9)\nSkeletons and meshworks are loaded as MeshParty meshwork object\npcg-skel and cloudvolume are necessary if generating your own skeletons from the meshes\n\nUsing CAVEclient requires having set up a CAVE auth token. See how to set up your CAVEclient token here.\n\n\n# !pip install caveclient\n# !pip install skeleton-plot\n\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n\nfrom caveclient import CAVEclient\n# Initialize a client for the \"minnie65_public\" datastack.\nclient = CAVEclient(datastack_name='minnie65_public') \n\n# see the available materialization versions\nclient.materialize.get_versions()\n\n# Uncomment to get more details on each release version\n# client.materialize.get_versions_metadata()\n\n[1078, 117, 661, 343, 795, 943]\n\n\n\n# set materialization version, for consistency\nmaterialization = 1078 # current public as of 6/5/2024\n# materialization = 661 # version at which skeletons were pre-generated\nclient.version = materialization\n\n\nimport skeleton_plot as skelplot\nimport skeleton_plot.skel_io as skel_io\n\n\n\nThe skeletons of the meshes are calculated at specific timepoints. The last collection of all neurons in the dataset was at materialization version 661.\nBoth the .swc skeletons and .h5 meshwork objects are available in the BossDB repository\n\n# path to the skeleton .swc files\nskel_path = \"s3://bossdb-open-data/iarpa_microns/minnie/minnie65/skeletons/v661/skeletons/\"\n\n# path to the skeleton and meshwork .h5 files\nmesh_path = \"s3://bossdb-open-data/iarpa_microns/minnie/minnie65/skeletons/v661/meshworks/\"\n\n\n\n\n\n# Skeleton\nnucleus_id = 292685\nsegment_id = 864691135122603047\nskel_filename = f\"{segment_id}_{nucleus_id}.swc\"\n\n# load the .swc skeleton\nsk = skel_io.read_skeleton(skel_path, skel_filename)\n\n\n# load the meshwork (may take minutes to locate, if using public credentials)\nmesh_filename = f\"{segment_id}_{nucleus_id}.h5\"\nmw = skel_io.load_mw(mesh_path, mesh_filename)\n\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_skel(\n    sk,\n    title=nucleus_id,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\n\n\n\nThe meshwork object (h5 file) includes additional information about the input and output synapse positions along the skeleton\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_mw_skel(\n    mw, \n    title=nucleus_id,\n    pull_radius = True,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n    plot_presyn = True,\n    plot_postsyn = True,)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s say you have observed a cell in neuroglancer that you want to work with, like this pyramidal cell (https://neuroglancer.neuvue.io/?json_url=https://global.daf-apis.com/nglstate/api/v1/6320379988541440)\n\nroot_id_v1078 = 864691135356151759\n\nThe segment root_id may change with time, but the 6-digit nucleus id is much more static. Look up the nucleus id from the current root id\n\nnuc_df = client.materialize.tables.nucleus_detection_v0(pt_root_id=root_id_v1078).query()\nnucleus_id = nuc_df.id.item()\nprint(nucleus_id)\n\n490689\n\n\nNow use the nucleus id to look up the previous segmentation root id for v661\n(this is necessary to find the exact filename match. But alternately, you can use directory search to find the matching filenames with just the nucleus id)\n\nsegment_id = client.materialize.tables.nucleus_detection_v0(id=nucleus_id).query(\n    materialization_version=661).pt_root_id.item()\nsegment_id\n\n864691135851524807\n\n\n\n# Load Skeleton\nskel_filename = f\"{segment_id}_{nucleus_id}.swc\"\n\nsk = skel_io.read_skeleton(skel_path, skel_filename)\n\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_skel(\n    sk,\n    title=nucleus_id,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\nNote that this skeleton has no axon compartment. That is because the axon has only been proofread after the skeleton was created. Non-proofread cells do not have their axon statistics generated.\n\n\n\n\nCells that have undergone proofreading in the dataset are annotated in the proofreading_status_and_strategy table. For more information, see the MICrONS-Explorer proofreading documentation\n\n# Load cells that have undergone proofreading\nproof_df = client.materialize.tables.proofreading_status_and_strategy(status_axon=True).query(\n    select_columns = ['pt_root_id','pt_position','status_dendrite','status_axon','strategy_dendrite','strategy_axon'],\n    materialization_version=1078)\n\n# This gives you the list of cells with axonal proofreading, anywhere in the dataset\nproof_df.head()\n\nTable Owner Notice on proofreading_status_and_strategy: NOTE: this table supercedes 'proofreading_status_public_release'. For more details, see: www.microns-explorer.org/manifests/mm3-proofreading.\n\n\n\n\n\n\n\n\n\npt_root_id\nstatus_dendrite\nstatus_axon\nstrategy_dendrite\nstrategy_axon\npt_position\n\n\n\n\n0\n864691135617152361\nt\nt\ndendrite_extended\naxon_partially_extended\n[185152, 185344, 21255]\n\n\n1\n864691136090326071\nt\nt\ndendrite_extended\naxon_fully_extended\n[192080, 190064, 22297]\n\n\n2\n864691135082864887\nt\nt\ndendrite_extended\naxon_interareal\n[303659, 166262, 17349]\n\n\n3\n864691136195284556\nt\nt\ndendrite_extended\naxon_fully_extended\n[173184, 217472, 21929]\n\n\n4\n864691135565870679\nt\nt\ndendrite_clean\naxon_fully_extended\n[184384, 108896, 21755]\n\n\n\n\n\n\n\nNote: for v661, the current preferred proofreading table did not exist. Use proofreading_status_public_release instead\n\n# Load cells that have underwent proofreading for v661\nproof_df = client.materialize.tables.proofreading_status_public_release().query(\n    select_columns = ['pt_root_id','pt_position','status_dendrite','status_axon'],\n    materialization_version=661)\n\n# This gives you the list of cells with axonal proofreading, anywhere in the dataset\nproof_df.head()\n\nTable Owner Notice on proofreading_status_public_release: NOTE: this table is deprecated and no longer receiving updates; please use 'proofreading_status_and_strategy' which is available in datastack version &gt;= 1078 (datastack = minnie65_public or minnie65_phase3_v1).\n\n\n\n\n\n\n\n\n\npt_root_id\nstatus_dendrite\nstatus_axon\npt_position\n\n\n\n\n0\n864691134884807418\nextended\nextended\n[299067, 123129, 22993]\n\n\n1\n864691134885430010\nextended\nnon\n[181280, 223040, 21399]\n\n\n2\n864691134885645050\nextended\nnon\n[172288, 222528, 21607]\n\n\n3\n864691134918370314\nclean\nclean\n[170528, 226848, 20316]\n\n\n4\n864691134918461194\nclean\nclean\n[189760, 127520, 20540]\n\n\n\n\n\n\n\n\n\n\nThis will let you create skeletons for more recently proofread cells that do not exist in the publicly pregenerated .swc files. However, these skeletons will come without apical dendrite labels\npcg-skel documentation\n\n# !pip install pcg-skel\n# !pip install cloud-volume\n\n\nimport cloudvolume\nimport pcg_skel\n\n# specify the materialization version\nclient.materialize.version = 1078\n\n# initialize cloudvolume client\ncv_minnie = cloudvolume.CloudVolume(client.info.segmentation_source(), use_https=True)\n\n\n# pcg-skel pipeline code\ninput_id = 864691135639556411\nid_is_nuc = False\n\n# Cell identification meta\nsynapse_table = client.info.get_datastack_info()['synapse_table']\nif id_is_nuc:\n    id_col = 'id'\nelse:\n    id_col = 'pt_root_id'\nuse_view = True\nif use_view:\n    row = client.materialize.query_view('nucleus_detection_lookup_v1', \n                filter_equal_dict = {id_col: input_id})\nelse:\n    row = client.materialize.query_table('nucleus_detection_v0', \n                filter_equal_dict = {id_col: input_id})\n    \n    row = row.drop('created', axis = 1)\n\nnuc_id = int(row['id']) \nroot_id = int(row['pt_root_id'])\nroot_point = row['pt_position'].values[0]\n\nprint(f'starting on body {nuc_id}, {root_id}')\n\n# create whole neuron with radius info\nresample_spacing = 1510\ncollapse_soma = True\ncollapse_radius = 10_000\nres = [4, 4, 40]\nnrn =  pcg_skel.coord_space_meshwork(root_id,\n                                     client=client,\n                                     root_point=root_point,\n                                     root_point_resolution=res,\n                                     collapse_soma=collapse_soma,\n                                     collapse_radius=collapse_radius,\n                                     synapses='all',\n                                     synapse_table=synapse_table,\n                                     cv = cv_minnie)\n\n# add radius properties df to annotations \npcg_skel.features.add_volumetric_properties(nrn, client)\nprint('adding segment properties')\npcg_skel.features.add_segment_properties(nrn)\nprint('segment properties added')\n\nstarting on body 267207, 864691135639556411\nadding segment properties\nsegment properties added\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\n\nskelplot.plot_tools.plot_verts(nrn.skeleton.vertices, \n                               nrn.skeleton.edges, \n                               nrn.skeleton.radius,\n                               plot_soma=True,\n                               soma_node=nrn.skeleton.root,\n                               x=\"z\",\n                               y=\"y\")\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\nplt.gca().invert_yaxis()\n\n\n\n\n\n\n\n\n\n\n\n# create compartment label\ncompartment_labels = np.zeros(len(nrn.skeleton.vertices)).astype(int)\n\n# add soma label\ncompartment_labels[int(nrn.skeleton.root)]=1\n\n# get the mesh volume properties\nvolume_df = nrn.anno.segment_properties.df\n\n# add column indicating skel index \nvolume_df['skel_index'] = nrn.anno.segment_properties.mesh_index.to_skel_index_padded\nsk_volume_df = volume_df.drop_duplicates('skel_index').sort_values('skel_index').reset_index()\n\n# set map for skel index -&gt; radius\nradius_labels = np.array(sk_volume_df['r_eff']) / 1000\n\n# metadata dictionary for keeping vertex variables together\nskeleton_properties = {}\nskeleton_properties['compartment'] = compartment_labels\nskeleton_properties['vertices'] = nrn.skeleton.vertices\nskeleton_properties['edges'] = nrn.skeleton.edges\nskeleton_properties['radius'] = radius_labels\n\n\nnrn.skeleton.export_to_swc(filename = 'test.swc', \n                           node_labels=compartment_labels, \n                           radius=radius_labels,\n                           avoid_root=True, \n                           resample_spacing = resample_spacing)",
    "crumbs": [
      "Introduction",
      "Examples",
      "Neuron Skeletons"
    ]
  },
  {
    "objectID": "examples/skeleton_load_and_generate.html#download-visualize-and-generate-neuron-skeletons",
    "href": "examples/skeleton_load_and_generate.html#download-visualize-and-generate-neuron-skeletons",
    "title": "Neuron Skeletons",
    "section": "",
    "text": "Created 7/2/2024 by Bethanny Danskin\nMICrONS Tutorial for VORTEX\nDepends on the following two packages: CAVEclient and Skeleton-Plot (version&gt;0.0.9)\nSkeletons and meshworks are loaded as MeshParty meshwork object\npcg-skel and cloudvolume are necessary if generating your own skeletons from the meshes\n\nUsing CAVEclient requires having set up a CAVE auth token. See how to set up your CAVEclient token here.\n\n\n# !pip install caveclient\n# !pip install skeleton-plot\n\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n\nfrom caveclient import CAVEclient\n# Initialize a client for the \"minnie65_public\" datastack.\nclient = CAVEclient(datastack_name='minnie65_public') \n\n# see the available materialization versions\nclient.materialize.get_versions()\n\n# Uncomment to get more details on each release version\n# client.materialize.get_versions_metadata()\n\n[1078, 117, 661, 343, 795, 943]\n\n\n\n# set materialization version, for consistency\nmaterialization = 1078 # current public as of 6/5/2024\n# materialization = 661 # version at which skeletons were pre-generated\nclient.version = materialization\n\n\nimport skeleton_plot as skelplot\nimport skeleton_plot.skel_io as skel_io\n\n\n\nThe skeletons of the meshes are calculated at specific timepoints. The last collection of all neurons in the dataset was at materialization version 661.\nBoth the .swc skeletons and .h5 meshwork objects are available in the BossDB repository\n\n# path to the skeleton .swc files\nskel_path = \"s3://bossdb-open-data/iarpa_microns/minnie/minnie65/skeletons/v661/skeletons/\"\n\n# path to the skeleton and meshwork .h5 files\nmesh_path = \"s3://bossdb-open-data/iarpa_microns/minnie/minnie65/skeletons/v661/meshworks/\"\n\n\n\n\n\n# Skeleton\nnucleus_id = 292685\nsegment_id = 864691135122603047\nskel_filename = f\"{segment_id}_{nucleus_id}.swc\"\n\n# load the .swc skeleton\nsk = skel_io.read_skeleton(skel_path, skel_filename)\n\n\n# load the meshwork (may take minutes to locate, if using public credentials)\nmesh_filename = f\"{segment_id}_{nucleus_id}.h5\"\nmw = skel_io.load_mw(mesh_path, mesh_filename)\n\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_skel(\n    sk,\n    title=nucleus_id,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\n\n\n\nThe meshwork object (h5 file) includes additional information about the input and output synapse positions along the skeleton\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_mw_skel(\n    mw, \n    title=nucleus_id,\n    pull_radius = True,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n    plot_presyn = True,\n    plot_postsyn = True,)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s say you have observed a cell in neuroglancer that you want to work with, like this pyramidal cell (https://neuroglancer.neuvue.io/?json_url=https://global.daf-apis.com/nglstate/api/v1/6320379988541440)\n\nroot_id_v1078 = 864691135356151759\n\nThe segment root_id may change with time, but the 6-digit nucleus id is much more static. Look up the nucleus id from the current root id\n\nnuc_df = client.materialize.tables.nucleus_detection_v0(pt_root_id=root_id_v1078).query()\nnucleus_id = nuc_df.id.item()\nprint(nucleus_id)\n\n490689\n\n\nNow use the nucleus id to look up the previous segmentation root id for v661\n(this is necessary to find the exact filename match. But alternately, you can use directory search to find the matching filenames with just the nucleus id)\n\nsegment_id = client.materialize.tables.nucleus_detection_v0(id=nucleus_id).query(\n    materialization_version=661).pt_root_id.item()\nsegment_id\n\n864691135851524807\n\n\n\n# Load Skeleton\nskel_filename = f\"{segment_id}_{nucleus_id}.swc\"\n\nsk = skel_io.read_skeleton(skel_path, skel_filename)\n\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_skel(\n    sk,\n    title=nucleus_id,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\nNote that this skeleton has no axon compartment. That is because the axon has only been proofread after the skeleton was created. Non-proofread cells do not have their axon statistics generated.\n\n\n\n\nCells that have undergone proofreading in the dataset are annotated in the proofreading_status_and_strategy table. For more information, see the MICrONS-Explorer proofreading documentation\n\n# Load cells that have undergone proofreading\nproof_df = client.materialize.tables.proofreading_status_and_strategy(status_axon=True).query(\n    select_columns = ['pt_root_id','pt_position','status_dendrite','status_axon','strategy_dendrite','strategy_axon'],\n    materialization_version=1078)\n\n# This gives you the list of cells with axonal proofreading, anywhere in the dataset\nproof_df.head()\n\nTable Owner Notice on proofreading_status_and_strategy: NOTE: this table supercedes 'proofreading_status_public_release'. For more details, see: www.microns-explorer.org/manifests/mm3-proofreading.\n\n\n\n\n\n\n\n\n\npt_root_id\nstatus_dendrite\nstatus_axon\nstrategy_dendrite\nstrategy_axon\npt_position\n\n\n\n\n0\n864691135617152361\nt\nt\ndendrite_extended\naxon_partially_extended\n[185152, 185344, 21255]\n\n\n1\n864691136090326071\nt\nt\ndendrite_extended\naxon_fully_extended\n[192080, 190064, 22297]\n\n\n2\n864691135082864887\nt\nt\ndendrite_extended\naxon_interareal\n[303659, 166262, 17349]\n\n\n3\n864691136195284556\nt\nt\ndendrite_extended\naxon_fully_extended\n[173184, 217472, 21929]\n\n\n4\n864691135565870679\nt\nt\ndendrite_clean\naxon_fully_extended\n[184384, 108896, 21755]\n\n\n\n\n\n\n\nNote: for v661, the current preferred proofreading table did not exist. Use proofreading_status_public_release instead\n\n# Load cells that have underwent proofreading for v661\nproof_df = client.materialize.tables.proofreading_status_public_release().query(\n    select_columns = ['pt_root_id','pt_position','status_dendrite','status_axon'],\n    materialization_version=661)\n\n# This gives you the list of cells with axonal proofreading, anywhere in the dataset\nproof_df.head()\n\nTable Owner Notice on proofreading_status_public_release: NOTE: this table is deprecated and no longer receiving updates; please use 'proofreading_status_and_strategy' which is available in datastack version &gt;= 1078 (datastack = minnie65_public or minnie65_phase3_v1).\n\n\n\n\n\n\n\n\n\npt_root_id\nstatus_dendrite\nstatus_axon\npt_position\n\n\n\n\n0\n864691134884807418\nextended\nextended\n[299067, 123129, 22993]\n\n\n1\n864691134885430010\nextended\nnon\n[181280, 223040, 21399]\n\n\n2\n864691134885645050\nextended\nnon\n[172288, 222528, 21607]\n\n\n3\n864691134918370314\nclean\nclean\n[170528, 226848, 20316]\n\n\n4\n864691134918461194\nclean\nclean\n[189760, 127520, 20540]\n\n\n\n\n\n\n\n\n\n\nThis will let you create skeletons for more recently proofread cells that do not exist in the publicly pregenerated .swc files. However, these skeletons will come without apical dendrite labels\npcg-skel documentation\n\n# !pip install pcg-skel\n# !pip install cloud-volume\n\n\nimport cloudvolume\nimport pcg_skel\n\n# specify the materialization version\nclient.materialize.version = 1078\n\n# initialize cloudvolume client\ncv_minnie = cloudvolume.CloudVolume(client.info.segmentation_source(), use_https=True)\n\n\n# pcg-skel pipeline code\ninput_id = 864691135639556411\nid_is_nuc = False\n\n# Cell identification meta\nsynapse_table = client.info.get_datastack_info()['synapse_table']\nif id_is_nuc:\n    id_col = 'id'\nelse:\n    id_col = 'pt_root_id'\nuse_view = True\nif use_view:\n    row = client.materialize.query_view('nucleus_detection_lookup_v1', \n                filter_equal_dict = {id_col: input_id})\nelse:\n    row = client.materialize.query_table('nucleus_detection_v0', \n                filter_equal_dict = {id_col: input_id})\n    \n    row = row.drop('created', axis = 1)\n\nnuc_id = int(row['id']) \nroot_id = int(row['pt_root_id'])\nroot_point = row['pt_position'].values[0]\n\nprint(f'starting on body {nuc_id}, {root_id}')\n\n# create whole neuron with radius info\nresample_spacing = 1510\ncollapse_soma = True\ncollapse_radius = 10_000\nres = [4, 4, 40]\nnrn =  pcg_skel.coord_space_meshwork(root_id,\n                                     client=client,\n                                     root_point=root_point,\n                                     root_point_resolution=res,\n                                     collapse_soma=collapse_soma,\n                                     collapse_radius=collapse_radius,\n                                     synapses='all',\n                                     synapse_table=synapse_table,\n                                     cv = cv_minnie)\n\n# add radius properties df to annotations \npcg_skel.features.add_volumetric_properties(nrn, client)\nprint('adding segment properties')\npcg_skel.features.add_segment_properties(nrn)\nprint('segment properties added')\n\nstarting on body 267207, 864691135639556411\nadding segment properties\nsegment properties added\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\n\nskelplot.plot_tools.plot_verts(nrn.skeleton.vertices, \n                               nrn.skeleton.edges, \n                               nrn.skeleton.radius,\n                               plot_soma=True,\n                               soma_node=nrn.skeleton.root,\n                               x=\"z\",\n                               y=\"y\")\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\nplt.gca().invert_yaxis()\n\n\n\n\n\n\n\n\n\n\n\n# create compartment label\ncompartment_labels = np.zeros(len(nrn.skeleton.vertices)).astype(int)\n\n# add soma label\ncompartment_labels[int(nrn.skeleton.root)]=1\n\n# get the mesh volume properties\nvolume_df = nrn.anno.segment_properties.df\n\n# add column indicating skel index \nvolume_df['skel_index'] = nrn.anno.segment_properties.mesh_index.to_skel_index_padded\nsk_volume_df = volume_df.drop_duplicates('skel_index').sort_values('skel_index').reset_index()\n\n# set map for skel index -&gt; radius\nradius_labels = np.array(sk_volume_df['r_eff']) / 1000\n\n# metadata dictionary for keeping vertex variables together\nskeleton_properties = {}\nskeleton_properties['compartment'] = compartment_labels\nskeleton_properties['vertices'] = nrn.skeleton.vertices\nskeleton_properties['edges'] = nrn.skeleton.edges\nskeleton_properties['radius'] = radius_labels\n\n\nnrn.skeleton.export_to_swc(filename = 'test.swc', \n                           node_labels=compartment_labels, \n                           radius=radius_labels,\n                           avoid_root=True, \n                           resample_spacing = resample_spacing)",
    "crumbs": [
      "Introduction",
      "Examples",
      "Neuron Skeletons"
    ]
  },
  {
    "objectID": "examples/skeleton_load_and_generate.html#optional-generate-myelin-labels-from-myelin-table",
    "href": "examples/skeleton_load_and_generate.html#optional-generate-myelin-labels-from-myelin-table",
    "title": "Neuron Skeletons",
    "section": "Optional: generate myelin labels from myelin table",
    "text": "Optional: generate myelin labels from myelin table\nFor the subset of neurons with manual labeling of myelination, see CAVE table and documentaiton for vortex_manual_myelination_v0\n\n# Skeleton utility functions\nfrom tqdm.notebook import tqdm, trange\ntqdm.pandas()\ndef get_myelin_at_vertex(mw, myelin_df, properties_dict, client, cv):\n    # given a df of all myelinated points on the axon, return the corresponding skeleton labels to the properties dictionary\n\n    # get level2 nodes at myelinated positions\n    myelin_lvl2 = myelin_df.progress_apply(pd_get_level2_point, axis=1, client=client, cv=cv)\n\n    # generate index lookup\n    mw_index_lookup = get_meshwork_index_lookup(mw)\n\n    # merge myelin_lvl2 to index lookup\n    myelin_merge = pd.merge(myelin_lvl2, mw_index_lookup, on='lvl2_id', how='inner')\n    \n    # empty myelin labels\n    try:\n        vertices = properties_dict['vertices']\n    except:\n        print('no vertex properties found; call get_skeleton_features_from_meshwork() first') \n        \n    myelin = np.zeros(len(vertices))\n    \n    # where myelin is present, set to 1\n    myelin[myelin_merge.skel_ind.values] = 1\n\n    properties_dict['myelin'] = myelin\n\n    return properties_dict\n\ndef pd_get_level2_point(row, client, cv, voxel_resolution=[4,4,40]):\n    point, root_id = row[['pt_position','valid_id']]\n\n    try:\n        lvl2_id = pcg_skel.chunk_tools.get_closest_lvl2_chunk(point,\n                                                              root_id,\n                                                              client,\n                                                              voxel_resolution=voxel_resolution,\n                                                              radius=200)\n        row['lvl2_id'] = lvl2_id\n    except:\n        row['lvl2_id'] = np.nan\n        \n    return row\n\ndef get_meshwork_index_lookup(mw):\n    # generate index lookup with lvl2, mesh, and skeleton indices\n    mw_index_lookup = mw.anno.lvl2_ids.df\n    mw_index_lookup['skel_ind'] = mw.mesh_indices.to_skel_index_padded\n    mw_index_lookup.set_index('mesh_ind_filt', inplace=True)\n\n    return mw_index_lookup\n\n\n# get myelin-labeled points (exists for a subset of manually annotated cells)\nmyelin_df = client.materialize.tables.vortex_manual_myelination_v0(valid_id=input_id, tag='t').query(\n    select_columns=['valid_id','tag','pt_position'],\n    materialization_version=1078,\n)\n\nprint(len(myelin_df))\n\nTable Owner Notice on vortex_manual_myelination_v0: Myelination status assessed for the axon of the VALID_ID, not the pt_root_id.\n\n\n197\n\n\n\n# Add myelin info (this takes upwards of 10 minutes to convert points to vertices)\nskeleton_properties = get_myelin_at_vertex(nrn, myelin_df, skeleton_properties, client=client, cv=cv_minnie)\n\n\n\n\n\nprint(skeleton_properties)\n\n{'compartment': array([0, 0, 0, ..., 0, 0, 1]), 'vertices': array([[ 395328.,  547312.,  924200.],\n       [ 396992.,  546184.,  923360.],\n       [ 397712.,  545840.,  922760.],\n       ...,\n       [1313160.,  846424.,  797800.],\n       [1314944.,  847008.,  792760.],\n       [ 717824.,  771840.,  863120.]]), 'edges': array([[ 842,  866],\n       [ 866,  867],\n       [ 867,  868],\n       ...,\n       [4602, 4595],\n       [4595, 4596],\n       [4596, 4597]], dtype=int64), 'radius': array([0.12334736, 0.12334736, 0.12334736, ..., 0.13528796, 0.13528796,\n       5.54981555]), 'myelin': array([0., 0., 0., ..., 0., 0., 0.])}",
    "crumbs": [
      "Introduction",
      "Examples",
      "Neuron Skeletons"
    ]
  },
  {
    "objectID": "examples/skeleton_load_and_generate.html#addendum-useful-chunkedgraph-functions-getting-segments-across-time",
    "href": "examples/skeleton_load_and_generate.html#addendum-useful-chunkedgraph-functions-getting-segments-across-time",
    "title": "Neuron Skeletons",
    "section": "Addendum: useful chunkedgraph functions (getting segments across time)",
    "text": "Addendum: useful chunkedgraph functions (getting segments across time)\nBecause the segment id may have changed since the skeletons were calculated, find the past segment id of this object\nChunkedgraph documentation and functions\n\nclient.materialize.get_versions()\n\n[1078, 117, 661, 343, 795, 943]\n\n\n\n# Get info on version 661\nclient.materialize.get_versions_metadata()[2]\n\n{'datastack': 'minnie65_public',\n 'is_merged': False,\n 'status': 'AVAILABLE',\n 'version': 661,\n 'id': 688,\n 'valid': True,\n 'time_stamp': datetime.datetime(2023, 4, 6, 20, 17, 9, 199182, tzinfo=datetime.timezone.utc),\n 'expires_on': datetime.datetime(2125, 3, 27, 19, 17, 9, 199182, tzinfo=datetime.timezone.utc)}\n\n\n\ntimestamp_v661 = client.materialize.get_versions_metadata()[2]['time_stamp']\ntimestamp_latest = client.materialize.get_versions_metadata()[0]['time_stamp']\n\n\nclient.chunkedgraph.suggest_latest_roots(segment_id, timestamp=timestamp_latest)\n\n864691135356151759\n\n\n\nclient.chunkedgraph.get_root_timestamps(root_id_v1078)\n\narray([datetime.datetime(2024, 4, 10, 4, 7, 54, 947000, tzinfo=&lt;UTC&gt;)],\n      dtype=object)\n\n\n\nclient.chunkedgraph.is_latest_roots(864691135808631069)\n\narray([ True])",
    "crumbs": [
      "Introduction",
      "Examples",
      "Neuron Skeletons"
    ]
  },
  {
    "objectID": "em_05_dash_apps.html",
    "href": "em_05_dash_apps.html",
    "title": "Dash Apps",
    "section": "",
    "text": "We have created two webapps to help explore and visualize the data.\nThese apps were created using the Dash framework, hence the name.",
    "crumbs": [
      "Introduction",
      "Dash Apps"
    ]
  },
  {
    "objectID": "em_05_dash_apps.html#table-viewer",
    "href": "em_05_dash_apps.html#table-viewer",
    "title": "Dash Apps",
    "section": "Table Viewer",
    "text": "Table Viewer\nThe Table Viewer app allows you to query individual tables in the database, filter them, and visualize the results in Neuroglancer.\n\n\n\nTable viewer input options\n\n\nIn the Table Viewer, you first select one of the tables from a dropdown in the upper left side.\nThe Live Query option is not relevent for the public data.\nFilter options allow you to limit the query to a subset of the data, if desired. Note that filtering can occur in the tables afterward as well.\nThe Cell IDs field lets you put in one or more root IDs, cell IDs (i.e. “Nucleus ID”), or IDs from the annotation table being queried. Note that if you use this option, you havet to set the dropdown to the appropriate ID type.\nThe Value Search field lets you search for a particular value, such as a specific cell type.\nOnce you have selected your table and set any pre-query filters, click the Submit button to run the query. When the query is done, a green bar will appear below the query input stating that it’s showing the state of the table you queried.\nThe first set of controls allow you to build a Neuroglancer link for the whole table using the “Generate Link” button.\nOptionally, if you would like to make a link where different values of a given column — example, different cell types — are shown in different layers, you can turn on the “Group Annotations” toggle on the right and select the column you want to use for groupings.\nAfter setting these controls, click the “Generate Link” button to create the Neuroglancer link.\n\n\n\nTable viewer query results.\n\n\nThe table itself is shown below the controls. You can sort the table by clicking on the column headers, and filter the table by typing in the filter box below the column headers.\nFor example, if you want to find only cells with cell type “5P-NP” in a cell type column, simply type 5P-NP in the box under the column header. Similarly, to find only cells whose nucleus volume is greater than 500, type &gt;500 in the box under the volume column header in a table with nucleus information.\nIn addition, individual rows can be selected by clicking on the checkbox to the left of the row.\nThe left button along the top is a Neuroglancer link specific to the selected rows, if present, or the table as filtered if no rows are selected.\nThe orange button will deselect all checkboxes, and the “Export to CSV” will download the table as filtered to a CSV.\n\n\n\n\n\n\nWarning\n\n\n\nIf the table is too large (well beyond 10,000 rows), there are too many annotations to show in Neuroglancer and no link will be automatically generated.",
    "crumbs": [
      "Introduction",
      "Dash Apps"
    ]
  },
  {
    "objectID": "em_05_dash_apps.html#connectivity-and-cell-type-viewer",
    "href": "em_05_dash_apps.html#connectivity-and-cell-type-viewer",
    "title": "Dash Apps",
    "section": "Connectivity and Cell Type Viewer",
    "text": "Connectivity and Cell Type Viewer\nThe other tool is the Connectivity Viewer, which is designed to let you glance at the synaptic output or input of a given cell and group connectivity by cell type or other annotations.\n\n\n\nConnectivity viewer input options.\n\n\nAlong the top, you can enter an ID (either a root ID or a cell ID) and, optionally, a table to use to define cell types for synaptic partners. ::: {.callout-note} Note that if you want to use cell IDs from the nucleus detection table, you must select the Nucleus ID option from the dropdown next to the entry box. :::\nThere are several tables to choose from, though the most informative will be one of the cell type classification tables. The aibs_metamodel_celltypes_v661 has the greatest coverage of the dataset, and is described in (Leila Elabbady and Mu 2022).\nThe table aibs_metamodel_mtypes_v661_v2 is the most current version version of the cell type classification, described in (Schneider-Mizell et al. 2023).\nPressing “Submit” will query the database for the cell and its synapses.\nOnce completed, you will see the bar under the input options turn green and report the data that it is displaying. For example, if we query root id 864691135408247241 and the default cell type table, we will see the following:\n\nImportantly, the data displayed below will match whatever is in this bar — if your query takes a long time or fails for some reason (i.e.bad internet, server error), the tables displayed might be stale.\nThere are two locations the resulting data is displayed, as we’ll see below. First, a bar of tabs allows you to quickly visualize the data in different ways. Second, the table can present synaptic partners in a more detailed way.\n\nVisualization tabs\nThe first and default option in the tabs is Tables Only which doesn’t actually show any data. This is merely a placeholder to let you look directly at the tablular data below.\nThe next tab, Plots lets you visualize the synaptic output of a neuron.\nWe focus on synaptic output only right now, because the data quality is such that automated dendritic reconstructions are reliable, but automated axon reconstructions are not (see Proofreading section). Because of that, if a neuron has been proofread we trust the cell types of its postsynaptic partners as a group but do not trust the cell types of its presynaptic partners.\n\n\n\nPlot view. Left, violin plot of the synaptic distribution of the queried cell. Blue/left is the distribution of the cortical depth of input synapses along, red/right is the distribution of cortical depth of synaptic outputs. Approximate layers are shown. Center, a scatterplot with a dot for each synaptic output of the queried cell that is onto a target with a unique cell body (i.e. no dendritic fragments and no cells erroneously merged into other cells). The y-axis is the depth of the synapse itself, while the x-axis is the depth of the cell body of the target neuron. If a cell body categroy is selected, the dots are colored by the cell type of the target neuron. Right, a bar plot showing of the number of synapses onto target cell types.\n\n\nThese plots are designed to give you a quick overview of a cell’s synaptic outputs.\nTo add a particularly cell type column, use the Color by value: dropdown below the row of plots to select a column to color by.\nThese plots are handled in Plotly, so you can hover over the plots to see more information, and you can click and drag to zoom in on a particular region and save by clicking on the camera icon. For the scatterplot, you can also turn on and off individual types by clicking on the legend.\nThe next tab, Neuroglancer Links lets you visualize the synapses of a neuron in Neuroglancer.\n\n\n\nNeuroglancer links. From left to right: Show all synaptic inputs to the cell, show all synaptic inputs to the cell grouped into different layers by value in the associated table (e.g. cell type), show all synaptic outputs of the cell, show all synaptic outputs of the cell grouped into different layers by value in the associated table (e.g. cell type). Below the dropdown specifies which column to use if grouping annotations. The Include No Type toggle specifies whether to show synapses with cells that have no cell type annotation in the grouped cases.\n\n\nThe Neuroglancer links generated by these buttons will span all synapses of the queried cell, and do not reflect the filtering in the table below. The synapse annotations are also associated with the synaptic partners, making it easy to browse through a broad sampling of synaptic connectivity.\n\n\nConnectivity Table\nThe table below shows a table of all synaptic partners.\n\n\n\nTable view options.\n\n\nOne of the first things you can see in the table viewer is that it is split into two tabs, Input and Output, with the tab names for each showing the total number of synapses in each category.\nTo move between them, just click the tab you want.\nAbove the Input/Ouput tabs is a link to the Neuroglancer view of the synapses show in the table, after filtering and/or selecting a subset of rows.\nThe table view here is similar to in the Annotation Table Viewer described above. You can sort the table by clicking column headers, filter the table using the row underneath the headers, and select individual rows. If you select individual rows, the Neuroglancer link will group all synapses from each partner together, making it particularly easy to see how synaptic connectivity relates to neuronal anatomy.\nThe default sorting of the table is by total number of synapses, but note that both summed total synapse size (net_size) and average synapse size (mean_size) are also shown. These can be particularly important values when thinking about connectivity between excitatory neurons.\nA CSV file of the entire table can be downloaded by clicking the “Export to CSV” button.",
    "crumbs": [
      "Introduction",
      "Dash Apps"
    ]
  },
  {
    "objectID": "em_03_proofreading.html",
    "href": "em_03_proofreading.html",
    "title": "Proofreading and Data Quality",
    "section": "",
    "text": "Understanding this variablity in data quality is critical when analyzing the data.\nAutomated segmentation of neuronal processes in dense EM imaging is challenging at the size of entire neurons, which can have millimeters of axons and dendrites. The automated segmentation algorithms used in the EM data for this project are not perfect, and so proofreading is necessary to correct errors in the segmentation.\nHowever, while automated proofreading methods are starting to appear, in general the proofreading process is time consuming and involves a lot of manual attention. Some cells in the MICrONs data are thus very well proofread, others are virtually untouched, and many are somewhere in between.\nEach kind of cell can be useful, but different approaches must be taken for each.\nIt is useful to distinguish between two kinds of proofreading errors: splits and merges.\nThese two kinds of errors impact analysis differently. While split errors reduce the number of correct synaptic connections, merge errors add incorrect connections.\nThe frequency of errors is roughly related to the size of the process, and thus axons and dendrites have extremely different error profiles. Because they are thicker, dendrites are largely well-segmented, with only a few split errors and most merge errors being with small axons rather than other dendrites.\nEven without proofreading a neuron’s dendrites, we thus expect that most of the input synapses onto the dendrites are correct (since the vast majority of merge errors are with axons that only have output synapses), though some may be missing due to split errors at tips or missing dendritic spines.\nIn contrast, axons, because they are thinner, have many more errors of both kinds. We thus do not trust axonal connectivity at all without additional proofreading.",
    "crumbs": [
      "Introduction",
      "Proofreading and Data Quality"
    ]
  },
  {
    "objectID": "em_03_proofreading.html#proofreading-categories",
    "href": "em_03_proofreading.html#proofreading-categories",
    "title": "Proofreading and Data Quality",
    "section": "Proofreading categories",
    "text": "Proofreading categories\nProofreading is not an all-or-nothing process. Because of the reasons described above, we distinguish between the proofread status of the axon and the dendrite for each neuron separately. In addition, we consider three levels of proofreading:\n\nUnproofread: The arbor (dendrite or axon) has not been comprehensively proofread, although edits may have happened here and there.\nClean: The arbor (dendrite or axon) has been comprehensively proofread to remove all merge errors, but may still have split errors. This means that the synapses that are there are correct, but incomplete.\nExtended: The arbor (dendrite or axon) has been comprehensively proofread to remove all merge and split errors. This means that the synapses that are there are correct and as-complete-as-possible by expert proofreaders. Note that even a well-proofread neuron often has some tips that cannot be followed due to the limitations of the underlying imagery, and of course processes may leave the segmented volume.\n\nThe upshot is that unproofread axons are not of a quality suitable for analysis, you can trust the targets of a clean or extended axon because the default dendrite quality is good. However, if your analysis truly demands knowing if a particular neuron is connected to another neuron or not (rather than connecting to a population), the proofreading standards are particularly high and possibly require additional checks.\n\nProofreading status\n\n\n\n\n\n\n\nPre Axon Status\nPost Dendrite Status\nAnalyzability\n\n\n\n\nUnproofread\nUnproofread\nNot analyzable\n\n\nUnproofread\nClean / Extended\nNot analyzable\n\n\nClean\nUnproofread / Clean / Extended\nAnalyzable but connectivity could be highly biased by limited axonal extent\n\n\nExtended\nUnproofread\nAnalyzable and connectivity is close to complete\n\n\n\nSince a single proofread axon can have many hundreds to ten thousand synaptic outputs, the vast majority onto local dendrites, each single axon provides a tremendous amount of data about that cell’s postsynaptic targets.",
    "crumbs": [
      "Introduction",
      "Proofreading and Data Quality"
    ]
  },
  {
    "objectID": "em_03_proofreading.html#proofreading-efforts",
    "href": "em_03_proofreading.html#proofreading-efforts",
    "title": "Proofreading and Data Quality",
    "section": "Proofreading efforts",
    "text": "Proofreading efforts\nThere have been several different proofreading efforts in the MICrONS data with different goals and levels of comprehensiveness. Importantly, neurons were not selected for proofreading randomly, but rather chosen based on various criteria. A few of the most signifcant proofreading efforts are described below.\n\nExcitatory neuron proofreading for functional connectomics. Excitatory neurons in retinotopically matched ares of VISp and RL were proofread to enable functional connectomics analysis, looking for the relationship between functional similarity and synaptic connectivity.\nMartinotti cell proofreading. Extremely extensive reconstructions of several layer 5 Martinotti cells in VISp were performed to enable analysis of the relationship between morphology and transcriptomics.\nMinnie Column. A 100 micron square area of VISp for a comprehensive census of its neuronal population across all layers. Originally, excitatory neurons in this column had dendritic cleaning and extension and inhibitory neurons had comprehensive cleaning and substantail but incomplete extension. However, this population has become a hub for proofreading and cell typing, and will be discussed further in the section below.\n\nThe NIH-funded Virtual Observatory of the Cortex (VORTEX) program supports continued proofreading efforts. Supported efforts include proofreading of: of excitatory functionally-coregistered cells, inhibitory neurons, and glial cell types including astrocytes and oligodendrocyte precursor cells. To learn more, visit microns-explorer.org/vortex",
    "crumbs": [
      "Introduction",
      "Proofreading and Data Quality"
    ]
  },
  {
    "objectID": "em_03_proofreading.html#proofreading-tables",
    "href": "em_03_proofreading.html#proofreading-tables",
    "title": "Proofreading and Data Quality",
    "section": "Proofreading tables",
    "text": "Proofreading tables\nTo keep track of the state of proofreading, the table proofreading_status_and_strategy in the annotation database has a row for each proofread neuron and provides a summary of the proofreading status of the axon and dendrite. For more details, look at the Proofreading table documentation.",
    "crumbs": [
      "Introduction",
      "Proofreading and Data Quality"
    ]
  },
  {
    "objectID": "em_03_proofreading.html#minnie-column",
    "href": "em_03_proofreading.html#minnie-column",
    "title": "Proofreading and Data Quality",
    "section": "Minnie Column",
    "text": "Minnie Column\nThe Minnie Column is a collection of more than 1,300 neurons whose nucleus centroid fell within a 100 x 100 micron square (viewed from the top) and extending across all layers.\n\n\n\nLocation of the Minnie column (yellow) within the dataset. Top view.\n\n\nThis was originally considered a region of interest for a comprehensive census of the neuronal population, and thus all neurons in this region were proofread to some extent and manually evaulated.\nAll excitatory neurons had their dendrites cleaned and extended to assess dendritic morphology and synaptic inputs, while inhibitory neurons had both their dendrites cleaned and extended, and then their axons were extensively cleaned and extended to capture their targeting properties. Efforts were made to extend inhibitory axons to every major region of their arbor, but not every individual tip was followed and the resulting analysis focused on population-level connectivity rather than individual connections.\nMorphological and connectivity-based cell typing was performed on these cells in terms of both classical and novel categories, and considerable expert attention ensured that the baseline data quality was high. Cell type results can be found in several of the cell type tablesavailable. These cell types were used for training the various whole-dataset cell typing classifiers.\nIn addition, the proofreading and multiple levels of cell typing applied to this dense and diverse population has made it a useful reference, and spawned further analysis. Subsequent work has cleaned many of the excitatory axons from layers 2–5, albeit to a variety of degrees of extension.",
    "crumbs": [
      "Introduction",
      "Proofreading and Data Quality"
    ]
  },
  {
    "objectID": "em_01_background.html",
    "href": "em_01_background.html",
    "title": "Introduction",
    "section": "",
    "text": "A 3D rendering of cells in the MICrONs dataset.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "em_01_background.html#em-connectomics",
    "href": "em_01_background.html#em-connectomics",
    "title": "Introduction",
    "section": "EM Connectomics",
    "text": "EM Connectomics\nThe function of the nervous system arises out of a combination of the properties of individual neurons and the properties of how they are connected into a larger network. The central goal of connectomics is to produce complete maps of the connectivity of the nervous system with synaptic resolution and analyze them to better understand the organization, development, and function of the nervous system. Electron microscopy (EM) has been a central tool in achieving these aims for two key reasons: 1) EM can easily image nanometer-scale structures such as synapses. This allows one to unambiguously observe chemical synapses and, if pushed to high enough resolution, gap junctions. (Note that the MICrONs dataset is not high enough resolution to unambiguously observe gap junctions.) 2) Dense staining methods allow the complete observation of a of rich collection of cellular features, including membranes, nuclei, mitochondria, and much more. This allows the reconstruction of the complete morphology of cells.\nThe general approach to EM involves staining tissue with heavy metals and using electrons to imaging thin sections of tissue. Sections can either be created by cutting tissue into slices (“serial section”) followed by imaging, or by imaging the face of a block and shaving off a thin layer of tissue after each run (“block face”). In both approaches, each section corresponds to only 30–50 nanometers of thickness (although it can be a millimeter wide), and thus assembling a substantial volumes involves thousands or tens of thousands of sections in a row. Finally, computational methods are used to align and assemble the sections into a single volume and then to segment the volume into individual cells and synapses.\nWhile the resolution afforded by EM has been used to study the nervous system since 1959, the first true “connectomics” project that merged dense mapping of neurons and synaptic connectivity was the study of the nematode C. elegans led by Sydney Brenner, John White, and others. Advances in EM methods and computational methods in the last ten years have allowed tremendous scaling of connectomics datasets. The largest current mammalian datasets, including the MICrONs volume will consider here, now span more than a cubic millimeter, and invertebrate datasets now can cover the entire brain of a fruit fly.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "em_01_background.html#what-questions-can-be-addressed-by-connectomic-data",
    "href": "em_01_background.html#what-questions-can-be-addressed-by-connectomic-data",
    "title": "Introduction",
    "section": "What questions can be addressed by connectomic data?",
    "text": "What questions can be addressed by connectomic data?\nFor invertebrates like the fruit fly, the nervous system is highly stereotyped in both the composition of cells and how they connect to one another. Indeed, in the fly many cell types exist as a single pair of cells, one from the left and one from the right hemisphere. Connectomic maps from one individual can thus act as a nearly universal map of the brain and powerfully inform theoretical and experimental studies.\nIn mouse cortex, this same type of stereotypy does not apply. There is no one-to-one match between any given cell in the brain of one mouse and another, and thousands of cells in a cortical area can belong to the same cell type. Because of this, the types of questions one can ask differ, but dense reconstruction of anatomy and connectivity still can be a powerful source of insight into questions like:\n\nWhat morphological or connectivity-based cell types can be found?\nWhat are the properties of the excitatory networks in cortex and how might they relate to learning rules?\nWhat rules dictate how inhibitory cells distribute their synaptic output across target cell types?\n\nIn some datasets, such as the MICrONs dataset, calcium imaging of the same tissue was performed prior to preparing the structural EM volume. In that case, we can address additional questions like:\n\nHow do functional responses relate to the morphology of cells?\nHow do functional responses relate to the connectivity of cells?\n\nIn addition, a whole host of non-neuronal cells are also present, such as astrocytes, oligodendrocytes, microglia, and pericytes. Many people are interested in studying the relationship between these cells and neurons, and connectomic datasets can be a powerful tool to discovery this structural context.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "em_01_background.html#the-microns-dataset",
    "href": "em_01_background.html#the-microns-dataset",
    "title": "Introduction",
    "section": "The MICrONs dataset",
    "text": "The MICrONs dataset\n\n\n\n\n\n\nNote\n\n\n\nFor more complete details about the generation of the volume, please see “Functional Connectomics Spanning Multiple Areas of Mouse Visual Cortex” (The MICrONS Consortium 2021).\n\n\nThe MICrONs dataset is an EM volume of mouse visual cortex that spans all cortical layers, and extends approximately 1 mm in width by 0.5 mm in depth, collected as a collaboration between the Allen Institute for Brain Science, Baylor College of Medicine, and Princeton University. The goal of this dataset was to produce a so-called “functional connectomics” volume, where the same population of neurons could be measured with both calcium imaging to record functional properties and EM to describe synapse-resolution structural properties.\nTo link structure and function, excitatory neurons were genetically labeled with GCaMPs and imaged under a battery of natural movies and parametric stimuli by the Tolias lab, then at Baylor College of Medicine. The mouse was shipped to the Allen Institute and the same cortical region was prepared for EM, with more than 25,000 sections cut from the block and imaged across a small fleet of electron microscopes. In order to capture the interactions between different visual regions, the volume location is at the edge of primary visual cortex and extends into higher visual areas AL and RL (Glickfeld and Olsen 2017). Imagery was then aligned and segmented by the team of Sebastian Seung at Princeton, who also automatically detected synapses and nuclei. A team of proofreaders at Johns Hopkins Applied Physics Laboratory helped correct the largest types of errors in the segmentation, leaving virtually every neuron as a distinct object, and coregister tens of thousands of neurons to the functional data. Further analysis at the Allen Institute and elsewhere has focused on curating a map of cell types across the entire dataset, and generating a database to link synapses, cell types, and more to the morphology of individual cells.\nThe result is a cubic millimeter scale dataset with a rich collection of information and the potential for a range of studies, with strengths and limitations due to the current state of proofreading and needs of the functional data collection.\nThe next sections of the Data Book will introduce how to access the various parts of the dataset and give some guidance about how to use them.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "em_02_dataset_basics.html",
    "href": "em_02_dataset_basics.html",
    "title": "Dataset Basics",
    "section": "",
    "text": "One of the more complicated things about using connectomics data is that it’s both highly multimodal — there are images, tables, meshes, and more — and that it has been designed to be dynamic, as the segmentation changes with proofeading and carries changes to synaptic connectivity and more. This section will cover what types of data exist and how to access them.\nThe types of data we will describe here are:\nIn addition, there are three primary ways to access the data:",
    "crumbs": [
      "Introduction",
      "Dataset Basics"
    ]
  },
  {
    "objectID": "em_02_dataset_basics.html#dataset-information-hub",
    "href": "em_02_dataset_basics.html#dataset-information-hub",
    "title": "Dataset Basics",
    "section": "Dataset Information Hub",
    "text": "Dataset Information Hub\nIf you need to look up any particular values or links for the dataset — how to open a Neuroglancer link for it, what annotation tables exist, how to find Dash Apps — the public MICrONs datastack has a useful information page. Go there if you need to look up basic details about the dataset.\nParticularly useful links here are the Neuroglancer link, the Dash Apps, and the list of all annotation tables associated with the most recent release version (v795).",
    "crumbs": [
      "Introduction",
      "Dataset Basics"
    ]
  },
  {
    "objectID": "em_02_dataset_basics.html#how-to-reference-a-neuron",
    "href": "em_02_dataset_basics.html#how-to-reference-a-neuron",
    "title": "Dataset Basics",
    "section": "How to reference a neuron",
    "text": "How to reference a neuron\nThe dynamic nature of the data means that there is more than one way to reference a neuron, and each way has a slightly different meaning.\nThe most frequent way to reference a specific neuron is its root id, a unique integer that identifies a specific segmentation, and can be used in numerous places. However, as proofreading occurs, the segmentation of a cell can change, and each edit creates a new “version” of a given neuron that is given its own unique root id. Root ids thus refer to not only a specific cell, but a specific version of that cell.\nWhen downloading meshes, querying the database, or browsing in Neuroglancer, we use the root id of a cell.\nHowever, when using data from different points in time, however, root ids can change and the data might not be consistent across two time points. One way to make this easier is to only use data from one moment in time.\nThis is the case for the public database that will be used in the course — all data from the public database is synched up to a single moment in time, but it does mean that the analysis cannot respond to any changes in segmentation. In some cases, in some cases, you might see the root id called a “segment id” or an “object id,” but they all refer to the same thing.\nFor segmentations with a cell body in the volume, we also have a cell id (or soma id), which is a unique integer that identifies each nucleus in the dataset. The advantage of the soma id is that it remains constant across all versions of a cell, so it can be used to track a cell across time. The disadvantage is that to look up the id at any given time or to load into Neuroglancer, you need to look up the root id.\nCell ids can be found in the nucleus_detection_v0 table in the database, as well as many of the tables that reference cell types.",
    "crumbs": [
      "Introduction",
      "Dataset Basics"
    ]
  },
  {
    "objectID": "em_04_neuroglancer.html",
    "href": "em_04_neuroglancer.html",
    "title": "Neuroglancer",
    "section": "",
    "text": "Note\n\n\n\nNeuroglancer works best in Chrome and Firefox, and does not always work as expected in Safari.\nNeuroglancer is a WebGL-based viewer developed by Jeremy Maitin-Shephard at the Google Connectomics team to visualize very large volumetric data, designed in large part for connectomics. We often use Neuroglancer to quickly explore data, visualize results in context, and share data.\nTo look at the MICrONS data in Neuroglancer, click this link. Note that you will need to authenticate with the same Google-associated account that you use to set up CAVEclient.",
    "crumbs": [
      "Introduction",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "em_04_neuroglancer.html#interface-basics",
    "href": "em_04_neuroglancer.html#interface-basics",
    "title": "Neuroglancer",
    "section": "Interface Basics",
    "text": "Interface Basics\nThe Neuroglancer interface is divided into panels. In the default view, one panel shows the imagery in the X/Y plane (left), one shows a 3d view centered at the same location, and the narrow third panel provides information about the specific layer. Note that at the center of the each panel is a collection of axis-aligned red, blue and, green lines. The intersection and direction of each of these lines is consistent across all panels.\nAlong the top left of the view, you can see tabs with different names. Neuroglancer organizes data into layers, where each layer tells Neuroglancer about a different aspect of the data. The default view has three layers:\n\nimg describes how to render imagery.\nseg describes how to render segmentation and meshes.\nann is a manual annotation layer, allowing the user to add annotations to the data.\n\nYou can switch between layers by right clicking on the layer tab. You will see the panel at the right change to provide controls for each layer as you click it.\nThe collection of all layers, the user view, and all annotations is stored as a JSON object called the state.\nThe basic controls for navigation are:\n\nsingle click/drag slides the imagery in X/Y and rotates the 3d view.\nscroll wheel up/down moves the imagery in Z.\nright click jumps the 3d view to the clicked location in either the imagery or on a segmented object.\ndouble click selects a segmentation and loads its mesh into the 3d view. Double clicking on a selected neuron deselects it.\ncontrol-scrool zooms the view under the cursor in or out.\nz snaps the view to the closest right angle.\n\nYou can paste a position into Neuroglancer by clicking the x, y, z coordinate in the upper left corner and pasting a space or comma-separated list of numbers and hitting enter. Note that Neuroglancer always works in voxel units, and you can see the resolution of the voxels in the extreme upper left corner.",
    "crumbs": [
      "Introduction",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "em_04_neuroglancer.html#selecting-objects",
    "href": "em_04_neuroglancer.html#selecting-objects",
    "title": "Neuroglancer",
    "section": "Selecting objects",
    "text": "Selecting objects\nThe most direct way to select a neuron is to double click in the imagery to select the object under your cursor. This will load all the voxels associated with that object and also display its mesh in the 3d view.\nTo see the list of selected objects, you can select the segmentation tab (right click on the seg tab). Underneath the list of options, there is a list of selected root ids and the color assigned to them in the view. You can change colors of all neurons randomly by pressing l or individually change colors as desired. In addition, you can press the checkbox to hide a selected object while keeping it in the list, or deselect it by clicking on the number itself. You can also copy a root id by pressing the clipboard icon next to its number, or copy all selected root ids by pressing the clipboard icon above the list.\nThis selection list also allows you to select objects by pasting one or more root ids into the text box at the top of the list and pressing enter.",
    "crumbs": [
      "Introduction",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "em_04_neuroglancer.html#annotations",
    "href": "em_04_neuroglancer.html#annotations",
    "title": "Neuroglancer",
    "section": "Annotations",
    "text": "Annotations\nAnnotations are stored in an annotation layer. The default state has an annotation layer called ann, but you can always add new annotation layers by command-clicking the + button to the right of the layer tabs.\nTo create an annotation, select the layer (right click on the tab), and then click the icon representing the type of annotation you want to use. The most basic annotation is a point, which is the icon to the left of the list. The icon will change to having a green background when selected.\nNow if you control-click in either the imagery or the 3d view, you will create a point annotation at the clicked location. The annotation will appear in the list to the right, with its coordinate (in voxels, not nanometers) displayed. Clicking any annotation in the list will jump to that annotation in 3d space. Each annotation layer can have one color, which you can change with the color picker to the left of the annotation list.",
    "crumbs": [
      "Introduction",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "em_04_neuroglancer.html#navigating-annotations",
    "href": "em_04_neuroglancer.html#navigating-annotations",
    "title": "Neuroglancer",
    "section": "Navigating Annotations",
    "text": "Navigating Annotations\nAnnotations in an annotation layer can be right-clicked on to jump to them, but can also be navigated as a list.\n\nspelunkerneuroglancer.neuvuengl.microns-explorer\n\n\nTo see the list of annotations, select the annotation layer (right click on the tab).\nEach annotation is listed as its location in Neuroglancer voxel coordinates.\nClicking on an row in this annotation list will not only jump to it in the view, but also select it. You can see the information about the selected neuron in the lower right corner.\n\n\nTo see the list of annotations, select the annotation layer (right click on the tab).\nEach annotation is listed as its location in Neuroglancer voxel coordinates.\nClicking on an row in this annotation list will not only jump to it in the view, but also select it. You can see the information about the selected neuron in the lower right corner.\n\n\n\n\n\n\nImportant\n\n\n\nThe following sections are not accurate for spelunker version of neuroglancer, but do represent planned feature addtions.\n\n\nOnce an annotation is selected, any associated root ids are loaded. The keys [ and ] will jump to the previous and next annotations in the list, respectively.\nEach annotation can have a full-text description associated with it for adding notes. This can be added in the lower right corner.\n\n\n\nAnnotation tags after populating them manually.\n\n\nHowever, the most convenient way to label data quickly is through Tags.\nTo add tags, click on the Shortcuts tab within the Annotation widget on the right, and then click on the + button to add a new tag.\nEach tag gets a text label and a key command to activate or deactivate it for a given annotation.\nBy default, the first tag is activated by pressing shift-q, the second by pressing shift-w, and so on down the qwerty line.\n\n\n\nAnnotation tags applied to annotations. The labels with with the # symbol indicate a tag is present.\n\n\nNow when you select an annotation, you can press the key command to attach that tag to it.\nPressing the same key command will remove the tag.\nAny number of tags can be added to each annotation.\nTogether with the [ and ] keys to navigate the list, this allows you to quickly label a large number of annotations.\n\n\nTo see the list of annotations, select the annotation layer (right click on the tab).\nEach annotation is listed as its location in Neuroglancer voxel coordinates.\nClicking on an row in this annotation list will not only jump to it in the view, but also select it. You can see the information about the selected neuron in the lower right corner.",
    "crumbs": [
      "Introduction",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "em_04_neuroglancer.html#saving-and-sharing-states",
    "href": "em_04_neuroglancer.html#saving-and-sharing-states",
    "title": "Neuroglancer",
    "section": "Saving and sharing states",
    "text": "Saving and sharing states\nLike many other websites that require logins, you cannot simply send your URL ot another person to have them see the view. Instead, to save the current state and make it available to yourself or others in the future, you need to save the state with the Share button at the top right corner. This will then give you a URL that you can copy and share with others or paste yourself. A typical sharing URL looks like the following:\nhttps://neuroglancer.neuvue.io/?json_url=https://global.daf-apis.com/nglstate/api/v1/4684616269037568\nThe first part is the URL for the Neuroglancer viewer, while the part after the ?json_url= is a URL that points to a JSON file that contains the state. The number at the end of the URL is used to uniquely identify the state and can be used programatically to retrieve information.\n\n\n\n\n\n\nWarning\n\n\n\nIf a URL contains ?local_id= instead of ?json_url, that means that it cannot be viewed by anyone else or even in another browser on your own computer.",
    "crumbs": [
      "Introduction",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "examples/generate_neuroglancer_states.html",
    "href": "examples/generate_neuroglancer_states.html",
    "title": "Generating Neuroglancer States",
    "section": "",
    "text": "Visualizing data in Neuroglancer is one of the easiest ways to explore it in its full context. The python package nglui was made to make it easy to generate Neuroglancer states from data, particularly pandas dataframes, in a progammatic manner. The package can be installed with pip install nglui.\n\nThe nglui package interacts prominantly with caveclient and annotations queried from the database. See the section on querying the database to learn more.\n\n\n\nThe nglui.parser package can be used to parse Neuroglancer states.\nThe simplest way to parse the annotations in a Neuroglancer state is to first save the state using the Share button, and then copy the state id (the last number in the URL).\nFor example, for the share URL https://neuroglancer.neuvue.io/?json_url=https://global.daf-apis.com/nglstate/api/v1/5560000195854336, the state id is 5560000195854336\nYou can then download the json and then use the annotation_dataframe function to generate a comprehensive dataframe of all the annotations in the state.\n\nimport os\nfrom caveclient import CAVEclient\nfrom nglui import parser\n\nclient = CAVEclient('minnie65_public')\n\nstate_id = 5560000195854336\nstate = client.state.get_state_json(state_id)\nparser.annotation_dataframe(state).head()\n\n\n\n\n\n\n\n\nlayer\nanno_type\npoint\npointB\nlinked_segmentation\ntags\ngroup_id\ndescription\n\n\n\n\n0\nsyns_in\npoint\n[294095, 196476, 24560]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n1\nsyns_in\npoint\n[294879, 196374, 24391]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n2\nsyns_in\npoint\n[300246, 200562, 24297]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n3\nsyns_in\npoint\n[300894, 201844, 24377]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n4\nsyns_in\npoint\n[294742, 199552, 23392]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n\n\n\n\n\nNote that tags in the dataframe are stored as a list of integers, with each integer corresponding to one of the tags in the list.\nTo get the mapping between the tag index and the tag name for each layer, you can use the tag_dictionary function.\n\nparser.tag_dictionary(state, layer_name='syns_out')\n\n{1: 'targets_spine', 2: 'targets_shaft', 3: 'targets_soma'}\n\n\n\n\n\nThe nglui.statebuilder package is used to build Neuroglancer states that express arbitrary data.\nThe general pattern is that one makes a “StateBuilder” object that has rules for how to build a Neuroglancer state layer by layer, including selecting certain neurons, and populate layers of annotations.\nYou then pass a DataFrame to the StateBuiler, and the rules tell it how to render the DataFrame into a Neuroglancer link.\nThe same set of rules can be used on similar dataframes but with different data, such as synapses from different neurons.\nTo understand the detailed use of the package, please see the tutorial.\nHowever, a number of basic helper functions allow nglui to be used for common functions in just a few lines.\nFor example, to generate a Neuroglancer state that shows a neuron and its synaptic inputs and outputs, we can use the make_neuron_neuroglancer_link helper function.\n\nfrom nglui.statebuilder import helpers\n\nhelpers.make_neuron_neuroglancer_link(\n    client,\n    864691135122603047,\n    show_inputs=True,\n    show_outputs=True,\n)\n\nNeuroglancer Link\n\n\nThe main helper functions are:\n\nmake_neuron_neuroglancer_link - Shows one or more neurons and, optionally, synaptic inputs and/or outputs.\nmake_synapse_neuroglancer_link - Using a pre-downloaded synapse table, make a link that shows the synapse and the listed synaptic partners.\nmake_point_statebuilder - Generate a statebuilder to map a dataframe containing points (by default, formatted like a cell types table) to a Neuroglancer link.\n\nIn all cases, please look at the docstrings for more information on how to use the functions.",
    "crumbs": [
      "Introduction",
      "Examples",
      "Generating Neuroglancer States"
    ]
  },
  {
    "objectID": "examples/generate_neuroglancer_states.html#programmatic-interaction-with-neuroglancer-states",
    "href": "examples/generate_neuroglancer_states.html#programmatic-interaction-with-neuroglancer-states",
    "title": "Generating Neuroglancer States",
    "section": "",
    "text": "Visualizing data in Neuroglancer is one of the easiest ways to explore it in its full context. The python package nglui was made to make it easy to generate Neuroglancer states from data, particularly pandas dataframes, in a progammatic manner. The package can be installed with pip install nglui.\n\nThe nglui package interacts prominantly with caveclient and annotations queried from the database. See the section on querying the database to learn more.\n\n\n\nThe nglui.parser package can be used to parse Neuroglancer states.\nThe simplest way to parse the annotations in a Neuroglancer state is to first save the state using the Share button, and then copy the state id (the last number in the URL).\nFor example, for the share URL https://neuroglancer.neuvue.io/?json_url=https://global.daf-apis.com/nglstate/api/v1/5560000195854336, the state id is 5560000195854336\nYou can then download the json and then use the annotation_dataframe function to generate a comprehensive dataframe of all the annotations in the state.\n\nimport os\nfrom caveclient import CAVEclient\nfrom nglui import parser\n\nclient = CAVEclient('minnie65_public')\n\nstate_id = 5560000195854336\nstate = client.state.get_state_json(state_id)\nparser.annotation_dataframe(state).head()\n\n\n\n\n\n\n\n\nlayer\nanno_type\npoint\npointB\nlinked_segmentation\ntags\ngroup_id\ndescription\n\n\n\n\n0\nsyns_in\npoint\n[294095, 196476, 24560]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n1\nsyns_in\npoint\n[294879, 196374, 24391]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n2\nsyns_in\npoint\n[300246, 200562, 24297]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n3\nsyns_in\npoint\n[300894, 201844, 24377]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n4\nsyns_in\npoint\n[294742, 199552, 23392]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n\n\n\n\n\nNote that tags in the dataframe are stored as a list of integers, with each integer corresponding to one of the tags in the list.\nTo get the mapping between the tag index and the tag name for each layer, you can use the tag_dictionary function.\n\nparser.tag_dictionary(state, layer_name='syns_out')\n\n{1: 'targets_spine', 2: 'targets_shaft', 3: 'targets_soma'}\n\n\n\n\n\nThe nglui.statebuilder package is used to build Neuroglancer states that express arbitrary data.\nThe general pattern is that one makes a “StateBuilder” object that has rules for how to build a Neuroglancer state layer by layer, including selecting certain neurons, and populate layers of annotations.\nYou then pass a DataFrame to the StateBuiler, and the rules tell it how to render the DataFrame into a Neuroglancer link.\nThe same set of rules can be used on similar dataframes but with different data, such as synapses from different neurons.\nTo understand the detailed use of the package, please see the tutorial.\nHowever, a number of basic helper functions allow nglui to be used for common functions in just a few lines.\nFor example, to generate a Neuroglancer state that shows a neuron and its synaptic inputs and outputs, we can use the make_neuron_neuroglancer_link helper function.\n\nfrom nglui.statebuilder import helpers\n\nhelpers.make_neuron_neuroglancer_link(\n    client,\n    864691135122603047,\n    show_inputs=True,\n    show_outputs=True,\n)\n\nNeuroglancer Link\n\n\nThe main helper functions are:\n\nmake_neuron_neuroglancer_link - Shows one or more neurons and, optionally, synaptic inputs and/or outputs.\nmake_synapse_neuroglancer_link - Using a pre-downloaded synapse table, make a link that shows the synapse and the listed synaptic partners.\nmake_point_statebuilder - Generate a statebuilder to map a dataframe containing points (by default, formatted like a cell types table) to a Neuroglancer link.\n\nIn all cases, please look at the docstrings for more information on how to use the functions.",
    "crumbs": [
      "Introduction",
      "Examples",
      "Generating Neuroglancer States"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MICrONs Tutorials",
    "section": "",
    "text": "Introduction to the MICrONs-Explorer data ecosystem\nA functional connectome containing 200,000 cells, 75,000 neurons with physiology, and 523 million synapses. This resource provides interactive visualizations of anatomical and functional data that span all 6 layers of mouse primary visual cortex and 3 higher visual areas (LM, AL, RL) within a cubic millimeter volume.\n\n\n\n\n\n\nUnder construction\n\n\n\nThis website will be undergoing major updates as we add tutorials and features.\n\n\n\n\n\nView in neuroglancer, from microns-explorer.org/mm3/explore\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "programmatic_access/em_py_02_cave_quickstart.html",
    "href": "programmatic_access/em_py_02_cave_quickstart.html",
    "title": "CAVE Quickstart",
    "section": "",
    "text": "The Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Quickstart"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_02_cave_quickstart.html#caveclient",
    "href": "programmatic_access/em_py_02_cave_quickstart.html#caveclient",
    "title": "CAVE Quickstart",
    "section": "CAVEclient",
    "text": "CAVEclient\nMost programmatic access to the CAVE services occurs through CAVEclient, a Python client to access various types of data from the online services.\nFull documentation for CAVEclient is available here.\nTo initialize a caveclient, we give it a datastack, which is a name that defines a particular combination of imagery, segmentation, and annotation database. For the MICrONs public data, we use the datastack name minnie65_public.\n\nimport os\nfrom caveclient import CAVEclient\ndatastack_name = 'minnie65_public'\nclient = CAVEclient(datastack_name)\n\n# Show the description of the datastack\nclient.info.get_datastack_info()['description']\n\n'This is the publicly released version of the minnie65 volume and segmentation. '",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Quickstart"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_02_cave_quickstart.html#caveclient-basics",
    "href": "programmatic_access/em_py_02_cave_quickstart.html#caveclient-basics",
    "title": "CAVE Quickstart",
    "section": "CAVEclient Basics",
    "text": "CAVEclient Basics\nThe most frequent use of the CAVEclient is to query the database for annotations like synapses. All database functions are under the client.materialize property. To see what tables are available, use the get_tables function:\n\nclient.materialize.get_tables()\n\n['baylor_gnn_cell_type_fine_model_v2',\n 'nucleus_alternative_points',\n 'allen_column_mtypes_v2',\n 'bodor_pt_cells',\n 'aibs_metamodel_mtypes_v661_v2',\n 'proofreading_status_public_release',\n 'allen_column_mtypes_v1',\n 'allen_v1_column_types_slanted_ref',\n 'aibs_column_nonneuronal_ref',\n 'nucleus_ref_neuron_svm',\n 'aibs_soma_nuc_exc_mtype_preds_v117',\n 'baylor_log_reg_cell_type_coarse_v1',\n 'coregistration_manual_v3',\n 'apl_functional_coreg_forward_v5',\n 'l5et_column',\n 'cell_edits_v661',\n 'pt_synapse_targets',\n 'aibs_metamodel_celltypes_v661',\n 'synapses_pni_2',\n 'nucleus_detection_v0',\n 'allen_minnie_extra_types',\n 'aibs_soma_nuc_metamodel_preds_v117',\n 'bodor_pt_target_proofread']\n\n\nFor each table, you can see the metadata describing that table. For example, let’s look at the nucleus_detection_v0 table:\n\nclient.materialize.get_table_metadata('nucleus_detection_v0')\n\n{'schema': 'nucleus_detection',\n 'id': 27096,\n 'created': '2020-11-02T18:56:35.530100',\n 'table_name': 'nucleus_detection_v0__minnie3_v1',\n 'valid': True,\n 'aligned_volume': 'minnie65_phase3',\n 'schema_type': 'nucleus_detection',\n 'user_id': '121',\n 'description': 'A table of nuclei detections from a nucleus detection model developed by Shang Mu, Leila Elabbady, Gayathri Mahalingam and Forrest Collman. Pt is the centroid of the nucleus detection. id corresponds to the flat_segmentation_source segmentID. Only included nucleus detections of volume&gt;25 um^3, below which detections are false positives, though some false positives above that threshold remain. ',\n 'notice_text': None,\n 'reference_table': None,\n 'flat_segmentation_source': 'precomputed://https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/nuclei',\n 'write_permission': 'PRIVATE',\n 'read_permission': 'PUBLIC',\n 'last_modified': '2022-10-25T19:24:28.559914',\n 'segmentation_source': '',\n 'pcg_table_name': 'minnie3_v1',\n 'last_updated': '2024-01-23T23:00:00.080429',\n 'annotation_table': 'nucleus_detection_v0',\n 'voxel_resolution': [4.0, 4.0, 40.0]}\n\n\nYou get a dictionary of values. Two fields are particularly important: the description, which offers a text description of the contents of the table and voxel_resolution which defines how the coordinates in the table are defined, in nm/voxel.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Quickstart"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_02_cave_quickstart.html#querying-tables",
    "href": "programmatic_access/em_py_02_cave_quickstart.html#querying-tables",
    "title": "CAVE Quickstart",
    "section": "Querying Tables",
    "text": "Querying Tables\nTo get the contents of a table, use the query_table function. This will return the whole contents of a table without any filtering, up to for a maximum limit of 200,000 rows. The table is returned as a Pandas DataFrame and you can immediately use standard Pandas function on it.\n\ncell_type_df = client.materialize.query_table('nucleus_detection_v0')\ncell_type_df.head()\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n730537\n2020-09-28 22:40:41.780734+00:00\nNaN\nt\n32.307937\n0\n0\n[381312, 273984, 19993]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n373879\n2020-09-28 22:40:41.781788+00:00\nNaN\nt\n229.045043\n96218056992431305\n864691136090135607\n[228816, 239776, 19593]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n601340\n2020-09-28 22:40:41.782714+00:00\nNaN\nt\n426.138010\n0\n0\n[340000, 279152, 20946]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n201858\n2020-09-28 22:40:41.783784+00:00\nNaN\nt\n93.753836\n84955554103121097\n864691135373893678\n[146848, 213600, 26267]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n600774\n2020-09-28 22:40:41.785273+00:00\nNaN\nt\n135.189791\n0\n0\n[339120, 276112, 19442]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nWhile most tables are small enough to be returned in full, the synapse table has hundreds of millions of rows and is too large to download this way\n\n\nTables have a collection of columns, some of which specify point in space (columns ending in _position), some a root id (ending in _root_id), and others that contain other information about the object at that point. Before describing some of the most important tables in the database, it’s useful to know about a few advanced options that apply when querying any table.\n\ndesired_resolution : This parameter allows you to convert the columns specifying spatial points to different resolutions. Many tables are stored at a resolution of 4x4x40 nm/voxel, for example, but you can convert to nanometers by setting desired_resolution=[1,1,1].\nsplit_positions : This parameter allows you to split the columns specifying spatial points into separate columns for each dimension. The new column names will be the original column name with _x, _y, and _z appended.\nselect_columns : This parameter allows you to get only a subset of columns from the table. Once you know exactly what you want, this can save you some cleanup.\nlimit : This parameter allows you to limit the number of rows returned. If you are just testing out a query or trying to inspect the kind of data within a table, you can set this to a small number to make sure it works before downloading the whole table. Note that this will show a warning so that you don’t accidentally limit your query when you don’t mean to.\n\nFor example, using all of these together:\n\ncell_type_df = client.materialize.query_table('nucleus_detection_v0', split_positions=True, desired_resolution=[1,1,1], select_columns=['pt_position', 'pt_root_id'], limit=10)\ncell_type_df\n\n\n\n\n\n\n\n\npt_position_x\npt_position_y\npt_position_z\npt_root_id\n\n\n\n\n0\n241856.0\n374464.0\n838720.0\n0\n\n\n1\n227200.0\n389120.0\n797160.0\n0\n\n\n2\n230144.0\n422336.0\n795320.0\n0\n\n\n3\n239488.0\n386432.0\n794120.0\n0\n\n\n4\n239744.0\n423488.0\n803120.0\n864691136050815731\n\n\n5\n245888.0\n384512.0\n800120.0\n0\n\n\n6\n249792.0\n391680.0\n807080.0\n0\n\n\n7\n243328.0\n403008.0\n794280.0\n0\n\n\n8\n247872.0\n386816.0\n805320.0\n0\n\n\n9\n260352.0\n416640.0\n802360.0\n864691135013273238",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Quickstart"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_02_cave_quickstart.html#filtering-queries",
    "href": "programmatic_access/em_py_02_cave_quickstart.html#filtering-queries",
    "title": "CAVE Quickstart",
    "section": "Filtering Queries",
    "text": "Filtering Queries\nFiltering tables so that you only get data about certain rows back is a very common operation. While there are filtering options in the query_table function (see documentation for more details), a more unified filter interface is available through a “table manager” interface.\nRather than passing a table name to the query_table function, client.materialize.tables has a subproperty for each table in the database that can be used to filter that table.\nThe general pattern for usage is\nclient.materialize.tables.{table_name}({filter options}).query({format and timestamp options})\nwhere {table_name} is the name of the table you want to filter, {filter options} is a collection of arguments for filtering the query, and {format and timestamp options} are those parameters controlling the format and timestamp of the query.\nFor example, let’s look at the table aibs_soma_nuc_metamodel_preds_v117, which has cell type predictions across the dataset. We can get the whole table as a DataFrame:\n\ncell_type_df = client.materialize.tables.aibs_soma_nuc_metamodel_preds_v117().query()\ncell_type_df.head()\n\n\n\n\n\n\n\n\nid_ref\ncreated_ref\nvalid_ref\ntarget_id\nclassification_system\ncell_type\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n553\n2022-07-26 23:54:55.895294+00:00\nt\n498173\naibs_neuronal\n6P-IT\n498173\n2020-09-28 22:43:20.177696+00:00\nt\n308.176159\n103884538719281829\n864691135373830344\n[284688, 211936, 15566]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n4509\n2022-07-27 00:00:10.165062+00:00\nt\n487329\naibs_neuronal\nMC\n487329\n2020-09-28 22:41:27.945151+00:00\nt\n295.937638\n105279407463397326\n864691135975935434\n[294544, 118624, 21745]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n4693\n2022-07-27 00:00:10.313814+00:00\nt\n106662\naibs_neuronal\n23P\n106662\n2020-09-28 22:42:56.452281+00:00\nt\n230.148178\n79524515478544304\n864691136084076652\n[107056, 119248, 19414]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n5061\n2022-07-27 00:00:10.592207+00:00\nt\n271350\naibs_neuronal\n6P-CT\n271350\n2020-09-28 22:41:38.906480+00:00\nt\n305.328128\n87351114324194368\n864691135777995965\n[163920, 235968, 20875]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n8652\n2022-07-27 00:01:29.589487+00:00\nt\n456040\naibs_neuronal\nMC\n456040\n2020-09-28 22:42:07.860678+00:00\nt\n257.463910\n101129507251445952\n864691136084057196\n[264544, 132528, 23988]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\nand we can add similar formatting options as in the last section to the query function:\n\ncell_type_df = client.materialize.tables.aibs_soma_nuc_metamodel_preds_v117().query(split_positions=True, desired_resolution=[1,1,1], select_columns=['pt_position', 'pt_root_id', 'cell_type'], limit=10)\ncell_type_df\n\n\n\n\n\n\n\n\ncell_type\npt_position_x\npt_position_y\npt_position_z\npt_root_id\n\n\n\n\n0\n23P\n257600.0\n487936.0\n802760.0\n864691135724233643\n\n\n1\n23P\n260992.0\n493568.0\n801560.0\n864691136436395166\n\n\n2\nNGC\n256256.0\n466432.0\n831040.0\n864691135462260637\n\n\n3\n23P\n255744.0\n480640.0\n833200.0\n864691136723556861\n\n\n4\n23P\n262144.0\n505856.0\n824880.0\n864691135776658528\n\n\n5\n23P\n257536.0\n521728.0\n804440.0\n864691135941166708\n\n\n6\n23P\n251136.0\n546048.0\n821320.0\n864691135479369926\n\n\n7\nastrocyte\n324096.0\n417920.0\n658880.0\n864691135937358133\n\n\n8\nNGC\n324032.0\n432960.0\n679800.0\n864691135207734905\n\n\n9\nNGC\n309568.0\n421120.0\n706000.0\n864691135758479438\n\n\n\n\n\n\n\nHowever, now we can also filter the table to get only cells that are predicted to have cell type \"BC\" (for “basket cell”).\n\nmy_cell_type = \"BC\"\nclient.materialize.tables.aibs_soma_nuc_metamodel_preds_v117(cell_type=my_cell_type).query()\n\n\n\n\n\n\n\n\nid_ref\ncreated_ref\nvalid_ref\ntarget_id\nclassification_system\ncell_type\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n39997\n2022-07-27 00:05:37.339316+00:00\nt\n193846\naibs_neuronal\nBC\n193846\n2020-09-28 22:40:41.897904+00:00\nt\n306.148966\n82838443188669165\n864691135684976823\n[131568, 168496, 16452]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n15248\n2022-07-27 00:02:08.492581+00:00\nt\n615735\naibs_neuronal\nBC\n615735\n2020-09-28 22:40:41.957345+00:00\nt\n314.539540\n112181247505371364\n864691136311774525\n[344880, 161104, 17084]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n41346\n2022-07-27 00:05:49.017547+00:00\nt\n369908\naibs_neuronal\nBC\n369908\n2020-09-28 22:40:41.814964+00:00\nt\n332.862751\n96002690286851358\n864691136522768017\n[227104, 207840, 20841]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n54811\n2022-07-27 00:07:14.246777+00:00\nt\n432338\naibs_neuronal\nBC\n432338\n2020-09-28 22:40:42.077679+00:00\nt\n403.668828\n100576589938580580\n864691136296706331\n[260560, 207360, 20577]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n30769\n2022-07-27 00:04:44.154032+00:00\nt\n613047\naibs_neuronal\nBC\n613047\n2020-09-28 22:40:41.982376+00:00\nt\n242.159780\n113234168401651200\n864691136065413528\n[352688, 141616, 25312]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3079\n60636\n2022-07-27 00:07:51.544023+00:00\nt\n438586\naibs_neuronal\nBC\n438586\n2020-09-28 22:45:25.430745+00:00\nt\n529.501389\n99807894274485381\n864691136897160046\n[254912, 247440, 23680]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3080\n76524\n2022-07-27 00:09:27.526460+00:00\nt\n452402\naibs_neuronal\nBC\n452402\n2020-09-28 22:45:25.323945+00:00\nt\n501.542912\n102109927354855018\n864691135386800769\n[271408, 97216, 18797]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3081\n56954\n2022-07-27 00:07:26.643176+00:00\nt\n376258\naibs_neuronal\nBC\n376258\n2020-09-28 22:45:25.357703+00:00\nt\n507.659387\n95022544591277972\n864691136578023700\n[220080, 245168, 22235]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3082\n74764\n2022-07-27 00:09:16.780389+00:00\nt\n676137\naibs_neuronal\nBC\n676137\n2020-09-28 22:45:25.371421+00:00\nt\n510.804296\n116061493832778551\n864691136119431960\n[372976, 235696, 25121]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3083\n20112\n2022-07-27 00:03:41.743075+00:00\nt\n591219\naibs_neuronal\nBC\n591219\n2020-09-28 22:45:25.526753+00:00\nt\n567.517839\n110216764830845707\n864691135279126177\n[330320, 204752, 25060]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n3084 rows × 15 columns\n\n\n\nor maybe we just want the cell types for a particular collection of root ids:\n\nmy_root_ids = [864691135771677771, 864691135560505569, 864691136723556861]\nclient.materialize.tables.aibs_soma_nuc_metamodel_preds_v117(pt_root_id=my_root_ids).query()\n\n\n\n\n\n\n\n\nid_ref\ncreated_ref\nvalid_ref\ntarget_id\nclassification_system\ncell_type\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n69892\n2022-07-27 00:08:42.855344+00:00\nt\n19116\naibs_neuronal\n23P\n19116\n2020-09-28 22:41:51.767906+00:00\nt\n301.426115\n74737997899501359\n864691135771677771\n[72576, 108656, 20291]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n1778\n2022-07-26 23:54:56.804122+00:00\nt\n21783\naibs_neuronal\n23P\n21783\n2020-09-28 22:41:59.966574+00:00\nt\n263.637074\n75795590176519004\n864691135560505569\n[80128, 124000, 16563]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n52713\n2022-07-27 00:07:02.006105+00:00\nt\n4074\naibs_neuronal\n23P\n4074\n2020-09-28 22:42:41.341179+00:00\nt\n313.678234\n73543309863605007\n864691136723556861\n[63936, 120160, 20830]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\nYou can get a list of all parameters than be used for querying with the standard IPython/Jupyter docstring functionality, e.g. client.materialize.tables.aibs_soma_nuc_metamodel_preds_v117.\n\n\n\n\n\n\nCaution\n\n\n\nUse of this functionality will show a brief warning that the interface is experimental. This is because the interface is still being developed and may change in the near future in response to user feedback.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Quickstart"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_02_cave_quickstart.html#querying-synapses",
    "href": "programmatic_access/em_py_02_cave_quickstart.html#querying-synapses",
    "title": "CAVE Quickstart",
    "section": "Querying Synapses",
    "text": "Querying Synapses\nWhile synapses are stored as any other table in the database, in this case synapses_pni_2, this table is much larger than any other table at more than 337 million rows, and it works best when queried in a different way.\nThe synapse_query function allows you to query the synapse table in a more convenient way than most other tables. In particular, the pre_ids and post_ids let you specify which root id (or collection of root ids) you want to query, with pre_ids indicating the collection of presynaptic neurons and post_ids the collection of postsynaptic neurons.\nUsing both pre_ids and post_ids in one call is effectively a logical AND, returning only those synapses from neurons in the list of pre_ids that target neurons in the list of post_ids.\nLet’s look at one particular example.\n\nmy_root_id = 864691135808473885\nsyn_df = client.materialize.synapse_query(pre_ids=my_root_id)\nprint(f\"Total number of output synapses for {my_root_id}: {len(syn_df)}\")\nsyn_df.head()\n\nTotal number of output synapses for 864691135808473885: 1498\n\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\nsize\npre_pt_supervoxel_id\npre_pt_root_id\npost_pt_supervoxel_id\npost_pt_root_id\npre_pt_position\npost_pt_position\nctr_pt_position\n\n\n\n\n0\n158405512\n2020-11-04 06:48:59.403833+00:00\nNaN\nt\n420\n89385416926790697\n864691135808473885\n89385416926797494\n864691135546540484\n[179076, 188248, 20233]\n[179156, 188220, 20239]\n[179140, 188230, 20239]\n\n\n1\n185549462\n2020-11-04 06:49:10.903020+00:00\nNaN\nt\n4832\n91356016507479890\n864691135808473885\n91356016507470163\n864691135884799088\n[193168, 190452, 19262]\n[193142, 190404, 19257]\n[193180, 190432, 19254]\n\n\n2\n138110803\n2020-11-04 06:49:46.758528+00:00\nNaN\nt\n3176\n87263084540201919\n864691135808473885\n87263084540199587\n864691135195078186\n[163440, 104292, 19808]\n[163498, 104348, 19806]\n[163460, 104356, 19804]\n\n\n3\n155339535\n2020-11-04 09:53:22.361558+00:00\nNaN\nt\n5624\n88540717319827050\n864691135808473885\n88540717319834759\n864691136039974142\n[173050, 186398, 21570]\n[173026, 186518, 21573]\n[173100, 186472, 21569]\n\n\n4\n148262628\n2020-11-04 06:53:27.294021+00:00\nNaN\nt\n3536\n88189766885093187\n864691135808473885\n88189835604584343\n864691135250533976\n[170154, 193170, 21123]\n[170046, 193240, 21123]\n[170118, 193220, 21128]\n\n\n\n\n\n\n\nNote that synapse queries always return the list of every synapse between the neurons in the query, even if there are multiple synapses between the same pair of neurons.\nA common pattern to generate a list of connections between unique pairs of neurons is to group by the root ids of the presynaptic and postsynaptic neurons and then count the number of synapses between them. For example, to get the number of synapses from this neuron onto every other neuron, ordered\n\nsyn_df.groupby(\n  ['pre_pt_root_id', 'post_pt_root_id']\n).count()[['id']].rename(\n  columns={'id': 'syn_count'}\n).sort_values(\n  by='syn_count',\n  ascending=False,\n)\n# Note that the 'id' part here is just a way to quickly extract one column.\n# This could be any of the remaining column names, but `id` is often convenient \n# because it is common to all tables.\n\n\n\n\n\n\n\n\n\nsyn_count\n\n\npre_pt_root_id\npost_pt_root_id\n\n\n\n\n\n864691135808473885\n864691135800958562\n20\n\n\n864691135214122296\n16\n\n\n864691136578647572\n15\n\n\n864691136066504856\n13\n\n\n864691135841325283\n11\n\n\n...\n...\n\n\n864691135544771112\n1\n\n\n864691135545198632\n1\n\n\n864691135545210408\n1\n\n\n864691135545311272\n1\n\n\n864691137197468481\n1\n\n\n\n\n1037 rows × 1 columns",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Quickstart"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_04_imagery_segmentation.html",
    "href": "programmatic_access/em_py_04_imagery_segmentation.html",
    "title": "Imagery and Segmentation",
    "section": "",
    "text": "Initial Setup\n\n\n\nBefore using any programmatic access to the data, you first need to set up your CAVEclient token.\n\n\n\n\n\n\n\n\nUnder construction\n\n\n\nPage to be expanded\n\n\n\nImagery and Segmentation\nWe recommend using ImageryClient to download imagery and segmentation data.\nImageryClient makes core use of CloudVolume, but adds convenience and better integration with the CAVEclient.\nYou can install ImageryClient with pip install imageryclient.\nImageryClient is designed to download aligned blocks of imagery and segmentation data, as well as has some convenience functions for creating overlays of the two.\nImagery is downloaded as blocks of 8-bit values (0-255) that indicate grayscale intensity, while segmentation is downloaded as blocks of 64-bit integers that describe the segmentation ID of each voxel.\nAlternatively, segmentation can be kept as a dictionary of boolean masks, where each key is a root ID and each value is a boolean mask of the same shape as the imagery.\nDetailed information on the options can be found in the documentation.\nA typical example would be to use ImageryClien to download and visualize a 512x512 pixel cutout of imagery and segmentation centered on a specific location based on the coordinates in Neuroglancer:\n\nimport os\nimport imageryclient as ic\nfrom caveclient import CAVEclient\n\nclient = CAVEclient('minnie65_public')\n\nimg_client = ic.ImageryClient(client=client)\n\nctr = [240640, 207872, 21360]\n\nimage, segs = img_client.image_and_segmentation_cutout(ctr,\n                                                       split_segmentations=True,\n                                                       bbox_size=(512, 512),\n                                                       scale_to_bounds=True,\n)\n\nic.composite_overlay(segs, imagery=image, palette='husl').convert(\"RGB\")\n# Note: the final `.convert('RGB')` is needed to build this documetnation, but is not required to run locally.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Imagery and Segmentation"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_06_skeletons.html",
    "href": "programmatic_access/em_py_06_skeletons.html",
    "title": "Skeletons",
    "section": "",
    "text": "Under construction\n\n\n\nTo add interactive visualization and code outputs",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Skeletons"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_06_skeletons.html#working-with-meshwork-files",
    "href": "programmatic_access/em_py_06_skeletons.html#working-with-meshwork-files",
    "title": "Skeletons",
    "section": "Working with Meshwork Files",
    "text": "Working with Meshwork Files\nLoading a meshwork file imports the level 2 graph (the “mesh”), the skeleton, and a collection of associated annotations.\n\nfrom meshparty import meshwork\nnrn = meshwork.load_meshwork(mesh_filename)\n\nThe main three properties of the meshwork object are:\n\nnrn.mesh : The l2graph representation of the reconstruction.\nnrn.skeleton : The skeleton representation of the reconstruction.\nnrn.anno : A table of annotation dataframes and associated metadata that links them to specific vertices in the mesh and skeleton.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Skeletons"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_06_skeletons.html#meshwork-nrn.mesh-vs-nrn.skeleton",
    "href": "programmatic_access/em_py_06_skeletons.html#meshwork-nrn.mesh-vs-nrn.skeleton",
    "title": "Skeletons",
    "section": "Meshwork nrn.mesh vs nrn.skeleton",
    "text": "Meshwork nrn.mesh vs nrn.skeleton\nSkeletons are “tree-like”, where every vertex (except the root vertex) has a single parent that is closer to the root than it, and any number of child vertices. Because of this, for a skeleton there are well-defined directions “away from root” and “towards root” and few types of vertices have special names:\n\nBranch point: vertices with two or more children, where a neuronal process splits.\nEnd point: vertices with no childen, where a neuronal process ends.\nRoot point: The one vertex with no parent node. By convention, we typically set the root vertex at the cell body, so these are equivalent to “away from soma” and “towards soma”.\nSegment: A collection of vertices along an unbranched region, between one branch point and the next end point or branch point downstream.\n\nMeshes are arbitrary collections of vertices and edges, but do not have a notion of “parent” or “child” “branch point” or “end point”. Here, this means the “mesh” used here includes a vertex for every level 2 chunk, even where it is thick like at a cell body or very thick dendrite. However, by default this means that there is not always a well-defined notion of parent or child nodes, or towards or away from root.\nIn contrast “Meshes” (really, graphs of connected vertices) do not have a unique “inward” and “outward” direction. For the sake of rapid skeletonization, the “meshes” we use here are really the graph of level 2 vertices as described above. These aren’t a mesh in the visualization sense of the section on downloading Meshes, but have the same data representation.\nTo handle this, the meshwork object associates each mesh vertices with a single nearby skeleton vertex, and each skeleton vertex is associated with one or more mesh vertices. By representing data this way, annotations like synapses can be directly associated with a mesh vertex (because synapses can be anywhere on the object) and then mapped to the skeleton in order to enjoy the topological benefits of the skeleton representation.\n\n# By the definition of skeleton vs mesh, we would expect that mesh contains more vertices than the skeleton. \n# We can see this by looking at the size of the skeleton vertex location array vs the size of the mesh vertex location array.\n\nprint('Skeleton vertices array length:', len(nrn.skeleton.vertices))\nprint('Mesh vertices array length:', len(nrn.mesh.vertices))\n\n\n# Let us try to visualize the skeleton:\n# Visualize the whole skeleton \n\n# here's a simple way to plot vertices of the skeleton\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\n%matplotlib notebook \n\nfig = plt.figure(figsize=(6, 6))\nax = plt.axes(projection='3d')\nax.scatter3D(nrn.skeleton.vertices[:,0], nrn.skeleton.vertices[:,1], nrn.skeleton.vertices[:,2], s=1)\n\n\n\n\nScatterplot of skeleton vertices as a point cloud",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Skeletons"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_06_skeletons.html#skeleton-properties-and-methods",
    "href": "programmatic_access/em_py_06_skeletons.html#skeleton-properties-and-methods",
    "title": "Skeletons",
    "section": "Skeleton Properties and Methods",
    "text": "Skeleton Properties and Methods\nTo plot this skeleton in a more sophisticated way, you have to start thinking of it as a graph, and the meshwork object has a bunch of tools and properties to help you utilize the skeleton graph.\nLet’s list some of the most useful ones below You access each of these with nrn.skeleton.* Use the ? to read more details about each one\nProperties\n\n`branch_points``: a list of skeleton vertices which are branches\nroot: the skeleton vertice which is the soma\ndistance_to_root: an array the length of vertices which tells you how far away from the root each vertex is\nroot_position: the position of the root node in nanometers\nend_points: the tips of the neuron\ncover_paths: a list of arrays containing vertex indices that describe individual paths that in total cover the neuron without repeating a vertex. Each path starts at an end point and continues toward root, stopping once it gets to a vertex already listed in a previously defined path. Paths are ordered to start with the end points farthest from root first. Each skeleton vertex appears in exactly one cover path.\ncsgraph: a scipy.sparse.csr.csr_matrix containing a graph representation of the skeleton. Useful to do more advanced graph operations and algorithms. https://docs.scipy.org/doc/scipy/reference/sparse.csgraph.html\nkdtree: a scipy.spatial.ckdtree.cKDTree containing the vertices of skeleton as a kdtree. Useful for quickly finding points that are nearby. https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.cKDTree.html\n\nMethods\n\npath_length(paths=None): the path length of the whole neuron if no arguments, or pass a list of paths to get the path length of that. A path is just a list of vertices which are connected by edges.\npath_to_root(vertex_index): returns the path to the root from the passed vertex\npath_between(source_index, target_index): the shortest path between the source vertex index and the target vertex index\nchild_nodes(vertex_indices): a list of arrays listing the children of the vertex indices passed in\nparent_nodes(vertex_indices): an array listing the parent of the vertex indices passed in\n\n\n# A better way to plot a neuron is to use cover_paths\n# and plot those as 3d lines\ndef plot_neuron_skeleton(neuron, ax, c='b', linewidth=1):\n\n    for cover_path in neuron.skeleton.cover_paths:\n        path_verts = neuron.skeleton.vertices[cover_path,:]\n        ax.plot(path_verts[:,0], path_verts[:,1], path_verts[:,2], c=c, linewidth=linewidth)\n\nfig = plt.figure(figsize=(6, 6))\nax = plt.axes(projection='3d')\nplot_neuron_skeleton(nrn, ax)",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Skeletons"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_06_skeletons.html#indexing-and-selecting-sets-of-points",
    "href": "programmatic_access/em_py_06_skeletons.html#indexing-and-selecting-sets-of-points",
    "title": "Skeletons",
    "section": "Indexing And Selecting Sets of Points",
    "text": "Indexing And Selecting Sets of Points\nThe meshworks object contains a mesh with lots of vertices and a skeleton which holds a subset of these vertices. Therefore, in python these points have different “indices” in the mesh and skeleton. For example, if the mesh contains 10000 vertices, the indexing of those would run from 0 - 9999. The skeleton, which contains a subset of 100 of these would have indexing from 0-99. How would you figure out which of the mesh vertices these correspond to?\nLuckily, we have some really nifty functions that help us distinguish those:\nLet us first look at some attributes in the meshworks objects:\nA few nifty function for subselecting points: downstream points and `path_between``. For a given point, downstream points are defined as points on paths from endpoints to the root which are further than the given point. For example, if the skeleton path is : A-B-C-D-E where A is the root, D and E are downstream points of C. With branching, this can be more complex. To find the downstream points from say the 9th branch point, we can do:\n\n# Downstream points\nnrn.downstream_of(nrn.branch_points[9])\n\npath_between returns the vertices from one point to another. For example, we can get the every mesh vertex from the end point 5 to the root point. As a quick visualization, we can look at the distance to root along the, showing that it is descreasing.\n\nfig, ax = plt.subplots()\nax.plot(\n    nrn.distance_to_root(\n        nrn.path_between(\n            nrn.end_points[5],\n            nrn.root,\n        )\n    ) / 1_000,\n)\n\nax.set_ylabel('Distance from root ($\\mu m$)')\nax.set_ylabel('Vertex along path')\n\n\n\n\nDistance to root along graph path indicated above. Note that this flattens out at zero because “distance” is computed along the skeleton and many graph vertices in the level 2 graph are associated with the soma vertex of the skeleton",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Skeletons"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_06_skeletons.html#masking",
    "href": "programmatic_access/em_py_06_skeletons.html#masking",
    "title": "Skeletons",
    "section": "Masking",
    "text": "Masking\nJust like meshes, we can mask the meshwork object. Like all basic meshwork functions, the expected input is in mesh vertices. Importantly, doing so will be synchronized across the mesh, the skeleton, and annotations.\n\n# Let's now use masking to highlight one particular cover path.\n\nfig = plt.figure(figsize=(6, 6))\nax = plt.axes(projection=\"3d\")\n\nplot_neuron_skeleton(nrn, ax, \"b\")\n\nnrn.reset_mask()  # This just makes sure we are working with the same baseline object.\n\n# This will make a mask with only the vertices in the first cover path.\nwith neuron.mask_context(nrn.skeleton.cover_paths[0].to_mesh_mask) as nrnf:\n  plot_neuron_skeleton(nrnf, ax, \"r\", linewidth=2)\n  ax.scatter(\n      nrnf.skeleton.root_position[0],\n      nrnf.skeleton.root_position[1],\n      nrnf.skeleton.root_position[2],\n  )\n\nWhile mask_context acts to mask the skeleton and then unmasks it at the end of operations, you can also just mask a skeleton and let it stay that way. In that case, use the nrn.apply_mask function.\n\n\n\n\n\n\nWarning\n\n\n\nDo not use the nrn.mesh.apply_mask or nrn.skeleton.apply_mask functions, which will not synchronize the mask across the mesh, skeleton, and annotations.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Skeletons"
    ]
  },
  {
    "objectID": "programmatic_access/em_py_06_skeletons.html#annotations",
    "href": "programmatic_access/em_py_06_skeletons.html#annotations",
    "title": "Skeletons",
    "section": "Annotations",
    "text": "Annotations\nnrn.anno has set of annotation tables containing some additional information for analysis. Each annotation table has both a Pandas DataFrame object storing data and additional information that allow the rows of the DataFrame to be mapped to mesh and skeleton vertices. For the neurons that have been pre-computed, there is a consisent set of annotation tables:\n\npost_syn: Postsynaptic sites (inputs) for the cell\npre_syn: Presynaptic sites (outputs) for the cell\nis_axon: List of vertices that have been labeled as part of the axon\nlvl2_ids: Gives the PCG level 2 id for each mesh vertex, largely for book-keeping reasons.\nsegment_properties: For each vertex, information about the approximate radius, surface area, volume, and length of the segment it is on.\nvol_prop: For every vertex, information about the volume and surface area of the level 2 id it is associated with.\n\nTo access one of the DataFrames, use the name of the table as an attribute of the anno object and then get the .df property. For example, to get the postsynaptic sites, we can do:\n\nfrom meshparty import meshwork\nnrn = meshwork.load_meshwork('data/864691134940133219_skel.h5')\nnrn.anno.post_syn.df.head()\n\nHowever, in addition to these rows, you can also easily get the mesh vertex index or skeleton vertex index that a row corresponds to with nrn.anno.post_syn.mesh_index or nrn.anno.post_syn.skel_index respectively. This seems small, but it allows you to integrate skeleton-like measurements with annotations trivially.\nFor example, to get the distance from the cell body for each postsynaptic site, we can do:\n\nnrn.distance_to_root(nrn.anno.post_syn.mesh_index)\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the nrn.distance_to_root, like all basic meshwork object functions, expects a mesh vertex index rather than a skeleton vertex index.\n\n\nA common pattern is to copy a synapse dataframe and then add columns to it. For example, to add the distance to root to the postsynaptic sites, we can do:\n\nsyn_df = nrn.anno.pre_syn.df.copy()\nsyn_df['dist_to_root'] = nrn.distance_to_root(nrn.anno.pre_syn.mesh_index)\n\n\nFiltering Annotations by Skeleton Arbor\nEach annotation table has a ‘filter_query’ method that takes a boolean mesh mask and returns only those rows of the dataframe associated with those particular locations on the mesh.\nLet’s use what we learned above in two examples: first, getting all input synapses within 50 microns of the root and second, getting all input synapses on one particular branch off of the soma.\n\ndtr = nrn.distance_to_root() / 1_000   # Convert from nanometers to microns\nnrn.anno.post_syn.filter_query( dtr &lt; 50).df\n\nWe can also use one set of annotations as an input to filter query from another set of annotations. For example, due to errors in segmentation or mistakes in synapse detection, there can be synaptic outputs on the dendrite. However, if we have an is_axon annotation that simply contains a collection of vertices that correspond to the cell’s axon.\nWe can use this annotation to create a mask and filter out all of the synapses that are not on the axon.\n\naxon_mask = nrn.anno.is_axon.mesh_mask\nnrn.anno.pre_syn.filter_query(~axon_mask).df # The \"~\" is a logical not operation that flips True and False\n\nAs a sanity check, we can use nglui to see if these synapses we have labeled as being on the axon are all where we expect.\n\nimport os\nfrom caveclient import CAVEclient\nfrom nglui.statebuilder.helpers import make_synapse_neuroglancer_link\n\nclient = CAVEclient('minnie65_public')\n\nmake_synapse_neuroglancer_link(\n    nrn.anno.pre_syn.filter_query(axon_mask).df,\n    client,\n    return_as=\"html\"\n)\n\n(Click one of the synapse annotations to load the neuron mesh).\nAnother common example might be to pick one of the child nodes of the soma and get all of the synapses on that branch. We can do this by using the nrn.skeleton.get_child_nodes function to get the skeleton vertex indices of the child nodes and then use that to filter the synapses.\n\nbranch_index = 0 # Let's just use the first child vertex of the root node, which is at the soma by default.\nbranch_inds = nrn.downstream_of(nrn.child_index(nrn.root)[branch_index])\nbranch_mask = branch_inds.to_mesh_mask\n\nmake_synapse_neuroglancer_link(\n    nrn.anno.post_syn.filter_query(branch_mask).df,\n    client,\n    return_as=\"html\"\n)",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Skeletons"
    ]
  },
  {
    "objectID": "programmatic_access/functional_tutorial/em_functional_intro.html",
    "href": "programmatic_access/functional_tutorial/em_functional_intro.html",
    "title": "MICrONs Tutorial",
    "section": "",
    "text": "(em:functional-data)= # MICrONS Functional Data\nThe MICrONs mouse went through a battery of functional imaging experiments before be prepared for electron microscopy. Excitatory neurons were imaged with 2-photon calcium imaging using GCaMP6s. Because of the size of the volume, different populations of neurons were imaged in different sessions, spanning several days. Each neuronal {term}`ROI`` from the functional data is uniquely determined based on the combination of image session, scan index, and ROI unit id. During each imaging session, the mouse was head-fixed and presented a variety of visual stimuli to the left visual field, including both natural movies and synthetic parametric movies. In addition to functional 2p imaging, the treadmill rotation, left eye position, and left pupil diameter were recorded to provide behavioral context."
  },
  {
    "objectID": "programmatic_access/functional_tutorial/em_functional_intro.html#stimuli-and-digital-twin",
    "href": "programmatic_access/functional_tutorial/em_functional_intro.html#stimuli-and-digital-twin",
    "title": "MICrONs Tutorial",
    "section": "Stimuli and “Digital twin”",
    "text": "Stimuli and “Digital twin”\nThe visual stimuli presented to the mouse included many natural movies, as well as parametric stimuli that were designed to emphasize specific visual features, such as orientation and spatial frequency. Because imaging sessions were performed over several days and imaging time in any one location was limited, comparisons between cells in different sessions are not straightforward. To get around this, the principle goal of this collection of stimuli was to be used as training data for deep neural networks that were trained to predict the response of each cell to an arbitrary stimulus. These so-called “digital twins” were designed to be used to probe functional responses to cells to stimuli that were outside the original training set. More details about this digital twin approach can be found in Wang et al. 2023.\nAs a measure of the reliability of visual responses, a collection of six movies totalling one minute were presented to the mouse ten times per imaging session. An oracle score was computed based on the signal correlation of a given cell to the oracle stimuli across the imaging session, where higher numbers indicate more reliable responses. Specifically, the oracle score is the mean signal correlation of the response of each presentation to the average of the other nine presentations."
  },
  {
    "objectID": "programmatic_access/functional_tutorial/em_functional_intro.html#coregistration",
    "href": "programmatic_access/functional_tutorial/em_functional_intro.html#coregistration",
    "title": "MICrONs Tutorial",
    "section": "Coregistration",
    "text": "Coregistration\nThe process of aligning the location of cells from the functional imaging with the same cells in the EM imaging is called coregistration. It is a challenging problem due to the need for micron-scale alignment of image volumes, despite signifiant differences in the imaging modalities, tissue deformations under different conditions and potential distortions introduced by sample preparation.\nThe dataset contains two approaches to coregistration, one semi-manual and one fully automatic. In the semi-manual approach, a transform between the EM data and the 2p data was generated based on fiducual points such as blood vessels and cell bodies. This transform was then applied to the functional data to identify a location in the EM space, and a human annotator then identified the cell body in the EM data that was a best match to the functional ROI based on location and context. In the fully automatic approach, blood vessels were segmented in both 2p and EM volumes, and a transform was generated based on matching this 3d structure. More details can be found in the MICrONS dataset preprint.\n\nCoregistration Quality Metrics\n\n\n\n```lvzdpexi img/coreg-metrics.png\n\n\n\n\nalign: center\n\n\n\nCartoon illustrating the coregistration metrics.\n\nTwo values are available to help assess the quality of the coregistration.\nThe **residual** indicates the distance between the location of an ROI after transformation to the EM space and the location of the matched cell body in the EM data.\nThe **score** (or separation score) is the distance between the matched cell body and the nearest other cell body in the EM data.\nThis attempts to measure how the residual compares with the distance to other potential matches in the data.\nLarger values indicate fewer potential matches and therefore a more confident match, in general.\n\nWhen using the automated coregistration, it is important to filter the data based on assignment confidence.\nA guide for this can be found by comparing the subset of cells matched by both the automated and manual coregistration methods.\n\n```{figure} img/coreg-agreement.png\n---\nalign: center\n---\nRelationship between separation threshold (left) and residual (right) and the accuracy of automated coregistration compared to manual coregistration. Orange curves depict the fraction of cells that remain after filtering out those matches beyond the threshold indicated on the x-axis.\n\n\nMatching EM to Function\nThe combination of session index, scan index, and ROI unit id uniquely identifies each ROI in the functional data. The annotation database contains tables with the results of each of the coregistration methods. Each row in each table contains the nucleus id, centroid, and root ID of an EM cell as well as the scan/session/unit indices required to match it. In additoin, the residual and score metrics for each match are provided to filter by quality. For manual coregistration, the table is called coregistration_manual_v3 and for automated coregistration, the table is called apl_functional_coreg_forward_v5.\nFull column definitions can be found on Annotation Tables page."
  },
  {
    "objectID": "programmatic_access/functional_tutorial/em_functional_intro.html#functional-data",
    "href": "programmatic_access/functional_tutorial/em_functional_intro.html#functional-data",
    "title": "MICrONs Tutorial",
    "section": "Functional data",
    "text": "Functional data\nA collection of in silico model responses of neurons to a variety of visual stimuli has been precomputed. The data can be found in a collection of files:\n\n:header-rows: 1 * - Filename - Format - Info * - nat_movie.npy - numpy.ndarray - dimensions=[clip, frame, height, width] * - nat_resp.npy - numpy.ndarray - dimensions=[unit, bin] * - nat_unit.csv - importable into pandas.DataFrame - columns=[animal_id, scan_session, scan_idx, unit_id, row_idx] * - monet_resp.npy - numpy.ndarray - dimensions=[unit, direction, trial] * - monet_dir.npy - numpy.ndarray - dimensions=[direction] * - monet_unit.csv - importable into pandas.DataFrame - columns=[animal_id, scan_session, scan_idx, unit_id]\n\nThe model response data is organized into two sets of files, one for natural movies and one for Monet stimuli, a type of parametric stimulus that measures orientation tuning. The .npy files can be read with the numpy function np.load and the .csv files with the pandas function pd.read_csv.\n\n\n\n```lvzdpexi img/function-stimulus.png\n\n\n\n\nalign: center\n\n\n\nExample images from a variety fo the stimuli used to probe functional responses. Natural movies include scenes from cinema, POV nature videos, and rendered 3d scenes. Monet stimuli are a parametic textured stimulus that varies in orientation and spatial frequency of correlated motion. ```\nThe natural movie data is organized into three files:\n\nnat_movie contains 250 10-sec clips of natural movies. Movies were shown to the model at a resolution of 72 * 128 pixels at 30 hz. This is downsampled from the in vivo presentation resolution of 144 * 256 pixels.\nnat_resp contains 104,171 units’ binned responses to natural movies. Responses were binned into 500 msec non-overlapping bins. This results in 250 clips * 10 sec / 500 msec = 5000 bins, with every binned response corresponding to 15 frames of natural movies.\nnat_unit contains the mapping from rows in nat_resp to functional unit keys. Since there is only a single animal for the EM data, use the session, scan, and unit id values to match to the EM data via one of the coregistration tables described above.\n\nThe Monet stimulus data is similarly organized into three files:\n\nmonet_resp contains 104,171 units’ mean responses to Monet stimuli. 120 trials of 16-direction Monet stimuli were shown to the model. The 120 trials were generated with different random seeds.\nmonet_dir contains the direction (in units of degrees) for the Monet stimulus shown to the model. The order of directions matches the order in the direction dimension of monet_resp. Here, 0 degrees is vertical feature moving leftwards, with degrees increases in the anti-clockwise direction.\nmonet_unit contains the functional unit keys of the 104,171 units. The order of the unit keys matches the order in the unit dimension of monet_resp. Since there is only a single animal for the EM data, use the session, scan, and unit id values to match to the EM data via one of the coregistration tables described above."
  }
]