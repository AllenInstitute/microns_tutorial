[
  {
    "objectID": "vortex-overview.html",
    "href": "vortex-overview.html",
    "title": "Virtual Observatory of the Cortex (VORTEX)",
    "section": "",
    "text": "A BRAIN initiative funded project to bring the MICrONS dataset to the scientific research community.\nIs there an observation you could make from this dataset that would help you answer a scientific question, but you aren’t sure how to make it? Or maybe you know how to make it, but don’t have the time or resources to repair or annotate enough examples to answer your question.\nThe Virtual Observatory of the Cortex is a grant funded mechanism designed to assist you in precisely this situation. We provide the labor and experience to make corrections and annotations in the data. The results of these efforts will be integrated with an evolving version of the dataset, and shared with you and the larger scientific community.\nWe think of it as a metaphor for a large scale astronomy survey: we are providing the infrastructure and focusing mechanisms, the community is steering the observations to the spots of maximum interest, and everyone can see what comes out!",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Virtual Observatory of the Cortex (VORTEX)"
    ]
  },
  {
    "objectID": "vortex-overview.html#how-it-works",
    "href": "vortex-overview.html#how-it-works",
    "title": "Virtual Observatory of the Cortex (VORTEX)",
    "section": "How it works:",
    "text": "How it works:\n1. Submit a Request\nMembers of the scientific community submits a short request for information they believe might be contained within this dataset, but might require some work to extract. For example:\n\nproofreading axons and dendrites for a specific group of neurons\nannotating the locations of dense core vesicles in these particular cells\nannotating locations where microglia have engulfed synapses\n\n2. Develop a Plan\nA scientist at the Allen Institute will work with you on developing a plan that helps address the scientific question you are interested in, including any manual proofreading and annotation work that needs to be done.\nPlans will be designed to maximize impact across all requests when possible.\n3. Plan Execution\nA Scientific Steering Group will evaluate all the plans and prioritize the manual work based upon the resources available to the virtual observatory.\nPlans will then be carried out by staff of the virtual observatory, with no costs to the requestor.\n4. Data Sharing\nThe data produced from the efforts of the manual proofreaders is then shared with the entire scientific community for everyone to benefit from.\nData releases will occur at least quarterly.\n\nData Sharing Policy\nData created by the Vortex project, such a proofreading and annotations, will be integrated into broader publicly available dataset for everyone to use. Proofreading updates the segmentation, which will be released along with any other changes that are part of the centralized dataset, no matter their source. Annotations will be made on the dataset and released as tables via the CAVEclient, and periodically as static file dumps available via google cloud. See the specific dataset pages for details on data access.\n\n\nSubmit the Form\nAccess the submission system at:\nmicrons-explorer.org/requests | Scientific Request Form\nWe are also conducting a community survey. If you are using, have used, or plan to use the MICrONS data, give us some feedback!\nMICrONS Community Survey",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Virtual Observatory of the Cortex (VORTEX)"
    ]
  },
  {
    "objectID": "vortex-overview.html#scientific-proposals-recieved",
    "href": "vortex-overview.html#scientific-proposals-recieved",
    "title": "Virtual Observatory of the Cortex (VORTEX)",
    "section": "Scientific Proposals Recieved",
    "text": "Scientific Proposals Recieved\n\nVORTEX Requests Recieved\n\n\nRecieved\nTopic\nTask\nStatus\n\n\n\n\n\nY1\nHow does the orientation tuning of visual cortical neurons arise from the functional organization of inputs onto their dendritic tree?\nAxon extension of functionally recorded cells\nComplete (111 cells): Y1: 1078; (547 cells): Y2: 1300\n\n\n\nY1\nHow does neural computation arise from the combination of inputs along the dendritic tree?\nAxon extension of functionally recorded cells\nComplete (111 cells): Y1: 1078; (547 cells): Y2: 1300\n\n\n\nY1\nTo what extent do interareal cortical connections exhibit precise recurrent interactions, and are these connection probabilities governed by shared retinotopy?\nAxon extension of functionally recorded cells\nComplete (111 cells): Y1: 1078; (547 cells): Y2: 1300\n\n\n\nY1\nWhat are the detailed connectivity properties of cortical PV basket cells?\nAnnotation of parvalbumin interneurons\nComplete (20 cells): Y1: 1078; (5 cells): Y2: 1300\n\n\n\nY1\nHow does the connectivity pattern of SST/Chodl cells give rise to their physiological feature: synchronization of cortical states?\nAxon extension of putative Chodl cells\nIncomplete: pending putative Chodl cells\n\n\n\nY1\nHow do astrocytes interact with neurons, at multiple scales of organization and operation?\nProofread ‘cleaning’ of astrocytes\nComplete (12 cells): Y1: 1078\n\n\n\nY1\nHow does the presence of vasculature affect the distribution of electric charge due to an applied electric field (brain stimulation)?\nAnalysis support\nComplete: Y1: 1078\n\n\n\nY1\nWhat is the mechanism of electrical neuro-vasculature coupling, at the microscopic level?\nAnalysis support\nComplete: Y1: 1078\n\n\n\nY1\nCensus of spinehead apparatus\nAnnotation of pyramidal spine inputs\nComplete (8 cells): Y2: 1300\n\n\n\nY1\nOPC spacing and cilia orientation\nOPC proofreading and annotation\nIncomplete: pending resources\n\n\n\nY1\nMyelin density in cortex\nAnnotation of myelin\nComplete: Y2: 1300\n\n\n\nY1\nPerivascular fibroblast\nAnalysis support\nComplete\n\n\n\nY1\nLocal environment of spine synapses\nAnnotation of pyramidal spine inputs\nComplete (8 cells): Y2: 1300\n\n\n\nY1\nInhibitory-Excitatory connectivity motifs\nAxon extenstion of functionally coregistered cells\nComplete (547 cells: Y2:1300\n\n\n\nY2\nProximity metrics for pyramidal cell dendrites\nAnalysis support\nOngoing\n\n\n\nY2\nSubcellular distribution of large secretory vesicles\nAnnotation of dense core vesicles\nOngoing\n\n\n\nY2\nNeuromodulation at the blood-brain barrier\nAxon extenstion of putative neuromodulatory axons\nOngoing\n\n\n\nY2\nNeuron structure modeling, using skeletonized neurons\nAxon extension: pyramidal cells ; Axon extension: inhibitory cells\nOngoing\n\n\n\nY2\nMicroglia interaction at synapses\nAnnotation of microglia lysosomes\nOngoing\n\n\n\nY2\nUncovering local and long-range circuit organization\nAxon extension: pyramidal cells ; Axon extension: inhibitory cells\nOngoing\n\n\n\nY2\nConnectomics of response heterogeneity in mouse visual cortex\nAxon extension of functionally recorded cells\nOngoing\n\n\n\nY2\nDesign principles underlying astrocyte morphology\nProofread ‘cleaning’ of astrocyte\nOngoing\n\n\n\nY2\nConnection between SST subtypes and pyramidal neurons\nAxon extension: inhibitory cells\nOngoing\n\n\n\nY2\nDopaminergic projections to cortex\nAxon extenstion of putative neuromodulatory axons\nOngoing",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Virtual Observatory of the Cortex (VORTEX)"
    ]
  },
  {
    "objectID": "vortex-overview.html#project-outcomes",
    "href": "vortex-overview.html#project-outcomes",
    "title": "Virtual Observatory of the Cortex (VORTEX)",
    "section": "Project Outcomes",
    "text": "Project Outcomes\n\nYear 1: Period 1\n\n\n\nVORTEX Proofreading Plans | Year 1 Period 1\n\n\nSSG Rank\nTask\n\n\n\n\n1.6\nAxon extension of functionally recorded cells\n\n\n2.0\nProofread ‘cleaning’ of astrocytes\n\n\n2.6\nVasculature Analysis support\n\n\n3.4\nAnnotation of parvalbumin interneurons\n\n\n4.6\nAxon extension of putative Chodl cells\n\n\n\n\n\n\n\n\nProofreading edits by type\n\n\n\n\n\nAxon extension of functionally recorded cells\n\nProofreading and extending functionally coregistered pyramidal cells\n\n\n\nSynaptic connectivity added: version 943 to 1078\n\n\n\n\n\nProofreading status upgraded: version 943 to 1078\n\n\nQueried from CAVE table: proofreading_status_and_strategy:\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()\n\nProofread ‘cleaning’ of astrocytes\n\nSelecting quality astrocytes within the V1 Column, and clean to remove false-merges: 12 astrocytes, 7 cleaned\n\n\n\nSix physically apposed ‘cleaned’ astrocytes\n\n\nCAVE table: vortex_astrocyte_proofreading_status\n# Standard query\nclient.materialize.query_table('vortex_astrocyte_proofreading_status')\n\n# Content-aware query\nclient.materialize.tables.vortex_astrocyte_proofreading_status(status='clean').query()\n\nAnnotation of parvalbumin interneurons\n\nManual annotation of spine and shaft synaptic targets of basket cells: 53517 annotations\nQueried from CAVE table: vortex_compartment_targets\n# Standard query\nclient.materialize.query_table('vortex_compartment_targets', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_compartment_targets(pre_pt_root_id=example_root_id).query()\nManual annotation of myelination and nodes of Ranvier along basket cell axons: 73237 annotations\n\n\n\nMyelination annotated along basket cell (putative PV) axon\n\n\nCAVE table: vortex_manual_myelination_v0\n# Standard query\nclient.materialize.query_table('vortex_manual_myelination_v0', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_manual_myelination_v0(valid_id=example_root_id).query()\nCAVE table: vortex_manual_nodes_of_ranvier\n# Standard query\nclient.materialize.query_table('vortex_manual_nodes_of_ranvier')\n\n# Content-aware query\nclient.materialize.tables.vortex_manual_nodes_of_ranvier(valid_id=example_root_id).query()\n\n\nYear 1: Period 2\n\n\n\nVORTEX Proofreading Plans | Year 1 Period 1{.light .hover}\n\n\nSSG Rank\nTask\n\n\n\n\n1.3\nForward-tracing of pyramidal cell axons\n\n\n1.7\nAnnotation of basket cell spine targets\n\n\n2\nAnnotation of pyramidal spine input\n\n\n3.7\nAnnotation of basket cell myelin\n\n\n5\nAnnotation of spine input (any cell type)\n\n\n5\nOPC proofreading and annotation\n\n\n5.7\nAnnotation of myelin (any cell type)\n\n\n\n\n\n\n\n\nProofreading edits by type\n\n\n\n\n\nAxon extension of functionally recorded cells\n\nProofreading and extending functionally coregistered pyramidal cells\n\n\n\nSynaptic connectivity added: version 1078 to 1300\n\n\n\n\n\nProofreading status upgraded: version 1078 to 1300\n\n\nWith the combined effort across Year 1 and Year 2, VORTEX proofreading has doubled the number of synapses between excitatory cells, and tripled the number of coregistered-coregistered connections\n\n\n\nImprovement in connectivity between functionally-coregistered pyramidal cells\n\n\nQueried from CAVE table: proofreading_status_and_strategy:\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()\n\nAnnotation of spine synapses\n\nManual annotation of spine and shaft synaptic targets of 1) basket cells, and 2) inputs onto pyramidal cells: 67440 annotations\n\n\n\nDense manual annotations on the inputs to a pyramidal cell\n\n\nQueried from CAVE table: vortex_compartment_targets\n# Standard query\nclient.materialize.query_table('vortex_compartment_targets', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_compartment_targets(pre_pt_root_id=example_root_id).query()\nProvided ground truth for a new automatic classifier:\n# Standard query\nclient.materialize.query_table('synapse_target_predictions_ssa', limit=10)\n\n# Content-aware query\nclient.materialize.tables.synapse_target_predictions_ssa(post_pt_root_id=example_root_id).query()\n\nAnnotation of myelin\n\nManual annotation of myelination and nodes of Ranvier 1) along basket cell axons, and 2) dense annotations within a subvolume: 26393 annotations\nCAVE table: vortex_manual_myelination_v0\n# Standard query\nclient.materialize.query_table('vortex_manual_myelination_v0', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_manual_myelination_v0(valid_id=example_root_id).query()\nCAVE table: vortex_manual_nodes_of_ranvier\n# Standard query\nclient.materialize.query_table('vortex_manual_nodes_of_ranvier')\n\n# Content-aware query\nclient.materialize.tables.vortex_manual_nodes_of_ranvier(valid_id=example_root_id).query()",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Virtual Observatory of the Cortex (VORTEX)"
    ]
  },
  {
    "objectID": "vortex-overview.html#contributions",
    "href": "vortex-overview.html#contributions",
    "title": "Virtual Observatory of the Cortex (VORTEX)",
    "section": "Contributions",
    "text": "Contributions\nVORTEX Community Manager: Bethanny Danskin\nProofreading Management: Szi-chieh Yu, Celia David\nProofreading: Erika Neace, Rachael Swanstrom, Agnes Bodor, Mai Dumale, Elanur Acmad, Al-Mishrie Sahijuan, Joselito Astrologo, Arvin Caballes, Rey Cabasag, Jovie Cano, Jessi Deocampo, Emely Deresas, Mark Encallado, Mary Krestine Guzarem, Hannie Grace Hordillo, Logic Domme Ibanez, Angeline Libre, Geronimo Mon, Geomar Pagalan, Joren Paulines, Joshua Angel Remullo, Rezi Selgas,\nPrincipal Investigators: Forrest Collman, Nuno Da Costa, R. Clay Reid, Casey Schneider-Mizell, H. Sebastian Seung\nNIH BRAIN Initiative: U24 NS120053",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Virtual Observatory of the Cortex (VORTEX)"
    ]
  },
  {
    "objectID": "release_manifests/version-943.html",
    "href": "release_manifests/version-943.html",
    "title": "943 (Jan 2024)",
    "section": "",
    "text": "Data Release v943 (January 22, 2024) is the first quarterly release of 2024.\nThis version ADDS the following tables\nThe following tables from previous versions are included:\nThe following tables have been REMOVED (removed tables still available in version 795):\nInformation about the version 943 release can be queried with the following:\n# report timestamp metadata on the version of interest\nclient.materialize.get_version_metadata(943)['time_stamp']\n\ndatetime.datetime(2024, 1, 22, 8, 10, 1, 497934, tzinfo=datetime.timezone.utc)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "943 (Jan 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-943.html#functional-properties-of-coregistered-cells",
    "href": "release_manifests/version-943.html#functional-properties-of-coregistered-cells",
    "title": "943 (Jan 2024)",
    "section": "Functional properties of coregistered cells",
    "text": "Functional properties of coregistered cells\nTable name: functional_properties_v3_bcm\nA summary of the functional properties for each of the coregistered neurons (as of coregistration_manual_v3). For details, see (Ding et al. 2025)\nThe key columns are:\n\nFuctional properties of coregistered neurons\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\nsession\nThe session index from functional imaging\n\n\nscan_idx\nThe scan index from functional imaging\n\n\nunit_id\nThe functional unit index from imaging. Only unique within scan and session\n\n\npref_ori\npreferred orientation in radians (0 - pi), horizontal bar moving upward is 0 and orientation increases clockwise, extracted from model responses to oriented noise stimuli\n\n\npref_dir\npreferred direction in radians (0 - 2pi), horizontal bar moving upward is 0 and orientation increases clockwise, extracted from model responses to oriented noise stimuli\n\n\ngOSI\nglobal orientation selectivity index\n\n\ngDSI\nglobal direction selectivity index\n\n\ncc_abs\nprediction performance of the model, higher is better\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis table includes duplicate entries for the same ‘pt_root_id’ and nucleus id if the coregistered cell has multiple unit recordings\n\n\n# Standard query\nclient.materialize.query_table('functional_properties_v3_bcm')\n\n# Content-aware query\nclient.materialize.tables.functional_properties_v3_bcm(id=example_nucleus_id).query()\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "943 (Jan 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-943.html#manual-coregistration",
    "href": "release_manifests/version-943.html#manual-coregistration",
    "title": "943 (Jan 2024)",
    "section": "Manual Coregistration",
    "text": "Manual Coregistration\nTable name: coregistration_manual_v4\nA table of EM nucleus centroids manually matched to Baylor functional units. A unique functional unit is identified by its session, scan_idx and unit_id. An EM nucleus centroid may have matched to more than one functional unit if it was scanned on more than one imaging field.\nThe key columns are:\n\nCoregistration table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nsession\nThe session index from functional imaging\n\n\nscan_idx\nThe scan index from functional imaging\n\n\nunit_id\nThe functional unit index from imaging. Only unique within scan and session\n\n\nfield\nThe field index from functional imaging\n\n\nresidual\nThe residual distance between the functional and the assigned structural points after transformation, in microns\n\n\nscore\nA separation score, measuring the difference between the residual distance to the assigned neuron and the distance to the nearest non-assigned neuron, in microns. This can be negative if the non-assigned neuron is closer than the assigned neuron. Larger values indicate fewer nearby neurons that could be confused with the assigned neuron.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis table includes duplicate entries for the same ‘pt_root_id’ and nucleus id if the coregistered cell has multiple unit recordings\n\n\n# Standard query\nclient.materialize.query_table('coregistration_manual_v4')\n\n# Content-aware query\nclient.materialize.tables.coregistration_manual_v4(id=example_nucleus_id).query()\nThis table coregistration_manual_v4 supercedes previous iterations of this table:\n\ncoregistration_manual_v3\ncoregistration_manual\n\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "943 (Jan 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-943.html#automated-coregistration",
    "href": "release_manifests/version-943.html#automated-coregistration",
    "title": "943 (Jan 2024)",
    "section": "Automated Coregistration",
    "text": "Automated Coregistration\nTable name: coregistration_auto_phase3_fwd_apl_vess_combined\nA table of EM nucleus centroids automatically matched to Baylor functional units. This table reconciles the following two tables that both make a best match of the of registration using different techniques: coregistration_auto_phase3_fwd and apl_functional_coreg_vess_fwd. A unique functional unit is identified by its session, scan_idx and unit_id. An EM nucleus centroid may have matched to more than one functional unit if it was scanned on more than one imaging field.\nThe key columns are:\n\nCoregistration table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nsession\nThe session index from functional imaging\n\n\nscan_idx\nThe scan index from functional imaging\n\n\nunit_id\nThe functional unit index from imaging. Only unique within scan and session\n\n\nfield\nThe field index from functional imaging\n\n\nresidual\nThe residual distance between the functional and the assigned structural points after transformation, in microns\n\n\nscore\nA separation score, measuring the difference between the residual distance to the assigned neuron and the distance to the nearest non-assigned neuron, in microns. This can be negative if the non-assigned neuron is closer than the assigned neuron. Larger values indicate fewer nearby neurons that could be confused with the assigned neuron.\n\n\n\n\n\n\n\n\n\nDeprecated table\n\n\n\nThis table remains available from materialization versions: 943, 1078, 1181\nUse coregistration_auto_phase3_fwd_apl_vess_combined_v2 instead.\nThis table includes duplicate entries for the same pt_root_id and nucleus id if the coregistered cell has multiple unit recordings\n\n\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "943 (Jan 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-943.html#flat-segmentation",
    "href": "release_manifests/version-943.html#flat-segmentation",
    "title": "943 (Jan 2024)",
    "section": "Flat Segmentation",
    "text": "Flat Segmentation\n\nminnie65 flat segmentation v943\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nProofread Segmentation (v943)\nminnie65\nhttps://storage.googleapis.com/iarpa_microns/minnie/minnie65/seg_m943\nMulit-resolution flat / static cellular segmentation voxels and meshes from 8,8,40 nm and above\nPrecomputed Shareded Compressed Segmentation (12 TB)\n\n\n\nThis contains the fixed state of the cellular segmentation at each version, where each voxel has been assigned an ID which is unique to each cellular object at 8,8,40, along with downsampled versions. Not all objects have been proofread, but a summary of the most focused efforts on cells can be found in the proofreading status metadata. In addition the mesh folder contains meshes of each object available at 3 different levels of downsampling. Folder contains many files, for download use cloud-volume, tensor-store, for bulk download use igneous, AWS CLI or gsutil CLI.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "943 (Jan 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-661.html",
    "href": "release_manifests/version-661.html",
    "title": "661 (Jun 2023)",
    "section": "",
    "text": "Data Release v661 (April 6, 2023) is the first quarterly release of 2023.\nThis version ADDS the following tables\nThe following tables from previous versions are included:\nThe following tables have been REMOVED (removed tables still available in version 343):\nInformation about the version 661 release can be queried with the following:\n# report timestamp metadata on the version of interest\nclient.materialize.get_version_metadata(661)['time_stamp']\n\ndatetime.datetime(2023, 4, 6, 20, 17, 9, 199182, tzinfo=datetime.timezone.utc)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "661 (Jun 2023)"
    ]
  },
  {
    "objectID": "release_manifests/version-661.html#manual-cell-types-v1-column",
    "href": "release_manifests/version-661.html#manual-cell-types-v1-column",
    "title": "661 (Jun 2023)",
    "section": "Manual Cell Types (V1 Column)",
    "text": "Manual Cell Types (V1 Column)\n\nNeurons\nTable name: allen_v1_column_types_slanted_ref\nA subset of nucleus detections in a 100 um column (n=2204) in VISp were manually classified by anatomists at the Allen Institute into categories of cell subclasses, first distinguishing cells into classes of non-neuronal, excitatory and inhibitory; then into subclasses.\nFor the non-neuronal subclasses, see aibs_column_nonneuronal_ref\nThe key columns are:\n\nAIBS Manual Cell Types, V1 Column\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nOne of aibs_coarse_excitatory or aibs_coarse_inhibitory for detected neurons, or aibs_coarse_nonneuronal for non-neurons (glia/pericytes).\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\nThe cell types in the table are:\n\n\n\n\n\n\nManual Cell Types (neurons)\n\n\n\n\n\n\nAIBS Manual Cell Type definitions (neurons)\n\n\n\n\n\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\n23P\nExcitatory\nLayer 2/3 cells\n\n\n4P\nExcitatory\nLayer 4 cells\n\n\n5P-IT\nExcitatory\nLayer 5 intratelencephalic cells\n\n\n5P-ET\nExcitatory\nLayer 5 extratelencephalic cells\n\n\n5P-NP\nExcitatory\nLayer 5 near-projecting cells\n\n\n6P-IT\nExcitatory\nLayer 6 intratelencephalic cells\n\n\n6P-CT\nExcitatory\nLayer 6 corticothalamic cells\n\n\nBC\nInhibitory\nBasket cell\n\n\nBPC\nInhibitory\nBipolar cell. In practice, this was used for all cells thought to be VIP cell, not only those with a bipolar dendrite\n\n\nMC\nInhibitory\nMartinotti cell. In practice, this label was used for all inhibitory neurons that appeared to be Somatostatin cell, not only those with a Martinotti cell morphology\n\n\nUnsure\nInhibitory\nUnsure. In practice, this label also is used for all likely-inhibitory neurons that did not match other types\n\n\n\n\n\n\n# Standard query\nclient.materialize.query_table('allen_v1_column_types_slanted_ref')\n\n# Content-aware query\nclient.materialize.tables.allen_v1_column_types_slanted_ref(id=example_nucleus_id).query()\n\n\nNon-neurons\nTable name: aibs_column_nonneuronal_ref\nA subset of nucleus detections in a 100 um column (n=2204) in VISp were manually classified by anatomists at the Allen Institute into categories of cell subclasses, first distinguishing cells into classes of non-neuronal, excitatory and inhibitory; then into subclasses.\nFor the neuronal subclasses, see allen_v1_column_types_slanted_ref\nThe key columns are:\n\nAIBS Manual Cell Types, V1 Column\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nOne of aibs_coarse_excitatory or aibs_coarse_inhibitory for detected neurons, or aibs_coarse_nonneuronal for non-neurons (glia/pericytes).\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\nThe cell types in the table are:\n\n\n\n\n\n\nManual Cell Types (non-neurons)\n\n\n\n\n\n\nAIBS Manual Cell Type definitions (non-neurons)\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\nOPC\nNon-neuronal\nOligodendrocyte precursor cell\n\n\nastrocyte\nNon-neuronal\nAstrocyte\n\n\nmicroglia\nNon-neuronal\nMicroglia\n\n\npericyte\nNon-neuronal\nPericyte\n\n\noligo\nNon-neuronal\nOligodendrocyte\n\n\n\n\n\n\n# Standard query\nclient.materialize.query_table('aibs_column_nonneuronal_ref')\n\n# Content-aware query\nclient.materialize.tables.aibs_column_nonneuronal_ref(id=example_nucleus_id).query()\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "661 (Jun 2023)"
    ]
  },
  {
    "objectID": "release_manifests/version-661.html#automated-cell-types",
    "href": "release_manifests/version-661.html#automated-cell-types",
    "title": "661 (Jun 2023)",
    "section": "Automated Cell Types",
    "text": "Automated Cell Types\n\nmtypes\nTable name: aibs_soma_nuc_exc_mtype_preds_v117\nThis table contains excitatory M-type predictions for cells throughout the entire dataset (Schneider-Mizell et al. 2025). The model was trained with soma and nucleus features (Elabbady et al. 2025). The cells selected were those that were predicted as excitatory based on the soma and nucleus feature trained metamodel - these prediction are available in the aibs_soma_nuc_metamodel_preds_v117 table. This is a reference table where target_id refers to the unique nucleus id in the nucleus_detection_v0 table. Classification_system refers to the coarse predictions from the soma and nucleus model and cell_type denotes the excitatory M-type prediction. Errors and soma-soma mergers have been filtered out based on the status of cells in materialization version 117. For more details, see (Schneider-Mizell et al. 2025).\nThe key columns are:\n\nAllen Motif-type (mtype) Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nexcitatory or inhibitory\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\n\n\n\n\n\n\nDeprecated table\n\n\n\nThis table remains available from materialization versions: 661, 795\nThis table is deprecated, use aibs_metamodel_mtypes_v661_v2 instead.\n\n\n\n\nmtypes (V1 column)\nTable name: allen_column_mtypes_v1\nClustering of anatomical properties (and connectivity for inhibitory cells) based on morphological features on cells in the V1 column, with segmentation as of v117. For more details, see (Schneider-Mizell et al. 2025).\nThe key columns are:\n\nAllen Motif-type (mtype) Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nexcitatory or inhibitory\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\n\n\n\n\n\n\nDeprecated table\n\n\n\nThis table remains available from materialization versions: 661, 795\nThis table is deprecated, use aibs_metamodel_mtypes_v661_v2 instead.\n\n\n\n\nmtypes connectivity groups\nTable name: connectivity_groups_v507\nInhibitory column connectivity ‘motif’ groups (cells that distribute their outputs onto target cell types in a similar way) for the V1 column.\nFor details, see (Schneider-Mizell et al. 2025).\nThe key columns are:\n\nAllen Inhibitory Motif-type (mtype) Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nconnectivity_group\n\n\ncell_type\nconnectivity motif type\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('connectivity_groups_v507')\n\n# Content-aware query\nclient.materialize.tables.connectivity_groups_v507(id=example_nucleus_id).query()\n\n\nBaylor (coarse)\nTable name: baylor_log_reg_cell_type_coarse_v1\nThis table contains the results of a logistic regression classifier trained on properties of neuronal dendrites. This was applied to many cells in the dataset, but required more data than soma and nucleus features alone and thus more cells did not complete the pipeline. It has very good performance on excitatory vs inhibitory neurons because it focuses on dendritic spines, a characteristic property of excitatory neurons. It is a good table to double check E/I classifications if in doubt.\nFor details, see (Celii et al. 2025).\nThe key columns are:\n\nBaylor Coarse Cell Type Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nbaylor_log_reg_cell_type_coarse for all entries\n\n\ncell_type\nexcitatory or inhibitory\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('baylor_log_reg_cell_type_coarse_v1')\n\n# Content-aware query\nclient.materialize.tables.baylor_log_reg_cell_type_coarse_v1(id=example_nucleus_id).query()\n\n\nBaylor (fine)\nTable name: baylor_gnn_cell_type_fine_model_v2\nExciatory/Inhibitory Subclass cell types derived from a supervised GNN classifier trained on the hand labeled cell types(‘23P, 4P, 5P-IT, 5P-NP, 5P-PT, 6P-CT, 6P-IT, BC, BPC, MC, NGC’). Process for generating labels\n\nCells Automatically Proofread\n\nGraph Objects generated for each neuron with the following node attributes (no z coordinate information included):\n\n\n\nskeleton information (‘skeletal_length’, ‘skeleton_vector_upstream_theta’, ‘skeleton_vector_upstream_phi’ , ‘skeleton_vector_downstream_theta’, ‘skeleton_vector_downstream_phi’),\n\nwidth information (‘width_upstream’, ‘width_no_spine’, ‘width_downstream’),\n\nsynapse information(‘n_synapses_post’, ‘n_synapses_pre’, ‘n_synapses_head_postsyn’, ‘n_synapses_neck_postsyn’, ‘n_synapses_shaft_postsyn’, ‘n_synapses_no_head_postsyn’, ‘synapse_volume_shaft_postsyn_sum’, ‘synapse_volume_head_postsyn_sum’, ‘synapse_volume_no_head_postsyn_sum’, ‘synapse_volume_neck_postsyn_sum’, ‘synapse_volume_postsyn_sum’,)\nspine information (‘n_spines’, ‘spine_volume_sum’,),\n\nglobal neuron information (“soma_start_angle_max”, “max_soma_volume”, “n_syn_soma”)\n\n\n\nGCN trained on labeled data with 60:20:20 training/validation/testing split\n\nGCN model generates class probabiliites and highest probability label given\n\nFor details, see (Celii et al. 2025).\nThe key columns are:\n\nBaylor Fine Cell Type Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nbaylor_gnn_cell_type_fine for all entries\n\n\ncell_type\none of: (‘23P, 4P, 5P-IT, 5P-NP, 5P-PT, 6P-CT, 6P-IT, BC, BPC, MC, NGC’)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('baylor_gnn_cell_type_fine_model_v2')\n\n# Content-aware query\nclient.materialize.tables.baylor_gnn_cell_type_fine_model_v2(id=example_nucleus_id).query()",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "661 (Jun 2023)"
    ]
  },
  {
    "objectID": "release_manifests/version-661.html#functional-coregistration",
    "href": "release_manifests/version-661.html#functional-coregistration",
    "title": "661 (Jun 2023)",
    "section": "Functional Coregistration",
    "text": "Functional Coregistration\n\nManual Coregistration\nTable name: coregistration_manual_v3\nA table of EM nucleus centroids manually matched to Baylor functional units. A unique functional unit is identified by its session, scan_idx and unit_id. An EM nucleus centroid may have matched to more than one functional unit if it was scanned on more than one imaging field.\n\n\n\n\n\n\nDeprecated table\n\n\n\nThis table remains available from materialization versions: 661, 795, 943, 1078\nUse coregistration_manual_v4 instead.\n\n\n\n\nAutomated Coregistration\nTable name: apl_functional_coreg_forward_v5\nTable tracking automated functional matches from EM-&gt;2P using an automated diffeomorphic-based approach. The “score” column represents the match separation in microns. Functional matching done by Justin Joyce, JHUAPL. Table managed by Daniel Xenes, JHUAPL.\n\n\n\n\n\n\nDeprecated table\n\n\n\nThis table remains available from materialization versions: 661\nUse apl_functional_coreg_vess_fwd instead.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "661 (Jun 2023)"
    ]
  },
  {
    "objectID": "release_manifests/version-661.html#nucleus",
    "href": "release_manifests/version-661.html#nucleus",
    "title": "661 (Jun 2023)",
    "section": "Nucleus",
    "text": "Nucleus\n\nNeuron nucleus\nTable name: nucleus_ref_neuron_svm\nWhile the table of centroids for all nuclei is nucleus_detection_v0, this includes neuronal nuclei, non-neuronal nuclei, and some erroneous detections. The table nucleus_ref_neuron_svm shows the results of a classifier that was trained to distinguish neuronal nuclei from non-neuronal nuclei and errors. For the purposes of analysis, we recommend using the nucleus_ref_neuron_svm table to get the most broad collection of neurons in the dataset.\nThe key columns of nucleus_ref_neuron_svm are:\n\nNucleus table column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\nclassification-system\nDescribes how the classification was done. All values will be is_neuron for this table\n\n\ncell_type\nThe output of the classifier. All values will be either neuron or not-neuron (glia or error) for this table\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('nucleus_ref_neuron_svm')\n\n# Content-aware query\nclient.materialize.tables.nucleus_ref_neuron_svm(target_id=example_nucleus_id).query()\n\n\nNucleus alternative points\nTable name: nucleus_alternative_points\nA table with alternative segid lookup points for nuclei created using an automated method which looked for nuclei centroids outside nuclie, or centroids which were not in the segmentation. The point at least 320 nm within the nucleus and the segmentation, and closet to the original centroid was choosen in a 3um window.\nThe key columns of are:\n\nNucleus alternative points\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nSoma ID for the cell\n\n\npt_position\nBound spatial point associated with the centroid of the nucleus (manually corrected)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a corrections table, and meant to act upon its reference nucleus_detection_v0.\nUse client.materialize.views.nucleus_detection_lookup_v1().query() to access the combined table.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "661 (Jun 2023)"
    ]
  },
  {
    "objectID": "release_manifests/version-1412.html",
    "href": "release_manifests/version-1412.html",
    "title": "1412 (Apr 2025)",
    "section": "",
    "text": "Data Release v1412 (April 29, 2025) is the second quarterly release of 2025.\nThis version ADDS the following tables\nThe following tables have updated status or entries, from VORTEX proofreading and annotations\nThe following tables from previous versions are included:\nInformation about the version 1412 release can be queried with the following:\n# report timestamp metadata on the version of interest\nclient.materialize.get_version_metadata(1412)['time_stamp']\n\ndatetime.datetime(2025, 4, 29, 10, 10, 1, 200893, tzinfo=datetime.timezone.utc)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1412 (Apr 2025)"
    ]
  },
  {
    "objectID": "release_manifests/version-1412.html#spines-with-multiple-synaptic-inputs",
    "href": "release_manifests/version-1412.html#spines-with-multiple-synaptic-inputs",
    "title": "1412 (Apr 2025)",
    "section": "Spines with multiple synaptic inputs",
    "text": "Spines with multiple synaptic inputs\nTable name: multi_input_spine_predictions_ssa\nSpine-predicted objects with more than one input synapse prediction. Based on the same model used in synapse_target_predictions_ssa. The model is a random forest trained on spectral shape analysis features from the mesh. Current failure modes include some stubby spines being predicted as shafts, and some very thin dendrites being classified as spines.\nIt contains the following columns:\n\nSynapse target predictions column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npre_pt_position / pre_pt_supervoxel_id / pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse.\n\n\npost_pt_position / post_pt_supervoxel_id / post_pt_root_id\nThe bound spatial point data for the postsynaptic side of the synapse.\n\n\nsize\nThe size of the synapse in voxels. This correlates well, but not perfectly, with the surface area of synapse.\n\n\nctr_pt_position\nA position in the center of the detected synaptic junction. Of all points in the synapse table, this is usually the closest point to the surface (and thus mesh) of both neurons. Because it is at the edge of cells, it is not associated with a root id.\n\n\ngroup_id\na unique iterator for shared postsynaptic compartments. Use pandas.groupby() to associate synapses on the same target\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on synapses_pni_2, and can be indexed by the same synapse id.\n\n\n# Standard query\nclient.materialize.query_table('multi_input_spine_predictions_ssa', limit=10)\n\n# Content-aware query\nclient.materialize.tables.multi_input_spine_predictions_ssa(post_pt_root_id=example_root_id).query().value_counts('group_id')\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1412 (Apr 2025)"
    ]
  },
  {
    "objectID": "release_manifests/version-1412.html#vortex-efforts",
    "href": "release_manifests/version-1412.html#vortex-efforts",
    "title": "1412 (Apr 2025)",
    "section": "VORTEX Efforts",
    "text": "VORTEX Efforts\n\nProofreading Status and Strategy\nTable name: proofreading_status_and_strategy\nThe table proofreading_status_and_strategy describes the status of cells that have undergone manual proofreading.\nBecause of the inherent difference in the difficulty and time required for different kinds of proofreading, we describe the status of axons and dendrites separately.\nEach compartment status may be either:\n\nFALSE: indicates no comprehensive proofreading has been performed, or is not applicable.\n\nTRUE: indicates that false merges have been comprehensively removed, and the compartment is at least ‘clean’. Consult the strategy column if completeness of the compartment is relevant to your research.\n\nAn axon or dendrite labeled as status=TRUE can be trusted to be correct, but may not be complete. The degree of completion can be read from the strategy column. For more information, please see Proofreading and Data Quality; or also the microns-explorer page on proofreading strategies.\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made. NOTE: if this does not match the pt_root_id then the cell has undergone further changes. This is usually and improvement in proofreading, but proceed with caution.\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. May be TRUE or FALSE\n\n\nstatus_axon\nThe status of the axon proofreading. May be TRUE or FALSE\n\n\nstrategy_dendrite\nThe strategy employed to proofread the dendrite. See strategy table below for details\n\n\nstrategy_axon\nThe strategy employed to proofread the axon. See strategy table below for details\n\n\n\nThe specific strategies are as follows (and will update over time):\n\nProofreading Strategies\n\nProofreading Strategy Table\n\n\n\n\n\n\nStrategy\nDescription\n\n\n\n\nnone\nNo cleaning, and no extension. Indicates an entry in proofreading_status that is FALSE for that compartment\n\n\ndendrite_clean\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. The dendrite may be incorrectly truncated by segmentation error. Not all dendrite tips have been checked for extension. No comprehensive attempt was made to re-attach spine heads.\n\n\ndendrite_extended\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. Every tip was identified, manually inspected, and extended if possible. No comprehensive attempt was made to re-attach spine heads.\n\n\naxon_column_truncated\nAThe axon was extended within the V1 cortical column, with a preference for local connections. In some cases the axon was cut at the column boundary and/or the layer boundary, especially the boundary between layers 2/3 and layer 4. Output synapses represent a sampling of potential partners\n\n\naxon_interareal\nThe axon was extended with a preference for branches that projected to other brain areas. Some axon branches were fully extended, but local connections may be incomplete. Output synapses represent a sampling of potential partners.\n\n\naxon_partially_extended\nThe axon was extended outward from the soma, following each branch to its termination. Output synapses represent a sampling of potential partners.\n\n\naxon_fully_extended\nAxon was extended outward from the soma, following each branch to its termination. After initial extension, every endpoint was identified, manually inspected, and extended again if possible. Output synapses represent a largely complete sampling of partners.\n\n\n\nThis table, proofreading_status_and_strategy, supercedes proofreading_status_public_release.\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()\n\n\n\nVORTEX proofread astrocytes\nTable name: vortex_astrocyte_proofreading_status\nManual screening and proofreading of astrocytes as part of the as part of the VORTEX project.\nThe key columns are:\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nstatus\nOne of ‘non’ indicating no cleaning or extension; ‘clean’ indicating neurite and non-astrocytic elements have been manually removed; ‘extended’ meaning astrocytic processes within the volume of the astrocyte have been comprehensively reviewed and assigned to the most likely cell\n\n\nvalid_id\nThe root id of the astrocyte when it the proofreading assessment was made\n\n\n\n# Standard query\nclient.materialize.query_table('vortex_astrocyte_proofreading_status')\n\n# Content-aware query\nclient.materialize.tables.vortex_astrocyte_proofreading_status(status='clean').query()\n\n\nVORTEX proofread thalamic axons\nTable name: vortex_thalamic_proofreading_status\nThis table reports the status of a manually selected subset of putative thalamocortical axons as part of the as part of the VORTEX project.\nThe key columns are:\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the entry point of the axon into the EMvolume\n\n\nstatus\nOne of ‘non’ indicating no cleaning or extension; ‘clean’ indicating synapses can be trusted but may not be complete; ‘extended’ meaning all end points visited and extended to completion\n\n\nvalid_id\nThe root id of the axon when it the proofreading assessment was made\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThalamic axons in this table that are complete and extended are also posted to proofreading_status_and_strategy with strategy: axon_fully_extended and dendrite_none.\n\n\n# Standard query\nclient.materialize.query_table('vortex_thalamic_proofreading_status')\n\n# Content-aware query\nclient.materialize.tables.vortex_thalamic_proofreading_status(status='extended').query()\nFor more on how to interpret the table, see Annotation Tables",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1412 (Apr 2025)"
    ]
  },
  {
    "objectID": "release_manifests/version-1181.html",
    "href": "release_manifests/version-1181.html",
    "title": "1181 (Sep 2024)",
    "section": "",
    "text": "Data Release v1181 (September 16, 2024) is the third quarterly release of 2024.\nThis release added no new tables.\nThe following tables have updated status or entries, from VORTEX proofreading and annotations:\nThe following tables from previous versions are included:\nThe following tables have been REMOVED (removed tables still available in version 1078):\nInformation about the version 1181 release can be queried with the following:\n# report timestamp metadata on the version of interest\nclient.materialize.get_version_metadata(1181)['time_stamp']\n\ndatetime.datetime(2024, 9, 16, 10, 10, 1, 121167, tzinfo=datetime.timezone.utc)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1181 (Sep 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-1181.html#vortex-efforts",
    "href": "release_manifests/version-1181.html#vortex-efforts",
    "title": "1181 (Sep 2024)",
    "section": "VORTEX Efforts",
    "text": "VORTEX Efforts\n\nProofreading Status and Strategy\nTable name: proofreading_status_and_strategy\nThe table proofreading_status_and_strategy describes the status of cells that have undergone manual proofreading.\nBecause of the inherent difference in the difficulty and time required for different kinds of proofreading, we describe the status of axons and dendrites separately.\nEach compartment status may be either:\n\nFALSE: indicates no comprehensive proofreading has been performed, or is not applicable.\n\nTRUE: indicates that false merges have been comprehensively removed, and the compartment is at least ‘clean’. Consult the strategy column if completeness of the compartment is relevant to your research.\n\nAn axon or dendrite labeled as status=TRUE can be trusted to be correct, but may not be complete. The degree of completion can be read from the strategy column. For more information, please see Proofreading and Data Quality; or also the microns-explorer page on proofreading strategies.\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made. NOTE: if this does not match the pt_root_id then the cell has undergone further changes. This is usually and improvement in proofreading, but proceed with caution.\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. May be TRUE or FALSE\n\n\nstatus_axon\nThe status of the axon proofreading. May be TRUE or FALSE\n\n\nstrategy_dendrite\nThe strategy employed to proofread the dendrite. See strategy table below for details\n\n\nstrategy_axon\nThe strategy employed to proofread the axon. See strategy table below for details\n\n\n\nThe specific strategies are as follows (and will update over time):\n\nProofreading Strategies\n\nProofreading Strategy Table\n\n\n\n\n\n\nStrategy\nDescription\n\n\n\n\nnone\nNo cleaning, and no extension. Indicates an entry in proofreading_status that is FALSE for that compartment\n\n\ndendrite_clean\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. The dendrite may be incorrectly truncated by segmentation error. Not all dendrite tips have been checked for extension. No comprehensive attempt was made to re-attach spine heads.\n\n\ndendrite_extended\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. Every tip was identified, manually inspected, and extended if possible. No comprehensive attempt was made to re-attach spine heads.\n\n\naxon_column_truncated\nAThe axon was extended within the V1 cortical column, with a preference for local connections. In some cases the axon was cut at the column boundary and/or the layer boundary, especially the boundary between layers 2/3 and layer 4. Output synapses represent a sampling of potential partners\n\n\naxon_interareal\nThe axon was extended with a preference for branches that projected to other brain areas. Some axon branches were fully extended, but local connections may be incomplete. Output synapses represent a sampling of potential partners.\n\n\naxon_partially_extended\nThe axon was extended outward from the soma, following each branch to its termination. Output synapses represent a sampling of potential partners.\n\n\naxon_fully_extended\nAxon was extended outward from the soma, following each branch to its termination. After initial extension, every endpoint was identified, manually inspected, and extended again if possible. Output synapses represent a largely complete sampling of partners.\n\n\n\nThis table, proofreading_status_and_strategy, supercedes proofreading_status_public_release.\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()\n\n\n\nVORTEX synapse compartments\nTable name: vortex_compartment_targets\nManual labeling of synaptic targets of putative parvalbumin basket cells, and inputs to pyramidal cells as part of the VORTEX project. This is a reference table on the synapses table synapses_pni_2, and provided ground truth for the automatic classifier behind synapse_target_predictions_ssa.\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nreference id of the synapse in synapses_pni_2\n\n\ntag\nManual label of synapse type. One of: ‘spine’, ‘shaft’, ‘soma’, ‘soma_spine’, ‘orphan’, ‘other’. Orphan refers to orphan neurites; other is a catch-all for synapses that do not match other labels\n\n\npre_pt_position  pre_pt_supervoxel_id  pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse\n\n\npost_pt_position  post_pt_supervoxel_id  post_pt_root_id\nThe bound spatial point data for the postsynaptic side of the synapse\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on synapses_pni_2, and can be indexed by the same synapse id.\n\n\n# Standard query\nclient.materialize.query_table('vortex_compartment_targets', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_compartment_targets(pre_pt_root_id=example_root_id).query()\n\n\nVORTEX proofread astrocytes\nTable name: vortex_astrocyte_proofreading_status\nManual screening and proofreading of astrocytes as part of the as part of the VORTEX project.\nThe key columns are:\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nstatus\nOne of ‘non’ indicating no cleaning or extension; ‘clean’ indicating neurite and non-astrocytic elements have been manually removed; ‘extended’ meaning astrocytic processes within the volume of the astrocyte have been comprehensively reviewed and assigned to the most likely cell\n\n\nvalid_id\nThe root id of the astrocyte when it the proofreading assessment was made\n\n\n\n# Standard query\nclient.materialize.query_table('vortex_astrocyte_proofreading_status')\n\n# Content-aware query\nclient.materialize.tables.vortex_astrocyte_proofreading_status(status='clean').query()\nFor more on how to interpret the table, see Annotation Tables",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1181 (Sep 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-1078.html",
    "href": "release_manifests/version-1078.html",
    "title": "1078 (Jun 2024)",
    "section": "",
    "text": "Data Release v1078 (June 5, 2024) is the second quarterly release of 2024.\nThis version ADDS the following tables\nThe following tables from previous versions are included:\nInformation about the version 1078 release can be queried with the following:\n# report timestamp metadata on the version of interest\nclient.materialize.get_version_metadata(1078)['time_stamp']\n\ndatetime.datetime(2024, 6, 5, 10, 10, 1, 203215, tzinfo=datetime.timezone.utc)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1078 (Jun 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-1078.html#vortex-efforts",
    "href": "release_manifests/version-1078.html#vortex-efforts",
    "title": "1078 (Jun 2024)",
    "section": "VORTEX Efforts",
    "text": "VORTEX Efforts\n\nProofreading Status and Strategy\nTable name: proofreading_status_and_strategy\nThe table proofreading_status_and_strategy describes the status of cells that have undergone manual proofreading.\nBecause of the inherent difference in the difficulty and time required for different kinds of proofreading, we describe the status of axons and dendrites separately.\nEach compartment status may be either:\n\nFALSE: indicates no comprehensive proofreading has been performed, or is not applicable.\n\nTRUE: indicates that false merges have been comprehensively removed, and the compartment is at least ‘clean’. Consult the strategy column if completeness of the compartment is relevant to your research.\n\nAn axon or dendrite labeled as status=TRUE can be trusted to be correct, but may not be complete. The degree of completion can be read from the strategy column. For more information, please see Proofreading and Data Quality; or also the microns-explorer page on proofreading strategies.\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made. NOTE: if this does not match the pt_root_id then the cell has undergone further changes. This is usually and improvement in proofreading, but proceed with caution.\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. May be TRUE or FALSE\n\n\nstatus_axon\nThe status of the axon proofreading. May be TRUE or FALSE\n\n\nstrategy_dendrite\nThe strategy employed to proofread the dendrite. See strategy table below for details\n\n\nstrategy_axon\nThe strategy employed to proofread the axon. See strategy table below for details\n\n\n\nThe specific strategies are as follows (and will update over time):\n\nProofreading Strategies\n\nProofreading Strategy Table\n\n\n\n\n\n\nStrategy\nDescription\n\n\n\n\nnone\nNo cleaning, and no extension. Indicates an entry in proofreading_status that is FALSE for that compartment\n\n\ndendrite_clean\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. The dendrite may be incorrectly truncated by segmentation error. Not all dendrite tips have been checked for extension. No comprehensive attempt was made to re-attach spine heads.\n\n\ndendrite_extended\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. Every tip was identified, manually inspected, and extended if possible. No comprehensive attempt was made to re-attach spine heads.\n\n\naxon_column_truncated\nAThe axon was extended within the V1 cortical column, with a preference for local connections. In some cases the axon was cut at the column boundary and/or the layer boundary, especially the boundary between layers 2/3 and layer 4. Output synapses represent a sampling of potential partners\n\n\naxon_interareal\nThe axon was extended with a preference for branches that projected to other brain areas. Some axon branches were fully extended, but local connections may be incomplete. Output synapses represent a sampling of potential partners.\n\n\naxon_partially_extended\nThe axon was extended outward from the soma, following each branch to its termination. Output synapses represent a sampling of potential partners.\n\n\naxon_fully_extended\nAxon was extended outward from the soma, following each branch to its termination. After initial extension, every endpoint was identified, manually inspected, and extended again if possible. Output synapses represent a largely complete sampling of partners.\n\n\n\nThis table, proofreading_status_and_strategy, supercedes proofreading_status_public_release.\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()\n\n\n\nVORTEX proofread astrocytes\nTable name: vortex_astrocyte_proofreading_status\nManual screening and proofreading of astrocytes as part of the as part of the VORTEX project.\nThe key columns are:\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nstatus\nOne of ‘non’ indicating no cleaning or extension; ‘clean’ indicating neurite and non-astrocytic elements have been manually removed; ‘extended’ meaning astrocytic processes within the volume of the astrocyte have been comprehensively reviewed and assigned to the most likely cell\n\n\nvalid_id\nThe root id of the astrocyte when it the proofreading assessment was made\n\n\n\n# Standard query\nclient.materialize.query_table('vortex_astrocyte_proofreading_status')\n\n# Content-aware query\nclient.materialize.tables.vortex_astrocyte_proofreading_status(status='clean').query()\n\n\nVORTEX synapse compartments\nTable name: vortex_compartment_targets\nManual labeling of synaptic targets of putative parvalbumin basket cells, and inputs to pyramidal cells as part of the VORTEX project. This is a reference table on the synapses table synapses_pni_2, and provided ground truth for the automatic classifier behind synapse_target_predictions_ssa.\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nreference id of the synapse in synapses_pni_2\n\n\ntag\nManual label of synapse type. One of: ‘spine’, ‘shaft’, ‘soma’, ‘soma_spine’, ‘orphan’, ‘other’. Orphan refers to orphan neurites; other is a catch-all for synapses that do not match other labels\n\n\npre_pt_position  pre_pt_supervoxel_id  pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse\n\n\npost_pt_position  post_pt_supervoxel_id  post_pt_root_id\nThe bound spatial point data for the postsynaptic side of the synapse\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on synapses_pni_2, and can be indexed by the same synapse id.\n\n\n# Standard query\nclient.materialize.query_table('vortex_compartment_targets', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_compartment_targets(pre_pt_root_id=example_root_id).query()\n\n\nVORTEX manual myelin\nTable name: vortex_manual_myelination_v0\nManual labeling of myelin at 1 um resolution along axons, as part of the VORTEX project. This table was created in parallel to the node of Ranvier labeling, vortex_manual_nodes_of_ranvier, and used for ground truth of the myelin_auto_tags myelination classifier.\n\nManual Myelination\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npt_position\nThe position along the axon that was evaluated for myelin. Note that this position is not always on the segmentation of the axon\n\n\nvalid_id\nThe segmentation root_id associated with this assessment is the valid_id, NOT necessarily the pt_root_id associated with the point\n\n\ntag\nMyelination status, either True or False\n\n\npt_supervoxel_id  pt_root_id\nThe bound spatial point data for the evaluated point. This may not match the valid_id due to proofreading, or spatial downsampling when evaluating the points.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe myelination was evaluated for the valid_id and not necessarily the pt_root_id.\n\n\n# Standard query\nclient.materialize.query_table('vortex_manual_myelination_v0', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_manual_myelination_v0(valid_id=example_root_id).query()\n\n\nVORTEX nodes of Ranvier\nTable name: vortex_manual_nodes_of_ranvier\nThis table annotates manually identified nodes of Ranvier on the axon of a subset of cells. Each point listed was inspected and manually labeled for the presence of myelin; nodes were suggested where myelination disappeared and reappeared within 5 um. Labeling performed as part of the VORTEX project, and created in parallel to the myelin labeling, listed below as vortex_manual_myelination_v0.\n\nManual Myelination\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npt_position\nThe position along the axon that was evaluated for myelin. Note that this position is not always on the segmentation of the axon\n\n\nvalid_id\nThe segmentation root_id associated with this assessment is the valid_id, NOT necessarily the pt_root_id associated with the point\n\n\ntag\npresence of a node of Ranvier, labeled as node\n\n\npt_supervoxel_id  pt_root_id\nThe bound spatial point data for the evaluated point. This may not match the valid_id due to proofreading, or spatial downsampling when evaluating the points.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe myelination was evaluated for the valid_id and not necessarily the pt_root_id.\n\n\n# Standard query\nclient.materialize.query_table('vortex_manual_nodes_of_ranvier')\n\n# Content-aware query\nclient.materialize.tables.vortex_manual_nodes_of_ranvier(valid_id=example_root_id).query()\nFor more on how to interpret the table, see Annotation Tables",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1078 (Jun 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-1078.html#functional-area-nucleus-look-up",
    "href": "release_manifests/version-1078.html#functional-area-nucleus-look-up",
    "title": "1078 (Jun 2024)",
    "section": "Functional area nucleus look-up",
    "text": "Functional area nucleus look-up\nTable name: nucleus_functional_area_assignment\nGiven the nucleus detection table nucleus_detection_v0 and the transformation of 2-photon in vivo imaging to the EM structural space (see functional-coregistration-tables ), each cell in the volume has been assigned to one of four visual cortical areas.\nThe inferred functional brain area based on the position of the nucleus in the EM volume. Area boundaries estimated from the area-membership assignments of the 2P recorded cells, after transformation to EM space.\nThe table nucleus_functional_area_assignment is a reference table on nucleus_detection_v0 and adds the following columns:\n\nFunctional area assignment\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\ntag\nthe brain area label (one of V1, AL, RL, LM)\n\n\nvalue\nthe distance to the area boundary (in um), calculated as the mean-distance of the 10 nearest neighbors in a non-matching brain area. A larger value is further from the area boundaries, and can be interpretted as higher confidence in area assignment.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('nucleus_functional_area_assignment')\n\n# Content-aware query\nclient.materialize.tables.nucleus_functional_area_assignment(tag='V1').query()\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1078 (Jun 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-1078.html#synapse-target-structure",
    "href": "release_manifests/version-1078.html#synapse-target-structure",
    "title": "1078 (Jun 2024)",
    "section": "Synapse target structure",
    "text": "Synapse target structure\nTable name: synapse_target_structure\nPostsynaptic structure classification for individual synapses by Brendan Celii and Jacob Reimer at Baylor College of Medicine based on mesh structure and initial synapse detection and classification.\nIt contains the following columns:\n\nSynapse target predictions column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npre_pt_position / pre_pt_supervoxel_id / pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse.\n\n\npost_pt_position / post_pt_supervoxel_id / post_pt_root_id\nThe bound spatial point data for the postsynaptic side of the synapse.\n\n\nsize\nThe size of the synapse in voxels. This correlates well, but not perfectly, with the surface area of synapse.\n\n\nctr_pt_position\nA position in the center of the detected synaptic junction. Of all points in the synapse table, this is usually the closest point to the surface (and thus mesh) of both neurons. Because it is at the edge of cells, it is not associated with a root id.\n\n\nvalue\nEach synapse has a value from 0-6 indicating spine, shaft, or other target compartments. 0 : Synapse onto spine head 1 : Synapse onto spine neck 2 : Synapse onto dendritic shaft 3 : Synapse onto spine but no clear head/neck separation for spine. 4 : Synapse onto axonal bouton. 5 : Synapse onto axonal structure other than a bouton. 6 : Synapse onto soma\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on synapses_pni_2, and can be indexed by the same synapse id.\n\n\n# Standard query\nclient.materialize.query_table('synapse_target_structure', limit=10)\n\n# Content-aware query\nclient.materialize.tables.synapse_target_structure(post_pt_root_id=example_root_id).query()\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1078 (Jun 2024)"
    ]
  },
  {
    "objectID": "quickstart_notebooks/08-standard-transform.html",
    "href": "quickstart_notebooks/08-standard-transform.html",
    "title": "Standard Transform: Coordinate System",
    "section": "",
    "text": "Because of the many different data representations, dealing with coordinates in the MICrONs data is not entirely simple and it is easy to make mistakes by converting between coordinate systems incorrectly.\nThere are three main coordinate systems that wind up getting used:\n\nVoxel coordinates are the coordinates of a point in the original image volume. These are the coordinates that are used to index into the volumes you can see in Neuroglancer, but each number has a potentially different unit. In the MICrONs data, a voxel is 4 nm wide in the x and y directions, and 40 nm long in the z direction. This means that a 1x1x1 micron cube would be represented by a 250x250x25 voxel span. Annotations (such as synapses) are stored in voxel coordinates.\nNanometer coordinates are the coordinates of a point in the original image volume, but in nanometers. This is equivalent to the voxel coordinate multiplied by the voxel resolution, with no further transformation applied. Mesh and skeleton vertices are stored in nanometer coordinates.\nTransformed coordinates reflect a trasnsformation that has been applied to the original image volume. This transformation is a rotation to make the pia surface as flat as possible, a translation to move the pial surface to y=0, and a scaling to bring coordinates into microns. Transformed coordinates are convenient for more accurate computations of depth and the pia-to-white-matter axis, but are not stored by default. A python package standard_transform helps convert data to and from transformed coordinates.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that in all of these coordinate systems (including Neuroglancer), the y axis increases with depth. This is a standard definition when working with images, but is the opposite of what you usually think of with points. Because of that, when plotting annotations or neuroanatomy in matplotlib, you will usually have to invert the y axis with ax.invert_yaxis().",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Standard Transform: Coordinate System"
    ]
  },
  {
    "objectID": "quickstart_notebooks/08-standard-transform.html#introduction",
    "href": "quickstart_notebooks/08-standard-transform.html#introduction",
    "title": "Standard Transform: Coordinate System",
    "section": "",
    "text": "Because of the many different data representations, dealing with coordinates in the MICrONs data is not entirely simple and it is easy to make mistakes by converting between coordinate systems incorrectly.\nThere are three main coordinate systems that wind up getting used:\n\nVoxel coordinates are the coordinates of a point in the original image volume. These are the coordinates that are used to index into the volumes you can see in Neuroglancer, but each number has a potentially different unit. In the MICrONs data, a voxel is 4 nm wide in the x and y directions, and 40 nm long in the z direction. This means that a 1x1x1 micron cube would be represented by a 250x250x25 voxel span. Annotations (such as synapses) are stored in voxel coordinates.\nNanometer coordinates are the coordinates of a point in the original image volume, but in nanometers. This is equivalent to the voxel coordinate multiplied by the voxel resolution, with no further transformation applied. Mesh and skeleton vertices are stored in nanometer coordinates.\nTransformed coordinates reflect a trasnsformation that has been applied to the original image volume. This transformation is a rotation to make the pia surface as flat as possible, a translation to move the pial surface to y=0, and a scaling to bring coordinates into microns. Transformed coordinates are convenient for more accurate computations of depth and the pia-to-white-matter axis, but are not stored by default. A python package standard_transform helps convert data to and from transformed coordinates.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that in all of these coordinate systems (including Neuroglancer), the y axis increases with depth. This is a standard definition when working with images, but is the opposite of what you usually think of with points. Because of that, when plotting annotations or neuroanatomy in matplotlib, you will usually have to invert the y axis with ax.invert_yaxis().",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Standard Transform: Coordinate System"
    ]
  },
  {
    "objectID": "quickstart_notebooks/08-standard-transform.html#standard-transform",
    "href": "quickstart_notebooks/08-standard-transform.html#standard-transform",
    "title": "Standard Transform: Coordinate System",
    "section": "Standard Transform",
    "text": "Standard Transform\nStandard Transform is a python package designed to convert voxel and nanometer coordinates to transformed coordinates, with particular emphasis on the MICrONs data.\n\nInstallation\nStandard Transform can be installed from pip: pip install standard_transform\n\n\nWhy use Standard Transform?\nLet’s look at the coordinates of every excitatory neuron in the MICrONs data to see why we might want to use transformed coordinates.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom caveclient import CAVEclient\n\nclient = CAVEclient('minnie65_public')\n\nct_df = client.materialize.query_table('aibs_metamodel_celltypes_v661', split_positions=True)\n\n# convert to nanometers\nct_df['pt_position_x_nm'] = ct_df['pt_position_x'] * 4\nct_df['pt_position_y_nm'] = ct_df['pt_position_y'] * 4\nct_df['pt_position_z_nm'] = ct_df['pt_position_z'] * 40\n\nfig, ax = plt.subplots(figsize=(5,4))\nsns.scatterplot(\n    x='pt_position_x_nm',\n    y='pt_position_y_nm',\n    s=3,\n    data=ct_df.sample(10_000), # Pick a random sample of 10,000 points, enough to see the shape of the data.\n    ax=ax,\n)\n\n# Important to flip the y axis to have y increase with depth!\nax.invert_yaxis()\nsns.despine(ax=ax)\n\n\n\n\n\n\n\n\nThis is just the raw positions, converted to nanometers. You can see a few aspects that might make this hard to work with.\nFirst, if you look along the top of the data, you can see that the pia surface is not flat. In fact, there’s a roughly 5 degree slope from the left to the right. Second, if you look at the location of the pia surface, it’s at around y = 0.4 * 10^6.\nNot only are these units large, the offset is arbitrary and it would make much more sense to anchor y=0 to the pial surface.\nLet’s see how we can do this with standard_transform.\n\nfrom standard_transform import minnie_ds\nimport numpy as np\n\nX_transformed = minnie_ds.transform_vx.apply_dataframe('pt_position', ct_df)\nX_transformed = np.array(X_transformed)\nct_df['pt_xt'] = X_transformed[:,0]\nct_df['pt_yt'] = X_transformed[:,1]\nct_df['pt_zt'] = X_transformed[:,2] \n\nfig, ax = plt.subplots(figsize=(5,4))\n\nsns.scatterplot(\n    x='pt_xt',\n    y='pt_yt',\n    s=3,\n    data=ct_df.sample(10_000),\n    ax=ax,\n)\n\nax.invert_yaxis()\nsns.despine(ax=ax)\n\n\n\n\n\n\n\n\nNow you can see that the surface is much more aligned with the x-axis, and the cells in layer 1 start just below y=0. In addition, the units have been converted to microns, which is much more readable and more consistent with measurements in other modalities.\n\n\nHow to use Standard Transform.\nA number of examples are available in the standard_transform readme.\n\nTransforming points\nThe main functions when working with point data are transform_vx and transform_nm, which transform from voxel coordinates and from nanometer coordinates respectively. In the above example, you will note that because we were working with annotations, we used the voxel coordinate transform.\nEach of the two transforms has the same functions that can be used:\n\nminnie_ds.{transform}.apply(x): This converts an Nx3 array of points from the original space to an Nx3 array of transformed coordinates.\nminnie_ds.{transform}.apply_project(projection_axis, x): This converts an Nx3 array of points from the original space to an N-length array of transformed coordinates, taking only values along the given axis (x, y, or z). This is typically done along the y axis to get the depth of each point.\nminnie_ds.{transform}.apply_dataframe(column_name, df, Optional[projection_axis]): This takes points from a dataframe column (or collection of three dataframe columns named {column_name}_x, {column_name}_y, and {column_name}_z) and converts them to transformed coordinates. If projection_axis is given, it will only return the values along that axis.\n\nWhere {transform} is either transform_vx or transform_nm.\nYou can also invert a transformation, for example if you want to convert transformed coordinates back to voxel coordinates to view in Neuroglancer.\n\nminnie_ds.{transform}.invert(X): This maps from an Nx3 array in the transformed space back to the original space (voxel or nanometer coordiantes, depending on which transform you used).\n\n\n\nTransforming meshes and skeletons\nStandard transform has the capability to transform MeshParty skeletons, MeshWorks, and MeshWork annotations using these same transforms as above.\nFor example, to transform all the vertices of meshwork object:\n\nnrn_transformed = minnie_ds.transform_nm.apply_meshwork_vertices(nrn)\n\nHowever, Meshwork objects also have annotations in dataframes as well as vertices. In order to transform these points, we need to specify both the name of the dataframe table and the columns to transform as a dictionary. For example, if you want to also remap the ctr_pt_position values from the pre_syn table and post_syn tables (which are in voxels by default), you would use:\n\nanno_dict={'pre_syn': 'ctr_pt_position', 'post_syn': 'ctr_pt_position'}\nnrn_transformed = minnie_ds.transform_vx.apply_meshwork_annotations(nrn_transformed, anno_dict, inplace=True)\n\nNote that without the inplace=True parameter, these meshwork transformations will return a new object, while ifinplace=True the original object is modified.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Standard Transform: Coordinate System"
    ]
  },
  {
    "objectID": "quickstart_notebooks/06-cloudvolume-download-mesh.html",
    "href": "quickstart_notebooks/06-cloudvolume-download-mesh.html",
    "title": "Download Meshes",
    "section": "",
    "text": "Version update\n\n\n\nWe have released a new public version 1412, as part of our quarterly release schedule. See details at Release Manifests: 1412.\nHowever, 1412 does not have a flat segmentation. This tutorial remains pinned to v1300 as the latest major version.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Meshes"
    ]
  },
  {
    "objectID": "quickstart_notebooks/06-cloudvolume-download-mesh.html#connected-morphological-representations-meshes",
    "href": "quickstart_notebooks/06-cloudvolume-download-mesh.html#connected-morphological-representations-meshes",
    "title": "Download Meshes",
    "section": "Connected morphological representations: meshes",
    "text": "Connected morphological representations: meshes\nWhen trying to understand the fine 3d morphology of a neuron (e.g. features under 1 micron in scale), meshes are a particularly useful representation.More precisely, a mesh is a collection of vertices and faces that define a 3d surface.\nThe meshes that one sees in Neuroglancer are available to download through the python client cloud-volume, and can be loaded for analysis and visualization in other tools.\nThe MICrONS data can be representing and rendered in multiple formats, at different levels of abstraction from the original imagery.\n\n\n\n\n\n\n\n\n\nSegmentation: more expensive\n\n\n\n\n\n\n\nMesh\n\n\n\n\n\n\n\nSkeleton: less expensive",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Meshes"
    ]
  },
  {
    "objectID": "quickstart_notebooks/06-cloudvolume-download-mesh.html#what-is-a-mesh",
    "href": "quickstart_notebooks/06-cloudvolume-download-mesh.html#what-is-a-mesh",
    "title": "Download Meshes",
    "section": "What is a mesh?",
    "text": "What is a mesh?\nA mesh is a set of vertices connected via triangular faces to form a 3 dimensional representation of the outer membrane of a neuron, glia or nucleus.\n\nMeshes can either be static or dynamic:\n\nStatic:\n\npros: smaller files thus easier to work with, multiple levels of detail (lod) which can be accessed (example below)\ncons: may include false gaps and merges from self contacts, updated less frequently\n\nExample path: precomputed://gs://iarpa_microns/minnie/minnie65/seg_m1300\n\n\nDynamic:\n\npros: highly detailed thus more reflective of biological reality and backed by proofreading infrastructure CAVE (Connectome Annotation Versioning Engine)\ncons: much larger files, only one level of detail\n\nExample path: graphene://https://minnie.microns-daf.com/segmentation/table/minnie65_public",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Meshes"
    ]
  },
  {
    "objectID": "quickstart_notebooks/06-cloudvolume-download-mesh.html#cloud-volume-for-downloading-meshes",
    "href": "quickstart_notebooks/06-cloudvolume-download-mesh.html#cloud-volume-for-downloading-meshes",
    "title": "Download Meshes",
    "section": "Cloud-Volume for downloading meshes",
    "text": "Cloud-Volume for downloading meshes\nCloudVolume is a serverless Python client for random access reading and writing of Neuroglancer volumes in “Precomputed” format, a set of representations for arbitrarily large volumetric images, meshes, and skeletons.\nPrecomputed volumes are typically stored on AWS S3, Google Storage, or locally. CloudVolume can read and write to these object storage providers given a service account token with appropriate permissions. However, these volumes can be stored on any service, including an ordinary webserver or local filesystem, that supports key-value access.\ncloud-volume is the mechanism for many programmatic data queries, and integrates heavily with the CAVE ecosystem but is distinct. You can install cloud-volume with:\npip install cloud-volume\nThe Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data.\n\n\n\n\n\n\nInitial Setup\n\n\n\nBefore using any programmatic access to the data, you first need to set up your CAVEclient token. When CAVEclient saves the token to your local machine, cloudvolume will access the same token.\n\n\n\nfrom caveclient import CAVEclient\nfrom cloudvolume import CloudVolume\n\ndatastack_name = 'minnie65_public'\nclient = CAVEclient(datastack_name)\n\nThe datastack includes information about the segmentation source, and can provide that information when prompted\n\nclient.info.segmentation_source()\n\n'graphene://https://minnie.microns-daf.com/segmentation/table/minnie65_public'\n\n\nNote that this is the same as the dynamic segmentation path above\n\n# this can be used to initialize a cloudvolume object\ncv = CloudVolume(client.info.segmentation_source(), progress=False, use_https=True)\n\n\nExamples from the cloudvolume README:\ncv.mesh.get(12345) # return the mesh as vertices and faces instead of writing to disk\ncv.mesh.get([ 12345, 12346 ]) # return these two segids fused into a single mesh\ncv.mesh.get([ 12345, 12346 ], fuse=False) # return { 12345: mesh, 12346: mesh }\n\n\nDownload dynamic mesh\nGiven a root_id, CloudVolume can be used to retrieve the mesh. cloudvolume.mesh.get() returns a dictionary with the neuron segment id as the key and the mesh as the value\n\n# specify the materialization version, for consistency across time\",\nclient.version = 1300\n\n# Example: pyramidal cell in v1300\nexample_cell_id = 864691135572530981\n%time mesh = cv.mesh.get(example_cell_id)[example_cell_id]\n\nWarning: deduplication not currently supported for this layer's variable layered draco meshes\nCPU times: total: 42 s\nWall time: 1min 5s\n\n\n\n\nDownload static mesh\nThe meshes that are available in the visualization on microns-explorer.org are fast to load because they are static and have been downsampled at multiple resolutions. However, this comes with the drawback of being less biologically accurate.\nLevel of detail is controlled with optional argument, where lod=0 is the highest level of detail\nwe can access one of these downsampled static meshes by setting the path here:\n\ncv = CloudVolume(\"precomputed://gs://iarpa_microns/minnie/minnie65/seg_m1300\", use_https=True)\n\n\n# the cloud volume interface is the same but it is a faster initial download \n%time mesh = cv.mesh.get(example_cell_id, lod=3)[example_cell_id]\n\nCPU times: total: 78.1 ms\nWall time: 1.19 s\n\n\n\n# as you can see the meshes aren't exactly the same as before. They because they have not been downsampled\nmesh.vertices.shape, mesh.faces.shape\n\n((56938, 3), (99936, 3))\n\n\nIn addition, the Static meshes are available in 3 levels of detail, this covers two orders of magnitude of detail which is what neuroglancer leverages to efficiently load the data at the resolution necessary to render the current scene.\n\nfor lod in range(4):\n    mesh = mesh = cv.mesh.get(example_cell_id, lod=lod)[example_cell_id]\n    print(f\"level of detail {lod}: n_verts: {mesh.vertices.shape[0]} n_faces: {mesh.faces.shape[0]}\")\n\nlevel of detail 0: n_verts: 5231141 n_faces: 10291430\n\n\nlevel of detail 1: n_verts: 1457607 n_faces: 2828410\n\n\nlevel of detail 2: n_verts: 253583 n_faces: 474769\n\n\nlevel of detail 3: n_verts: 56938 n_faces: 99936\n\n\nThe returned mesh includes: * vertices : Nx3 [x, y, z] positions in nm * faces: Kx3 [i, j, k] indices into vertices that describe the triangular meshes",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Meshes"
    ]
  },
  {
    "objectID": "quickstart_notebooks/06-cloudvolume-download-mesh.html#meshparty-for-processing-mesh-objects",
    "href": "quickstart_notebooks/06-cloudvolume-download-mesh.html#meshparty-for-processing-mesh-objects",
    "title": "Download Meshes",
    "section": "MeshParty for processing mesh objects",
    "text": "MeshParty for processing mesh objects\nThe Meshparty, which is a python package to work with meshes, designed around use cases for analyzing neuronal morphology.\npip install meshparty\nThe MeshParty object has more useful properties and attributes such as: * a scipy.csgraph sparse graph object (mesh.csgraph) * a networkx graph object (mesh.nxgraph)\nRead more about what you can do with MeshParty on its Documentation.\nIn particular it lets you associate skeletons, and annotations onto the mesh into a “meshwork” object.\n\nimport os\nfrom meshparty import trimesh_io\nfrom caveclient import CAVEclient\nclient = CAVEclient('minnie65_public')\n\nmm = trimesh_io.MeshMeta(\n  cv_path=client.info.segmentation_source(),\n  disk_cache_path=\"meshes\",\n)\n\nexample_cell_id = 864691135572530981\nmesh = mm.mesh(seg_id=example_cell_id)\n\nWarning: deduplication not currently supported for this layer's variable layered draco meshes\n\n\nOne convenience of using the MeshMeta approach is that if you have already downloaded a mesh for with a given root id, it will be loaded from disk rather than re-downloaded.\nIf you have to download many meshes, it is somewhat faster to use the bulk download_meshes function and use multiple threads via the n_threads argument. If you download them to the same folder used for the MeshMeta object, they can be loaded through the same interface.\nroot_ids = [864691135572530981, 864691135014128278, 864691134940133219]\nmm = trimesh_io.download_meshes(\n    seg_ids=root_ids,\n    target_dir='meshes',\n    cv_path=client.info.segmentation_source(),\n    n_threads=4, # Or whatever value you choose above one but less than the number of cores on your computer\n)\n\n\n\n\n\n\nFile size\n\n\n\nMeshes can be hundresds of megabytes in size, so be careful about downloading too many if the internet is not acting well or your computer doesn’t have much disk space!\n\n\n\nHealing Mesh Gaps\n\n\n\nExample of a continuous neuron whose mesh has a gap\n\n\nMany meshes are not actually fully continuous due to small gaps in the segmentation. However, information collected during proofreading allows one to partially repair these gaps by adding in links where the segmentation was merged across a small gap. If you are just visualizaing a mesh, these gaps are not a problem, but if you want to do analysis on the mesh, you will want to heal these gaps. Conveniently, there’s a function to do this:\nmesh.add_link_edges(\n    seg_id=example_cell_id, # This needs to be the same as the root id used to download the mesh\n    client=client.chunkedgraph,\n)\n\n\nProperties\nMeshes have a large number of properties, many of which come from being based on the Trimesh library’s mesh format, and others being specific to MeshParty.\nSeveral of the most important properties are:\n\nmesh.vertices : An N x 3 list of vertices and their 3d location in nanometers, where N is the number of vertices.\nmesh.faces : An P x 3 list of integers, with each row specifying a triangle of connected vertex indices.\nmesh.edges : An M x 2 list of integers, with each row specifying a pair of connected vertex indices based off of faces.\nmesh.edges : An M x 2 list of integers, with each row specifying a pair of connected vertex indices based off of faces.\nmesh.link_edges : An M_l x 2 list of integers, with each row specifying a pair of “link edges” that were used to heal gaps based on proofreading edits.\nmesh.graph_edges : An (M+M_l) x 2 list of integers, with each row specifying a pair of graph edges, which is the collection of both mesh.edges and mesh.link_edges.\nmesh.csgraph : A Scipy Compressed Sparse Graph representation of the mesh as an NxN graph of vertices connected to one another using graph edges and with edge weights being the distance between vertices. This is particularly useful for computing shortest paths between vertices.\n\n\n\n\n\n\n\nWarning\n\n\n\nMICrONs meshes are not generally “watertight”, a property that would enable a number of properties to be computed natively by Trimesh. Because of this, Trimesh-computed properties relating to solid forms or volumes like mesh.volume or mesh.center_mass do not have sensible values and other approaches should be taken. Unfortunately, because of the Trimesh implementation of these properties it is up to the user to be aware of this issue.\n\n\n\n\nVisualization\nThere are a variety of tools for visualizing meshes in python. MeshParty interfaces with VTK, a powerful but complex data visualization library that does not always work well in python. The basic pattern for MeshParty’s VTK integration is to create one or more “actors” from the data, and then pass those to a renderer that can be displayed in an interactive approach. The following code snippet shows how to visualize a mesh using this approach.\nmesh_actor = trimesh_vtk.mesh_actor(\n  mesh,\n  color=(1,0,0),\n  opacity=0.5,\n)\ntrimesh_vtk.render_actors([mesh_actor])\nNote that by default, neurons will appear upside down because the coordinate system of the dataset has the y-axis value increasing along the “downward” pia to white matter axis.\nMore documentation on the MeshParty VTK visualization can be found here.\nOther tools worth exploring are:\n\nPyVista\nPolyscope\nVedo\nMeshLab\nIf you have some existing experience, Blender (see Blender integration by our friends behind Navis, a fly-focused framework analyzing connectomics data).\n\n\n\nMasking\nOne of the most common operations on meshes is to mask them to a particular region of interest. This can be done by “masking” the mesh with a boolean array of length N where N is the number of vertices in the mesh, with True where the vertex should be kept and False where it should be omitted. There are several convenience functions to generate common masks in the Mesh Filters module.\nIn the following example, we will first mask out all vertices that aren’t part of the largest connected component of the mesh (i.e. get rid of floating vertices that might arise due to internal surfaces) and then mask out all vertices that are more than 20,000 nm away from the soma center.\nfrom meshparty import mesh_filters\n\nroot_id =864691134940133219 \nroot_point = client.materialize.tables.nucleus_detection_v0(pt_root_id=root_id).query()['pt_position'].values[0] * [4,4,40]  # Convert the nucleus location from voxels to nanometers via the data resolution.\n\nmesh = mm.mesh(seg_id=root_id)\n# Heal gaps in the mesh\nmesh.add_link_edges(\n    seg_id=864691134940133219,\n    client=client.chunkedgraph,\n)\n\n# Generate and use the largest component mask\ncomp_mask = mesh_filters.filter_largest_component(mesh)\nmask_filt = mesh.apply_mask(comp_mask)\n\nsoma_mask = mesh_filters.filter_spatial_distance_from_points(\n    mask_filt,\n    root_point,\n    20_000, # Note that this is in nanometers\n)\nmesh_soma = mesh.apply_mask(soma_mask)\nThis resulting mesh is just a small cutout around the soma.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Meshes"
    ]
  },
  {
    "objectID": "quickstart_notebooks/04-cave-query-synapses.html",
    "href": "quickstart_notebooks/04-cave-query-synapses.html",
    "title": "CAVE Query: Synaptic Connectivity",
    "section": "",
    "text": "Version update\n\n\n\nThis tutorial has recently been updated to materialization version 1412 (from 1300).\nWe have released a new public version 1412, as part of our quarterly release schedule. See details at Release Manifests: 1412.\nThe Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data.\nThe connectome data (synapses, cell types, etc.) can be accessed from the cloud via CAVE. However, because of the size of the connectivity tables, it is often preferable to download and compile the features of interest (in this case synapses) to work with offline. This notebook steps through downloading the synapses of the proofread neurons, as of materialization version 1300.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Synaptic Connectivity"
    ]
  },
  {
    "objectID": "quickstart_notebooks/04-cave-query-synapses.html#initialize-caveclient-with-a-datastack",
    "href": "quickstart_notebooks/04-cave-query-synapses.html#initialize-caveclient-with-a-datastack",
    "title": "CAVE Query: Synaptic Connectivity",
    "section": "Initialize CAVEclient with a datastack",
    "text": "Initialize CAVEclient with a datastack\nDatasets in CAVE are organized as datastacks. These are a combination of an EM dataset, a segmentation and a set of annotations. The datastack for MICrONS public release is minnie65_public. When you instantiate your client with this datastack, it loads all relevant information to access it.\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom caveclient import CAVEclient\nclient = CAVEclient(\"minnie65_public\")\n\n\nMaterialization versions\nData in CAVE is timestamped and periodically versioned - each (materialization) version corresponds to a specific timestamp. Individual versions are made publicly available. The Materialization client allows one to interact with the materialized annotation tables that were posted to the annotation service. These are called queries to the dataset, and available from client.materialize. For more, see the CAVEclient Documentation.\nPeriodic updates are made to the public datastack, which will include updates to the available tables. Some cells will have different pt_root_id because they have undergone proofreading.\n\n\n\n\n\n\nTip\n\n\n\nFor analysis consistency, is worth checking the version of the data you are using, and consider specifying the version with client.version = your_version\nRead more about setting the version of your analysis\n\n\n\nclient.materialize.get_versions()\n\n[1300, 1078, 117, 661, 343, 1181, 795, 943, 1412]\n\n\nAnd these are their associated timestamps (all timestamps are in UTC):\n\nfor version in client.materialize.get_versions():\n    print(f\"Version {version}: {client.materialize.get_timestamp(version)}\")\n\nVersion 1300: 2025-01-13 10:10:01.286229+00:00\nVersion 1078: 2024-06-05 10:10:01.203215+00:00\nVersion 117: 2021-06-11 08:10:00.215114+00:00\nVersion 661: 2023-04-06 20:17:09.199182+00:00\nVersion 343: 2022-02-24 08:10:00.184668+00:00\nVersion 1181: 2024-09-16 10:10:01.121167+00:00\nVersion 795: 2023-08-23 08:10:01.404268+00:00\nVersion 943: 2024-01-22 08:10:01.497934+00:00\nVersion 1412: 2025-04-29 10:10:01.200893+00:00\n\n\nThe client will automatically query the latest materialization version. You can specify a materialization_version for every query if you want to access a specific version.\n\n# set materialization version, for consistency\nclient.version = 1412 # current public as of 4/29/2025",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Synaptic Connectivity"
    ]
  },
  {
    "objectID": "quickstart_notebooks/04-cave-query-synapses.html#querying-synapses",
    "href": "quickstart_notebooks/04-cave-query-synapses.html#querying-synapses",
    "title": "CAVE Query: Synaptic Connectivity",
    "section": "Querying Synapses",
    "text": "Querying Synapses\nWhile synapses are stored as any other table in the database, in this case synapses_pni_2, this table is much larger than any other table at more than 337 million rows:\n\nclient.materialize.get_annotation_count('synapses_pni_2')\n\n337312429\n\n\nWhile we can query the synapse table directly, this is generally not recommended. It is too large to query all at once. CAVE limits to queries to 500,000 rows at once and will display a warning when that happens. Here, we demonstrate this with the limit set to 10:\n\nsynapse_table_name = client.info.get_datastack_info()[\"synapse_table\"]\nsyn_df = client.materialize.query_table(synapse_table_name, limit=10, desired_resolution=[1, 1, 1], split_positions=True)\nsyn_df\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\npre_pt_position_x\npre_pt_position_y\npre_pt_position_z\npost_pt_position_x\npost_pt_position_y\npost_pt_position_z\nctr_pt_position_x\nctr_pt_position_y\nctr_pt_position_z\nsize\npre_pt_supervoxel_id\npre_pt_root_id\npost_pt_supervoxel_id\npost_pt_root_id\n\n\n\n\n0\n4456\n2020-11-04 13:02:08.388988+00:00\nNaN\nt\n211448.0\n409744.0\n801440.0\n211448.0\n409744.0\n801440.0\n211612.0\n410172.0\n801400.0\n2956\n72063160986635724\n864691135533713769\n72063160986635724\n864691135533713769\n\n\n1\n4503\n2020-11-04 12:09:33.286834+00:00\nNaN\nt\n212456.0\n408032.0\n800360.0\n212456.0\n408032.0\n800360.0\n212168.0\n408088.0\n800400.0\n344\n72063092267156962\n864691135087527094\n72063092267156962\n864691135087527094\n\n\n2\n4508\n2020-11-04 13:02:13.024144+00:00\nNaN\nt\n212448.0\n411696.0\n801440.0\n212448.0\n411696.0\n801440.0\n212224.0\n411800.0\n801560.0\n344\n72063229706111827\n864691135533713769\n72063229706111827\n864691135533713769\n\n\n3\n4568\n2020-11-04 13:44:08.085705+00:00\nNaN\nt\n213392.0\n415448.0\n802920.0\n213392.0\n415448.0\n802920.0\n213096.0\n415176.0\n802880.0\n13816\n72133735889250131\n864691134530418554\n72133735889250131\n864691134530418554\n\n\n4\n4581\n2020-11-04 07:29:12.917622+00:00\nNaN\nt\n213552.0\n417184.0\n800800.0\n213552.0\n417184.0\n800800.0\n213240.0\n417080.0\n801080.0\n10436\n72133804608718799\n864691134745062676\n72133804608718799\n864691134745062676\n\n\n5\n4582\n2020-11-04 13:02:17.694701+00:00\nNaN\nt\n212880.0\n409120.0\n801440.0\n212880.0\n409120.0\n801440.0\n213016.0\n408832.0\n801520.0\n1344\n72063160986636743\n864691135533713769\n72063160986636743\n864691135533713769\n\n\n6\n4588\n2020-11-04 12:20:12.290593+00:00\nNaN\nt\n213200.0\n421120.0\n805520.0\n213200.0\n421120.0\n805520.0\n213064.0\n421000.0\n805600.0\n7128\n72133942047682150\n864691134609767690\n72133942047682150\n864691134609767690\n\n\n7\n4590\n2020-11-04 13:20:01.875310+00:00\nNaN\nt\n213504.0\n406440.0\n805160.0\n213504.0\n406440.0\n805160.0\n213336.0\n406596.0\n805200.0\n6572\n72133461011344162\n864691135091400630\n72133461011344162\n864691135091400630\n\n\n8\n4606\n2020-11-04 07:24:39.038223+00:00\nNaN\nt\n213384.0\n413792.0\n800800.0\n213384.0\n413792.0\n800800.0\n213256.0\n413976.0\n801040.0\n2100\n72133667169766499\n864691134609872906\n72133667169766499\n864691134609872906\n\n\n9\n4611\n2020-11-04 07:24:37.800341+00:00\nNaN\nt\n213336.0\n415304.0\n800960.0\n213336.0\n415304.0\n800960.0\n213192.0\n415604.0\n800960.0\n492\n72133735889243887\n864691134609872906\n72133735889243887\n864691134609872906\n\n\n\n\n\n\n\nInstead, you have several options for querying cells of interest:\n\nSpecifying the pre_ids\nSpecifying the post_ids\nSearching within a bounding_box\nUsing a combination of the above to iterate through large numbers of neurons\n\n\nQuery synapses given pt_root_id of interest\nThe synapse_query function allows you to query the synapse table in a more convenient way than most other tables. In particular, the pre_ids and post_ids let you specify which root id (or collection of root ids) you want to query, with pre_ids indicating the collection of presynaptic neurons and post_ids the collection of postsynaptic neurons.\nUsing both pre_ids and post_ids in one call is effectively a logical AND, returning only those synapses from neurons in the list of pre_ids that target neurons in the list of post_ids.\nLet’s look at one particular example.\n\n# Pick example cell\nexample_root_id = 864691136968109774\n\n# Query synapse table with synapse_query()\ninput_syn_df = client.materialize.synapse_query(post_ids=example_root_id)\n\nprint(f\"Total number of input synapses for {example_root_id}: {len(input_syn_df)}\")\ninput_syn_df.head()\n\nTotal number of input synapses for 864691136968109774: 4628\n\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\nsize\npre_pt_supervoxel_id\npre_pt_root_id\npost_pt_supervoxel_id\npost_pt_root_id\npre_pt_position\npost_pt_position\nctr_pt_position\n\n\n\n\n0\n165943687\n2020-11-04 06:48:59.403833+00:00\nNaN\nt\n6068\n89667372671545980\n864691135522708676\n89667372671554898\n864691136968109774\n[181250, 191912, 18361]\n[181278, 191844, 18364]\n[181276, 191873, 18366]\n\n\n1\n155169887\n2020-11-04 06:49:11.336675+00:00\nNaN\nt\n964\n88540717118418517\n864691135412525042\n88540717051854108\n864691136968109774\n[172816, 186434, 19971]\n[172892, 186468, 19969]\n[172821, 186483, 19965]\n\n\n2\n175655972\n2020-11-04 06:48:59.403833+00:00\nNaN\nt\n260\n90230391344454185\n864691136711146862\n90160022600247406\n864691136968109774\n[184962, 192380, 18359]\n[184836, 192448, 18366]\n[184916, 192396, 18359]\n\n\n3\n169597812\n2020-11-04 06:49:11.962249+00:00\nNaN\nt\n7164\n89598584945119102\n864691135459877874\n89598584945109756\n864691136968109774\n[180636, 203654, 21957]\n[180706, 203762, 21963]\n[180680, 203716, 21959]\n\n\n4\n159676064\n2020-11-04 06:48:59.876707+00:00\nNaN\nt\n14460\n89385691939052008\n864691135773033723\n89385691939060896\n864691136968109774\n[178870, 190176, 21429]\n[178870, 190204, 21437]\n[178878, 190132, 21434]\n\n\n\n\n\n\n\n\n# Query synapse table with synapse_query()\noutput_syn_df = client.materialize.synapse_query(pre_ids=example_root_id)\n\nprint(f\"Total number of output synapses for {example_root_id}: {len(output_syn_df)}\")\noutput_syn_df.head()\n\nTotal number of output synapses for 864691136968109774: 1499\n\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\nsize\npre_pt_supervoxel_id\npre_pt_root_id\npost_pt_supervoxel_id\npost_pt_root_id\npre_pt_position\npost_pt_position\nctr_pt_position\n\n\n\n\n0\n158405512\n2020-11-04 06:48:59.403833+00:00\nNaN\nt\n420\n89385416926790697\n864691136968109774\n89385416926797494\n864691135546540484\n[179076, 188248, 20233]\n[179156, 188220, 20239]\n[179140, 188230, 20239]\n\n\n1\n185549462\n2020-11-04 06:49:10.903020+00:00\nNaN\nt\n4832\n91356016507479890\n864691136968109774\n91356016507470163\n864691135884799088\n[193168, 190452, 19262]\n[193142, 190404, 19257]\n[193180, 190432, 19254]\n\n\n2\n138110803\n2020-11-04 06:49:46.758528+00:00\nNaN\nt\n3176\n87263084540201919\n864691136968109774\n87263084540199587\n864691135060075931\n[163440, 104292, 19808]\n[163498, 104348, 19806]\n[163460, 104356, 19804]\n\n\n3\n157378264\n2020-11-04 07:38:27.332669+00:00\nNaN\nt\n412\n89374490395905686\n864691136968109774\n89374490395921430\n864691135446953106\n[179218, 107132, 19372]\n[179204, 107010, 19383]\n[179196, 107072, 19380]\n\n\n4\n148262628\n2020-11-04 06:53:27.294021+00:00\nNaN\nt\n3536\n88189766885093187\n864691136968109774\n88189835604584343\n864691135250533976\n[170154, 193170, 21123]\n[170046, 193240, 21123]\n[170118, 193220, 21128]\n\n\n\n\n\n\n\nNote that synapse queries always return the list of every synapse between the neurons in the query, even if there are multiple synapses between the same pair of neurons.\nA common pattern to generate a list of connections between unique pairs of neurons is to group by the root ids of the presynaptic and postsynaptic neurons and then count the number of synapses between them. For example, use pandas.groupby() to get the number of synapses from this neuron onto every other neuron:\n\n# get count of synapses between presynaptic and postsynaptic partners\noutput_syn_df.groupby(\n  ['pre_pt_root_id', 'post_pt_root_id']\n).count()[['id']].rename(\n  columns={'id': 'syn_count'}\n).sort_values(\n  by='syn_count',\n  ascending=False,\n)\n# Note that the 'id' part here is just a way to quickly extract one column. This could be any of the remaining column names, \n# but `id` is often convenient because it is common to all tables.\n\n\n\n\n\n\n\n\n\nsyn_count\n\n\npre_pt_root_id\npost_pt_root_id\n\n\n\n\n\n864691136968109774\n864691135280056225\n20\n\n\n864691135456207722\n16\n\n\n864691134949547516\n15\n\n\n864691135784316467\n13\n\n\n864691136275521549\n11\n\n\n...\n...\n\n\n864691136911628017\n1\n\n\n864691135234846808\n1\n\n\n864691135236362017\n1\n\n\n864691136913679601\n1\n\n\n864691136965772238\n1\n\n\n\n\n1035 rows × 1 columns\n\n\n\n\n\nQuery synapses given bounding_box of interest\nThe synapse_query() can find all synapses in an arbitrary space of the volume. This is useful if, for example, you want to find all synapses in a radius around one synapse of interest, such as for building a null-model of connectivity based on proximity.\nbounding_box = [[min_x, min_y, min_z], [max_x, max_y, max_z]]\ndf=client.materialize.query_table(post_ids = example_root_id,\n                                  bounding_box=bounding_box)\nFor more details, see the CAVEclient documentation. Also compare to bounding box downloads through cloud-volume in the next quickstart notebooks.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Synaptic Connectivity"
    ]
  },
  {
    "objectID": "quickstart_notebooks/04-cave-query-synapses.html#query-proofread-cells-and-connectivity",
    "href": "quickstart_notebooks/04-cave-query-synapses.html#query-proofread-cells-and-connectivity",
    "title": "CAVE Query: Synaptic Connectivity",
    "section": "Query proofread cells and connectivity",
    "text": "Query proofread cells and connectivity\n\nProofread neurons\nThe table proofreading_status_and_strategy contains proofreading information about ~2000 neurons. For more on interpretting and using the proofreading table, see the previous quickstart notebook.\nHere we query all neurons in the dataset that have proofread axons\n\nproof_df = client.materialize.tables.proofreading_status_and_strategy(status_axon='t').query(desired_resolution=[1, 1, 1], split_positions=True)\nproof_df[\"strategy_axon\"].value_counts()\n\nstrategy_axon\naxon_partially_extended    1702\naxon_fully_extended         156\naxon_interareal             124\nName: count, dtype: int64\n\n\n\n\nQuery synapses between proofread neurons\nWe can query the graph spanned by the neurons with proofread axons using synapse query.\n\n\n\n\n\n\nTip\n\n\n\nHere we specify both the pre_ids and post_ids, meaning we will only be interpretting the connectivity between cells that both have proofread axons. This makes plotting the square connectivity matrix easier.\nHowever, connectivity between two cells is generally interpretable even if only the presynaptic cell has axon proofreading. Meaning, you can remove the post_ids argument in the section below, and still have interpretable data.\n\n\n\n%%time\n# This takes 4-6 minutes to complete\nsyn_proof_only_df = client.materialize.synapse_query(pre_ids=proof_df.pt_root_id,\n                                                  post_ids=proof_df.pt_root_id,\n                                                  remove_autapses=True,\n                                                 )\n\nprint(len(syn_proof_only_df))\n\n183905\nCPU times: total: 1.17 s\nWall time: 6min 14s\n\n\n\n\nPlot connectivity as binarized heatmap\nNow lets plot the connectivity between every proofread cell and every other cell. This uses the pandas.pivot_table() to turn the long-form synapse table into a connectivity matrix.\n\n%%time\n# This takes 2 minutes to complete\nsyn_mat = syn_proof_only_df.pivot_table(index=\"pre_pt_root_id\", \n                                        columns=\"post_pt_root_id\", \n                                        values=\"size\", \n                                        aggfunc=lambda x: float(np.sum(x) &gt; 0)\n                                       ).fillna(0)\nsyn_mat = syn_mat.reindex(columns=np.array(syn_mat.index))\n\nCPU times: total: 1min 49s\nWall time: 2min 37s\n\n\nPlot the binarized connectivity with Seaborn heatmap():\n\nfig, ax = plt.subplots(figsize=(7, 5), dpi=150)\nsns.heatmap(syn_mat, cmap=\"gray_r\", xticklabels=[], yticklabels=[], \n            ax=ax, square=True,\n            cbar_kws={\"label\": \"Connected - binary\"})\n\nax.set_title('Connectivity between proofread cells')\n\nText(0.5, 1.0, 'Connectivity between proofread cells')\n\n\n\n\n\n\n\n\n\nThere is some structure of highly interconnected cells. By adding information about the type of cells, we might infer more about the connectivity patterns",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Synaptic Connectivity"
    ]
  },
  {
    "objectID": "quickstart_notebooks/04-cave-query-synapses.html#add-cell-type-information-to-connectivity",
    "href": "quickstart_notebooks/04-cave-query-synapses.html#add-cell-type-information-to-connectivity",
    "title": "CAVE Query: Synaptic Connectivity",
    "section": "Add cell type information to connectivity",
    "text": "Add cell type information to connectivity\n\nQuerying cell type information\nThere are two distinct ways cell types were classified in the MICrONS dataset: manual and automated. Manual annotations are available for ~1,000 neurons (allen_v1_column_types_slanted_ref), automated classifications are available for all cell bodies based on these manual annotations (aibs_metamodel_celltypes_v661). For more about querying cell types tables, see the previous quickstart notebook.\n\n\n\n\n\n\nTip\n\n\n\nFor more on cell types and how to interpret them, see the Annotation Tables page.\n\n\n\nct_manual_df = client.materialize.tables.allen_v1_column_types_slanted_ref().query()\n\n# rename the reference column for clarity\nct_manual_df.rename(columns={'target_id': 'nucleus_id'}, inplace=True)\n\n# remove segments with multiple cell bodies\nct_manual_df.drop_duplicates(\"pt_root_id\", keep=False, inplace=True)\nct_manual_df.head(5)\n\n\n\n\n\n\n\n\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\nid_ref\ncreated_ref\nvalid_ref\nnucleus_id\nclassification_system\ncell_type\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n258319\n2020-09-28 22:40:42.476911+00:00\nt\n261.806162\n89309001002848425\n864691136021936376\n50\n2023-03-18 14:13:21.613360+00:00\nt\n258319\naibs_coarse_excitatory\n23P\n[178400, 143248, 21238]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n276438\n2020-09-28 22:40:42.700226+00:00\nt\n277.317714\n89465269428261699\n864691135164434989\n1119\n2023-03-18 14:13:22.506660+00:00\nt\n276438\naibs_coarse_excitatory\n6P-CT\n[179648, 258768, 23597]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n260552\n2020-09-28 22:40:42.745779+00:00\nt\n230.111805\n89170256379033022\n864691135784109363\n35\n2023-03-18 14:13:21.602813+00:00\nt\n260552\naibs_coarse_excitatory\n23P\n[177408, 157968, 21002]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n260263\n2020-09-28 22:40:42.746658+00:00\nt\n274.324193\n88044356338331571\n864691135694415551\n95\n2023-03-18 14:13:21.644304+00:00\nt\n260263\naibs_coarse_excitatory\n23P\n[169440, 158128, 20266]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n262898\n2020-09-28 22:40:42.749245+00:00\nt\n230.092308\n88468836747612860\n864691135778773856\n81\n2023-03-18 14:13:21.634505+00:00\nt\n262898\naibs_coarse_inhibitory\nBPC\n[172512, 175280, 21964]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\nThis table is a reference on the nucleus_detection_v0 table, and adds two additional data columns: classification_system and cell_type. The classification_system divides the cells into excitatitory and inhibitory neurons as well as non-neuronal cells. cell_type provides lower level cell annotations.\nNext, we query the automatically classified cell type information. The query works the same way:\n\nct_auto_df = client.materialize.tables.aibs_metamodel_celltypes_v661().query()\n\n# rename the reference column for clarity\nct_auto_df.rename(columns={'target_id': 'nucleus_id'}, inplace=True)\n\n# remove segments with multiple cell bodies\nct_auto_df.drop_duplicates(\"pt_root_id\", keep=False, inplace=True)\nct_auto_df.head(5)\n\n\n\n\n\n\n\n\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\nid_ref\ncreated_ref\nvalid_ref\nnucleus_id\nclassification_system\ncell_type\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n336365\n2020-09-28 22:42:48.966292+00:00\nt\n272.488202\n93606511657924288\n864691136274724621\n36916\n2023-12-19 22:47:18.659864+00:00\nt\n336365\nexcitatory_neuron\n5P-IT\n[209760, 180832, 27076]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n110648\n2020-09-28 22:45:09.650639+00:00\nt\n328.533443\n79385153184885329\n864691135489403194\n1070\n2023-12-19 22:38:00.472115+00:00\nt\n110648\nexcitatory_neuron\n23P\n[106448, 129632, 25410]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n112071\n2020-09-28 22:43:34.088785+00:00\nt\n272.929423\n79035988248401958\n864691136147292311\n1099\n2023-12-19 22:38:00.898837+00:00\nt\n112071\nexcitatory_neuron\n23P\n[103696, 149472, 15583]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n197927\n2020-09-28 22:43:10.652649+00:00\nt\n91.308851\n84529699506051734\n864691135655940290\n13259\n2023-12-19 22:41:14.417986+00:00\nt\n197927\nnonneuron\noligo\n[143600, 186192, 26471]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n198087\n2020-09-28 22:41:36.677186+00:00\nt\n161.744978\n83756261929388963\n864691135809440972\n13271\n2023-12-19 22:41:14.685474+00:00\nt\n198087\nnonneuron\nastrocyte\n[137952, 190944, 27361]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\n\nct_auto_df[\"classification_system\"].value_counts()\n\nclassification_system\nexcitatory_neuron    63776\nnonneuron            18716\ninhibitory_neuron     7855\nName: count, dtype: int64\n\n\n\nct_auto_df[\"cell_type\"].value_counts()\n\ncell_type\n23P          19647\n4P           14723\n6P-IT        11638\n5P-IT         7892\nastrocyte     7111\noligo         6900\n6P-CT         6759\nBC            3312\nMC            2438\nmicroglia     2400\n5P-ET         2159\nBPC           1484\nOPC           1459\n5P-NP          958\npericyte       846\nNGC            621\nName: count, dtype: int64\n\n\nWe can merge the manual and automatic cell types together into a single cell type table for convenience, using pandas.merge() on shared columns [pt_root_id, nucleus_id]. Here we perform an outer merge to keep rows that exist in either table.\n\nct_all_df = (pd.merge(ct_auto_df[['pt_root_id','classification_system','cell_type', 'nucleus_id']],\n                     ct_manual_df[['pt_root_id','classification_system','cell_type','nucleus_id']],\n                     on=['pt_root_id','nucleus_id'],\n                     how='outer',\n                     suffixes=['_auto','_manual'],\n                    )\n             .fillna({'cell_type_auto': 'unknown',\n                      'classification_system_auto': 'unknown',\n                      'cell_type_manual': 'unknown',\n                      'classification_system_manual ': 'unknown',}\n                    )\n            )\nct_all_df.tail()\n\n\n\n\n\n\n\n\npt_root_id\nclassification_system_auto\ncell_type_auto\nnucleus_id\nclassification_system_manual\ncell_type_manual\n\n\n\n\n90372\n864691137199238721\nexcitatory_neuron\n4P\n260813\nNaN\nunknown\n\n\n90373\n864691137199243585\nnonneuron\nastrocyte\n304889\nNaN\nunknown\n\n\n90374\n864691137199255873\nexcitatory_neuron\n23P\n358554\nNaN\nunknown\n\n\n90375\n864691137199268417\nnonneuron\nastrocyte\n266983\nNaN\nunknown\n\n\n90376\n864691137199312705\nexcitatory_neuron\n23P\n422139\nNaN\nunknown\n\n\n\n\n\n\n\n\n\nSorting the synapse matrix with cell types\nLet’s combine the synaptic connecitivity with the cell type information. Below we provide logic for sorting a connectivity matrix using a list of labels.\nThis example looks at the connectivity among: proofread excitatory cells in the V1 column . Using proofreading_status_and_strategy and allen_v1_column_types_slanted_ref to filter the connectivity. The same approach can be used for all cells in the dataset by substituting the cell_type_auto label from aibs_metamodel_celltypes_v661.\n\ndef sort_matrix_by_types(mat: pd.DataFrame, \n                         labels: pd.DataFrame, \n                         label_type_col: str = \"cell_type_auto\", \n                         label_id_col: str = \"pt_root_id\", \n                         post_labels: pd.DataFrame = None, \n                         post_label_type_col: str = None, \n                         post_label_id_col: str = None):\n    \"\"\"Sorts (synapse) matrix by labels.\n\n    This function assumes a square synapse matrix!\n\n    Args:\n        mat: synapse matrix as pandas DataFrame\n        labels: DataFrame with labels, e.g. the output of client.materialize.query_table('aibs_metamodel_celltypes_v661')\n        label_type_col: column name in labels for cell types\n        label_id_col: column name in labels for root ids\n        post_labels: DataFrame with labels, e.g. the output of client.materialize.query_table('aibs_metamodel_celltypes_v661')\n        post_label_type_col: column name in labels for cell types\n        post_label_id_col: column name in labels for root ids\n\n    Returns:\n        mat_sorted: sorted matrix\n        mat_labels: sorted labels; has the same length as matrix\n    \"\"\"\n    \n    if post_labels is None:\n        post_labels = labels\n    if post_label_type_col is None:\n        post_label_type_col = label_type_col\n    if post_label_id_col is None:\n        post_label_id_col = label_id_col\n        \n    mat_sorted = mat.copy()\n    \n    pre_mat_labels = np.array(labels.set_index(label_id_col).loc[mat_sorted.index][label_type_col])\n    pre_sorting = np.argsort(pre_mat_labels)\n\n    post_mat_labels = np.array(post_labels.set_index(post_label_id_col).loc[mat_sorted.T.index][post_label_type_col])\n    post_sorting = np.argsort(post_mat_labels)\n\n    mat_sorted = mat_sorted.iloc[pre_sorting].T.iloc[post_sorting].T\n\n    return mat_sorted, pre_mat_labels[pre_sorting], post_mat_labels[post_sorting]\n\n\n# Select the proofread, manually-identified excitatory cells\nmanual_exc_root_ids = ct_all_df.query(\"classification_system_manual=='aibs_coarse_excitatory'\").pt_root_id.to_numpy()\n\n# Filter the proofread synapses by the excitatory cells\nexc_syn_df = syn_proof_only_df.loc[(syn_proof_only_df.pre_pt_root_id.isin(manual_exc_root_ids) & \n                                    syn_proof_only_df.post_pt_root_id.isin(manual_exc_root_ids) \n                                   )]\n\n# Pivot synapse matrix\nexc_syn_mat = (exc_syn_df.pivot_table(index=\"pre_pt_root_id\", \n                                     columns=\"post_pt_root_id\", \n                                     values=\"size\", \n                                     aggfunc=lambda x: float(np.sum(x) &gt; 0)).fillna(0)\n              )\nexc_syn_mat = exc_syn_mat.reindex(columns=np.array(exc_syn_mat.index))\n\n\n# sort the matrix by cell types to render sensibly in heatmap\nsyn_mat_ct, syn_mat_cell_types, _ = sort_matrix_by_types(exc_syn_mat, ct_all_df, label_type_col=\"cell_type_manual\")\n\n\n\nPlot cell connectivity, sorted by cell type\n\nimport matplotlib\n\n# add colormap for cell type\ncts, ct_idx = np.unique(syn_mat_cell_types, return_inverse=True)\nct_colors = plt.get_cmap(\"tab10\")(ct_idx)\n\nfig, ax = plt.subplots(figsize=(7, 5), dpi=150)\nsns.heatmap(syn_mat_ct, cmap=\"gray_r\", xticklabels=[], yticklabels=[], \n            ax=ax, square=True,\n            cbar_kws={\"label\": \"Connected - binary\"})\n\n\n# add row and column colors for cell types\nfor i, color in enumerate(ct_colors):\n    ax.add_patch(plt.Rectangle(xy=(-0.01, i), width=0.01, height=1, color=color, lw=0,\n                               transform=ax.get_yaxis_transform(), clip_on=False))\n\nfor i, color in enumerate(ct_colors):\n    ax.add_patch(plt.Rectangle(xy=(i, 1), height=0.01, width=1, color=color, lw=0,\n                               transform=ax.get_xaxis_transform(), clip_on=False))\n\n# add a legend for the cell types\nlegend_elements = [matplotlib.lines.Line2D([0], [0], color=plt.get_cmap(\"tab10\")(i), label=ct) for i, ct in enumerate(cts)]\nplt.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1.3, 1), title=\"cell types\")\n\nax.set_title('Connectivity between proofread excitatory cells \\n (cell type manually labeled)')\nplt.show()",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Synaptic Connectivity"
    ]
  },
  {
    "objectID": "quickstart_notebooks/04-cave-query-synapses.html#aggregate-connectivity-across-cell-types",
    "href": "quickstart_notebooks/04-cave-query-synapses.html#aggregate-connectivity-across-cell-types",
    "title": "CAVE Query: Synaptic Connectivity",
    "section": "Aggregate connectivity across cell types",
    "text": "Aggregate connectivity across cell types\nSometimes it is more useful to consider connectivity between groups of cells, rather than individual cells–especially as the number of indivudal cells soars into the thousands. In this example we will aggregate synaptic connectivity to broad cell types.\n\nMerge cell types to synapse table\nHere, we can assign a cell type to the pre-synaptic and post-synaptic root id on the connectivity matrix. Given the established syn_proof_only_df and the merged cell type table ct_all_df, let’s consider the connectivity between manually identified cell types:\n\n# Merge the cell types to the presynaptic cell id\nsyn_proof_ct_df = ( syn_proof_only_df.merge(\n    ct_all_df[['pt_root_id','cell_type_manual']], \n    left_on='pre_pt_root_id', \n    right_on='pt_root_id',\n    how='left' )\n.rename(columns={'cell_type_manual': 'cell_type_pre'})\n.drop(columns=['pt_root_id'])\n)\n\n# Merge the cell types to the postsynaptic cell id\nsyn_proof_ct_df = ( syn_proof_ct_df.merge(\n    ct_all_df[['pt_root_id','cell_type_manual']], \n    left_on='post_pt_root_id', \n    right_on='pt_root_id',\n    how='left' )\n.rename(columns={'cell_type_manual': 'cell_type_post'})\n.drop(columns=['pt_root_id'])\n)\n\nsyn_proof_ct_df.head(3)\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\nsize\npre_pt_supervoxel_id\npre_pt_root_id\npost_pt_supervoxel_id\npost_pt_root_id\npre_pt_position\npost_pt_position\nctr_pt_position\ncell_type_pre\ncell_type_post\n\n\n\n\n0\n174979485\n2020-11-04 08:50:39.220791+00:00\nNaN\nt\n4688\n90161191099360974\n864691135817602127\n90161191099310671\n864691136674495623\n[184770, 200984, 19986]\n[184786, 201052, 19972]\n[184816, 200974, 19981]\nunknown\n5P-IT\n\n\n1\n152094175\n2020-11-04 07:20:36.943498+00:00\nNaN\nt\n14020\n88401010355440214\n864691135778700477\n88401010355435596\n864691135561699041\n[171952, 194076, 19787]\n[171996, 194038, 19786]\n[172002, 194074, 19793]\n23P\nMC\n\n\n2\n161901009\n2020-11-04 14:35:02.898645+00:00\nNaN\nt\n668\n89030412042836638\n864691135817602127\n89030480762316394\n864691136380816597\n[176280, 165046, 19727]\n[176256, 165108, 19730]\n[176238, 165052, 19722]\nunknown\nUnsure E\n\n\n\n\n\n\n\n\n\nPivot the synapse table to collect cell type connectivity\nUsing pandas.pivot() on the pre- and post-synaptic cell classes, we count the number of connections between:\n\n# Pivot table to get connectivity between cell types\nsyn_ct_counts = (syn_proof_ct_df.pivot_table(index='cell_type_pre',\n                                             columns='cell_type_post',\n                                             values='id', aggfunc=lambda x: len(x)).fillna(0)\n                 .drop(columns=['Unsure E','Unsure I','Unsure', 'WM-P', 'unknown'], index=['Unsure E','Unsure I','Unsure', 'WM-P', 'unknown'])\n                 .astype(int) \n                )\n\n# Sort by names\nsyn_ct_counts = syn_ct_counts.reindex(sorted(syn_ct_counts.columns), axis=1)\nsyn_ct_counts = syn_ct_counts.reindex(sorted(syn_ct_counts.index), axis=0)\n\n# consistent axis naming\nsyn_ct_counts = syn_ct_counts.rename_axis(['presyn cell type'], axis=0)\nsyn_ct_counts = syn_ct_counts.rename_axis(['postsyn cell type'], axis=1)\n\n\n\nPlot cell connectivity, aggregated by cell type\nThis now summarizes connectivity between broad cell classes\n\nfig, ax = plt.subplots(figsize=(7, 5), dpi=150)\n\nsns.heatmap(syn_ct_counts, cmap=\"gist_heat_r\", annot=True, ax=ax, fmt='d', vmin=0, vmax=15000,\n            cbar_kws={'label': 'synapses', 'location': 'right'},\n            annot_kws={\"fontsize\":6},\n            square=True)\n\nax.tick_params(left=False, bottom=False) \nax.hlines([8], *ax.get_xlim(), color='white',linewidth = 3)\nax.vlines([8], *ax.get_ylim(), color='white',linewidth = 3)\n\nax.set_title('Aggregate Connectivity between proofread cells \\n (cell type manually labeled)')\n\nText(0.5, 1.0, 'Aggregate Connectivity between proofread cells \\n (cell type manually labeled)')\n\n\n\n\n\n\n\n\n\n\nFinal note\n\nIt is worth keeping in mind that this connectivity matrix is highly dependendant on:\n\nWhich set of cell-type labels you include (here: manual labels in the V1 column)\nWhich set of proofread cells you include (here: most strict inclusion for pre- and post-synaptic partners)\n\nThe more cells are proofread in the dataset, the more consistency and reproducibility you will get from connecitivty diagrams. See VORTEX program to request specific proofreading.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Synaptic Connectivity"
    ]
  },
  {
    "objectID": "quickstart_notebooks/02-cave-query-cell-types.html",
    "href": "quickstart_notebooks/02-cave-query-cell-types.html",
    "title": "CAVE Query: Cell Types",
    "section": "",
    "text": "Version update\n\n\n\nThis tutorial has recently been updated to materialization version 1412 (from 1300).\nWe have released a new public version 1412, as part of our quarterly release schedule. See details at Release Manifests: 1412.\nThe Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Cell Types"
    ]
  },
  {
    "objectID": "quickstart_notebooks/02-cave-query-cell-types.html#caveclient-setup",
    "href": "quickstart_notebooks/02-cave-query-cell-types.html#caveclient-setup",
    "title": "CAVE Query: Cell Types",
    "section": "CAVEclient Setup",
    "text": "CAVEclient Setup\nMost programmatic access to the CAVE services occurs through CAVEclient, a Python client to access various types of data from the online services.\nFull documentation for CAVEclient is available here.\nTo initialize a caveclient, we give it a datastack, which is a name that defines a particular combination of imagery, segmentation, and annotation database. For the MICrONs public data, we use the datastack name minnie65_public.\n\nfrom caveclient import CAVEclient\ndatastack_name = 'minnie65_public'\nclient = CAVEclient(datastack_name)\n\n# Show the description of the datastack\nclient.info.get_datastack_info()['description']\n\n'This is the publicly released version of the minnie65 volume and segmentation. '\n\n\n\nMaterialization versions\nData in CAVE is timestamped and periodically versioned - each (materialization) version corresponds to a specific timestamp. Individual versions are made publicly available. The Materialization client allows one to interact with the materialized annotation tables that were posted to the annotation service. These are called queries to the dataset, and available from client.materialize. For more, see the CAVEclient Documentation.\nPeriodic updates are made to the public datastack, which will include updates to the available tables. Some cells will have different pt_root_id because they have undergone proofreading.\n\n\n\n\n\n\nTip\n\n\n\nFor analysis consistency, is worth checking the version of the data you are using, and consider specifying the version with client.version = your_version\nRead more about setting the version of your analysis\n\n\n\n# see the available materialization versions\nclient.materialize.get_versions()\n\n[1300, 1078, 117, 661, 343, 1181, 795, 943, 1412]\n\n\nAnd these are their associated timestamps (all timestamps are in UTC):\n\nfor version in client.materialize.get_versions():\n    print(f\"Version {version}: {client.materialize.get_timestamp(version)}\")\n\nVersion 1300: 2025-01-13 10:10:01.286229+00:00\nVersion 1078: 2024-06-05 10:10:01.203215+00:00\nVersion 117: 2021-06-11 08:10:00.215114+00:00\nVersion 661: 2023-04-06 20:17:09.199182+00:00\nVersion 343: 2022-02-24 08:10:00.184668+00:00\nVersion 1181: 2024-09-16 10:10:01.121167+00:00\nVersion 795: 2023-08-23 08:10:01.404268+00:00\nVersion 943: 2024-01-22 08:10:01.497934+00:00\nVersion 1412: 2025-04-29 10:10:01.200893+00:00\n\n\n\n# set materialization version, for consistency\nclient.version = 1412 # current public as of 4/29/2025",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Cell Types"
    ]
  },
  {
    "objectID": "quickstart_notebooks/02-cave-query-cell-types.html#caveclient-materialization-basics",
    "href": "quickstart_notebooks/02-cave-query-cell-types.html#caveclient-materialization-basics",
    "title": "CAVE Query: Cell Types",
    "section": "CAVEclient Materialization Basics",
    "text": "CAVEclient Materialization Basics\nThe most frequent use of the CAVEclient is to query the database for annotations like synapses. All database functions are under the client.materialize property. To see what tables are available, use the get_tables function:\n\nclient.materialize.get_tables()\n\n['bodor_pt_target_proofread',\n 'nucleus_functional_area_assignment',\n 'proofreading_status_and_strategy',\n 'coregistration_auto_phase3_fwd_apl_vess_combined_v2',\n 'vortex_thalamic_proofreading_status',\n 'multi_input_spine_predictions_ssa',\n 'synapse_target_structure',\n 'coregistration_auto_phase3_fwd_v2',\n 'vortex_astrocyte_proofreading_status',\n 'gamlin_2023_mcs_met_types',\n 'vortex_manual_myelination_v0',\n 'synapse_target_predictions_ssa',\n 'aibs_metamodel_celltypes_v661',\n 'baylor_gnn_cell_type_fine_model_v2',\n 'nucleus_alternative_points',\n 'allen_column_mtypes_v2',\n 'bodor_pt_cells',\n 'aibs_metamodel_mtypes_v661_v2',\n 'allen_v1_column_types_slanted_ref',\n 'aibs_column_nonneuronal_ref',\n 'nucleus_ref_neuron_svm',\n 'apl_functional_coreg_vess_fwd',\n 'vortex_compartment_targets',\n 'baylor_log_reg_cell_type_coarse_v1',\n 'functional_properties_v3_bcm',\n 'gamlin_2023_mcs',\n 'l5et_column',\n 'pt_synapse_targets',\n 'coregistration_manual_v4',\n 'cg_cell_type_calls',\n 'synapses_pni_2',\n 'nucleus_detection_v0',\n 'vortex_manual_nodes_of_ranvier']\n\n\nFor each table, you can see the metadata describing that table. For example, let’s look at the nucleus_detection_v0 table:\n\nclient.materialize.get_table_metadata('nucleus_detection_v0')\n\n{'aligned_volume': 'minnie65_phase3',\n 'table_name': 'nucleus_detection_v0',\n 'schema': 'nucleus_detection',\n 'valid': True,\n 'created': '2020-11-02T18:56:35.530100',\n 'id': 65567,\n 'schema_type': 'nucleus_detection',\n 'user_id': '121',\n 'description': 'A table of nuclei detections from a nucleus detection model developed by Shang Mu, Leila Elabbady, Gayathri Mahalingam and Forrest Collman. Pt is the centroid of the nucleus detection. id corresponds to the flat_segmentation_source segmentID. Only included nucleus detections of volume&gt;25 um^3, below which detections are false positives, though some false positives above that threshold remain. ',\n 'notice_text': None,\n 'reference_table': None,\n 'flat_segmentation_source': 'precomputed://https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/nuclei',\n 'write_permission': 'PRIVATE',\n 'read_permission': 'PUBLIC',\n 'last_modified': '2022-10-25T19:24:28.559914',\n 'segmentation_source': '',\n 'pcg_table_name': 'minnie3_v1',\n 'last_updated': '2025-06-16T22:00:00.121284',\n 'voxel_resolution': [4.0, 4.0, 40.0]}\n\n\nYou get a dictionary of values. Two fields are particularly important: the description, which offers a text description of the contents of the table and voxel_resolution which defines how the coordinates in the table are defined, in nm/voxel.\n\n\n\n\n\n\nAnnotation tables\n\n\n\nYou can also find a semantic description of the most commonly used tables at the Annotation Tables page.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Cell Types"
    ]
  },
  {
    "objectID": "quickstart_notebooks/02-cave-query-cell-types.html#querying-tables",
    "href": "quickstart_notebooks/02-cave-query-cell-types.html#querying-tables",
    "title": "CAVE Query: Cell Types",
    "section": "Querying Tables",
    "text": "Querying Tables\nTo get the contents of a table, use the query_table function. This will return the whole contents of a table without any filtering, up to for a maximum limit of 200,000 rows. The table is returned as a Pandas DataFrame and you can immediately use standard Pandas function on it.\n\ncell_type_df = client.materialize.query_table('nucleus_detection_v0')\ncell_type_df.head()\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n730537\n2020-09-28 22:40:41.780734+00:00\nNaN\nt\n32.307937\n0\n0\n[381312, 273984, 19993]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n373879\n2020-09-28 22:40:41.781788+00:00\nNaN\nt\n229.045043\n96218056992431305\n864691136090135607\n[228816, 239776, 19593]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n601340\n2020-09-28 22:40:41.782714+00:00\nNaN\nt\n426.138010\n0\n0\n[340000, 279152, 20946]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n201858\n2020-09-28 22:40:41.783784+00:00\nNaN\nt\n93.753836\n84955554103121097\n864691135373893678\n[146848, 213600, 26267]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n600774\n2020-09-28 22:40:41.785273+00:00\nNaN\nt\n135.189791\n0\n0\n[339120, 276112, 19442]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nWhile most tables are small enough to be returned in full, the synapse table has hundreds of millions of rows and is too large to download this way\n\n\nTables have a collection of columns, some of which specify point in space (columns ending in _position), some a root id (ending in _root_id), and others that contain other information about the object at that point. Before describing some of the most important tables in the database, it’s useful to know about a few advanced options that apply when querying any table.\n\ndesired_resolution : This parameter allows you to convert the columns specifying spatial points to different resolutions. Many tables are stored at a resolution of 4x4x40 nm/voxel, for example, but you can convert to nanometers by setting desired_resolution=[1,1,1].\nsplit_positions : This parameter allows you to split the columns specifying spatial points into separate columns for each dimension. The new column names will be the original column name with _x, _y, and _z appended.\nselect_columns : This parameter allows you to get only a subset of columns from the table. Once you know exactly what you want, this can save you some cleanup.\nlimit : This parameter allows you to limit the number of rows returned. If you are just testing out a query or trying to inspect the kind of data within a table, you can set this to a small number to make sure it works before downloading the whole table. Note that this will show a warning so that you don’t accidentally limit your query when you don’t mean to.\n\nFor example, using all of these together:\n\ncell_type_df = client.materialize.query_table('nucleus_detection_v0', split_positions=True, desired_resolution=[1,1,1], select_columns=['pt_position', 'pt_root_id'], limit=10)\ncell_type_df\n\n\n\n\n\n\n\n\npt_position_x\npt_position_y\npt_position_z\npt_root_id\n\n\n\n\n0\n241856.0\n374464.0\n838720.0\n0\n\n\n1\n227200.0\n389120.0\n797160.0\n0\n\n\n2\n230144.0\n422336.0\n795320.0\n0\n\n\n3\n239488.0\n386432.0\n794120.0\n0\n\n\n4\n239744.0\n423488.0\n803120.0\n864691136050815731\n\n\n5\n245888.0\n384512.0\n800120.0\n0\n\n\n6\n249792.0\n391680.0\n807080.0\n0\n\n\n7\n243328.0\n403008.0\n794280.0\n0\n\n\n8\n247872.0\n386816.0\n805320.0\n0\n\n\n9\n260352.0\n416640.0\n802360.0\n864691135013273238",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Cell Types"
    ]
  },
  {
    "objectID": "quickstart_notebooks/02-cave-query-cell-types.html#filtering-queries",
    "href": "quickstart_notebooks/02-cave-query-cell-types.html#filtering-queries",
    "title": "CAVE Query: Cell Types",
    "section": "Filtering Queries",
    "text": "Filtering Queries\nFiltering tables so that you only get data about certain rows back is a very common operation. While there are filtering options in the query_table function (see documentation for more details), a more unified filter interface is available through a “table manager” interface.\nRather than passing a table name to the query_table function, client.materialize.tables has a subproperty for each table in the database that can be used to filter that table.\nThe general pattern for usage is\nclient.materialize.tables.{table_name}({filter options}).query({format and timestamp options})\nwhere {table_name} is the name of the table you want to filter, {filter options} is a collection of arguments for filtering the query, and {format and timestamp options} are those parameters controlling the format and timestamp of the query.\n\n\n\n\n\n\nCaution\n\n\n\nUse of this functionality will show a brief warning that the interface is experimental. This is because the interface is still being developed and may change in the near future in response to user feedback.\n\n\nFor example, let’s look at the table aibs_metamodel_celltypes_v661, which has cell type predictions across the dataset. We can get the whole table as a DataFrame:\n\ncell_type_df = client.materialize.tables.aibs_metamodel_celltypes_v661().query()\ncell_type_df.head()\n\n\n\n\n\n\n\n\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\nid_ref\ncreated_ref\nvalid_ref\ntarget_id\nclassification_system\ncell_type\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n336365\n2020-09-28 22:42:48.966292+00:00\nt\n272.488202\n93606511657924288\n864691136274724621\n36916\n2023-12-19 22:47:18.659864+00:00\nt\n336365\nexcitatory_neuron\n5P-IT\n[209760, 180832, 27076]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n110648\n2020-09-28 22:45:09.650639+00:00\nt\n328.533443\n79385153184885329\n864691135489403194\n1070\n2023-12-19 22:38:00.472115+00:00\nt\n110648\nexcitatory_neuron\n23P\n[106448, 129632, 25410]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n112071\n2020-09-28 22:43:34.088785+00:00\nt\n272.929423\n79035988248401958\n864691136147292311\n1099\n2023-12-19 22:38:00.898837+00:00\nt\n112071\nexcitatory_neuron\n23P\n[103696, 149472, 15583]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n197927\n2020-09-28 22:43:10.652649+00:00\nt\n91.308851\n84529699506051734\n864691135655940290\n13259\n2023-12-19 22:41:14.417986+00:00\nt\n197927\nnonneuron\noligo\n[143600, 186192, 26471]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n198087\n2020-09-28 22:41:36.677186+00:00\nt\n161.744978\n83756261929388963\n864691135809440972\n13271\n2023-12-19 22:41:14.685474+00:00\nt\n198087\nnonneuron\nastrocyte\n[137952, 190944, 27361]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\nand we can add similar formatting options as in the last section to the query function:\n\ncell_type_df = client.materialize.tables.aibs_metamodel_celltypes_v661().query(split_positions=True, desired_resolution=[1,1,1], select_columns=['pt_position', 'pt_root_id', 'cell_type'], limit=10)\ncell_type_df\n\n\n\n\n\n\n\n\npt_position_x\npt_position_y\npt_position_z\npt_root_id\ncell_type\n\n\n\n\n0\n257600.0\n487936.0\n802760.0\n864691135724233643\n23P\n\n\n1\n260992.0\n493568.0\n801560.0\n864691136436395166\n23P\n\n\n2\n256256.0\n466432.0\n831040.0\n864691135462260637\nNGC\n\n\n3\n255744.0\n480640.0\n833200.0\n864691136723556861\n23P\n\n\n4\n262144.0\n505856.0\n824880.0\n864691135776658528\n23P\n\n\n5\n257536.0\n521728.0\n804440.0\n864691135941166708\n23P\n\n\n6\n251840.0\n552896.0\n832320.0\n864691135545065768\n23P\n\n\n7\n251136.0\n546048.0\n821320.0\n864691135479369926\n23P\n\n\n8\n256000.0\n626368.0\n814000.0\n864691135697633557\n23P\n\n\n9\n324096.0\n417920.0\n658880.0\n864691135937358133\nastrocyte\n\n\n\n\n\n\n\nHowever, now we can also filter the table to get only cells that are predicted to have cell type \"BC\" (for “basket cell”).\n\nmy_cell_type = \"BC\"\nclient.materialize.tables.aibs_metamodel_celltypes_v661(cell_type=my_cell_type).query()\n\n\n\n\n\n\n\n\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\nid_ref\ncreated_ref\nvalid_ref\ntarget_id\nclassification_system\ncell_type\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n369908\n2020-09-28 22:40:41.814964+00:00\nt\n332.862751\n96002690286851358\n864691136145011252\n43009\n2023-12-19 22:48:53.577191+00:00\nt\n369908\ninhibitory_neuron\nBC\n[227104, 207840, 20841]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n193846\n2020-09-28 22:40:41.897904+00:00\nt\n306.148966\n82838443188669165\n864691135578780933\n12051\n2023-12-19 22:40:57.133228+00:00\nt\n193846\ninhibitory_neuron\nBC\n[131568, 168496, 16452]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n615735\n2020-09-28 22:40:41.957345+00:00\nt\n314.539540\n112181247505371364\n864691135183493378\n83044\n2023-12-19 22:58:50.269173+00:00\nt\n615735\ninhibitory_neuron\nBC\n[344880, 161104, 17084]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n401681\n2020-09-28 22:40:42.066718+00:00\nt\n497.801462\n98465046644219429\n864691136052141043\n48718\n2023-12-19 22:50:21.192138+00:00\nt\n401681\ninhibitory_neuron\nBC\n[245232, 203952, 21268]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n613047\n2020-09-28 22:40:41.982376+00:00\nt\n242.159780\n113234168401651200\n864691136065413528\n82324\n2023-12-19 22:58:39.896999+00:00\nt\n613047\ninhibitory_neuron\nBC\n[352688, 141616, 25312]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3360\n208056\n2020-09-28 22:45:25.401800+00:00\nt\n521.621668\n84540007091735344\n864691135801456226\n15548\n2023-12-19 22:41:48.382554+00:00\nt\n208056\ninhibitory_neuron\nBC\n[143472, 262944, 23693]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3361\n438586\n2020-09-28 22:45:25.430745+00:00\nt\n529.501389\n99807894274485381\n864691135395662581\n55791\n2023-12-19 22:52:02.582669+00:00\nt\n438586\ninhibitory_neuron\nBC\n[254912, 247440, 23680]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3362\n170777\n2020-09-28 22:45:25.310708+00:00\nt\n499.103662\n81230957054577082\n864691135065994564\n8968\n2023-12-19 22:40:09.246333+00:00\nt\n170777\ninhibitory_neuron\nBC\n[119600, 250560, 15373]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3363\n591219\n2020-09-28 22:45:25.526753+00:00\nt\n567.517839\n110216764830845707\n864691135279126177\n79472\n2023-12-19 22:57:53.993099+00:00\nt\n591219\ninhibitory_neuron\nBC\n[330320, 204752, 25060]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3364\n419363\n2020-09-28 22:45:25.436862+00:00\nt\n530.642698\n99716496901116512\n864691135656051906\n50504\n2023-12-19 22:50:48.576826+00:00\nt\n419363\ninhibitory_neuron\nBC\n[254416, 90336, 20469]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n3365 rows × 15 columns\n\n\n\nor maybe we just want the cell types for a particular collection of root ids:\n\nmy_root_ids = [864691135771677771, 864691135560505569, 864691136723556861]\nclient.materialize.tables.aibs_metamodel_celltypes_v661(pt_root_id=my_root_ids).query()\n\n\n\n\n\n\n\n\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\nid_ref\ncreated_ref\nvalid_ref\ntarget_id\nclassification_system\ncell_type\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n19116\n2020-09-28 22:41:51.767906+00:00\nt\n301.426115\n74737997899501359\n864691135771677771\n11282\n2023-12-19 22:40:43.249642+00:00\nt\n19116\nexcitatory_neuron\n23P\n[72576, 108656, 20291]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n21783\n2020-09-28 22:41:59.966574+00:00\nt\n263.637074\n75795590176519004\n864691135560505569\n15681\n2023-12-19 22:41:50.365399+00:00\nt\n21783\nexcitatory_neuron\n23P\n[80128, 124000, 16563]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n4074\n2020-09-28 22:42:41.341179+00:00\nt\n313.678234\n73543309863605007\n864691136723556861\n50080\n2023-12-19 22:50:42.474168+00:00\nt\n4074\nexcitatory_neuron\n23P\n[63936, 120160, 20830]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\nYou can get a list of all parameters than be used for querying with the standard IPython/Jupyter docstring functionality, e.g. client.materialize.tables.aibs_metamodel_celltypes_v661.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Cell Types"
    ]
  },
  {
    "objectID": "quickstart_notebooks/00_skeletons.html",
    "href": "quickstart_notebooks/00_skeletons.html",
    "title": "Skeletons",
    "section": "",
    "text": "Under construction\n\n\n\nTo add interactive visualization and code outputs"
  },
  {
    "objectID": "quickstart_notebooks/00_skeletons.html#working-with-meshwork-files",
    "href": "quickstart_notebooks/00_skeletons.html#working-with-meshwork-files",
    "title": "Skeletons",
    "section": "Working with Meshwork Files",
    "text": "Working with Meshwork Files\nLoading a meshwork file imports the level 2 graph (the “mesh”), the skeleton, and a collection of associated annotations.\n\nfrom meshparty import meshwork\nnrn = meshwork.load_meshwork(mesh_filename)\n\nThe main three properties of the meshwork object are:\n\nnrn.mesh : The l2graph representation of the reconstruction.\nnrn.skeleton : The skeleton representation of the reconstruction.\nnrn.anno : A table of annotation dataframes and associated metadata that links them to specific vertices in the mesh and skeleton."
  },
  {
    "objectID": "quickstart_notebooks/00_skeletons.html#meshwork-nrn.mesh-vs-nrn.skeleton",
    "href": "quickstart_notebooks/00_skeletons.html#meshwork-nrn.mesh-vs-nrn.skeleton",
    "title": "Skeletons",
    "section": "Meshwork nrn.mesh vs nrn.skeleton",
    "text": "Meshwork nrn.mesh vs nrn.skeleton\nSkeletons are “tree-like”, where every vertex (except the root vertex) has a single parent that is closer to the root than it, and any number of child vertices. Because of this, for a skeleton there are well-defined directions “away from root” and “towards root” and few types of vertices have special names:\n\nBranch point: vertices with two or more children, where a neuronal process splits.\nEnd point: vertices with no childen, where a neuronal process ends.\nRoot point: The one vertex with no parent node. By convention, we typically set the root vertex at the cell body, so these are equivalent to “away from soma” and “towards soma”.\nSegment: A collection of vertices along an unbranched region, between one branch point and the next end point or branch point downstream.\n\nMeshes are arbitrary collections of vertices and edges, but do not have a notion of “parent” or “child” “branch point” or “end point”. Here, this means the “mesh” used here includes a vertex for every level 2 chunk, even where it is thick like at a cell body or very thick dendrite. However, by default this means that there is not always a well-defined notion of parent or child nodes, or towards or away from root.\nIn contrast “Meshes” (really, graphs of connected vertices) do not have a unique “inward” and “outward” direction. For the sake of rapid skeletonization, the “meshes” we use here are really the graph of level 2 vertices as described above. These aren’t a mesh in the visualization sense of the section on downloading Meshes, but have the same data representation.\nTo handle this, the meshwork object associates each mesh vertices with a single nearby skeleton vertex, and each skeleton vertex is associated with one or more mesh vertices. By representing data this way, annotations like synapses can be directly associated with a mesh vertex (because synapses can be anywhere on the object) and then mapped to the skeleton in order to enjoy the topological benefits of the skeleton representation.\n\n# By the definition of skeleton vs mesh, we would expect that mesh contains more vertices than the skeleton. \n# We can see this by looking at the size of the skeleton vertex location array vs the size of the mesh vertex location array.\n\nprint('Skeleton vertices array length:', len(nrn.skeleton.vertices))\nprint('Mesh vertices array length:', len(nrn.mesh.vertices))\n\n\n# Let us try to visualize the skeleton:\n# Visualize the whole skeleton \n\n# here's a simple way to plot vertices of the skeleton\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\n%matplotlib notebook \n\nfig = plt.figure(figsize=(6, 6))\nax = plt.axes(projection='3d')\nax.scatter3D(nrn.skeleton.vertices[:,0], nrn.skeleton.vertices[:,1], nrn.skeleton.vertices[:,2], s=1)\n\n\n\n\nScatterplot of skeleton vertices as a point cloud"
  },
  {
    "objectID": "quickstart_notebooks/00_skeletons.html#skeleton-properties-and-methods",
    "href": "quickstart_notebooks/00_skeletons.html#skeleton-properties-and-methods",
    "title": "Skeletons",
    "section": "Skeleton Properties and Methods",
    "text": "Skeleton Properties and Methods\nTo plot this skeleton in a more sophisticated way, you have to start thinking of it as a graph, and the meshwork object has a bunch of tools and properties to help you utilize the skeleton graph.\nLet’s list some of the most useful ones below You access each of these with nrn.skeleton.* Use the ? to read more details about each one\nProperties\n\n`branch_points``: a list of skeleton vertices which are branches\nroot: the skeleton vertice which is the soma\ndistance_to_root: an array the length of vertices which tells you how far away from the root each vertex is\nroot_position: the position of the root node in nanometers\nend_points: the tips of the neuron\ncover_paths: a list of arrays containing vertex indices that describe individual paths that in total cover the neuron without repeating a vertex. Each path starts at an end point and continues toward root, stopping once it gets to a vertex already listed in a previously defined path. Paths are ordered to start with the end points farthest from root first. Each skeleton vertex appears in exactly one cover path.\ncsgraph: a scipy.sparse.csr.csr_matrix containing a graph representation of the skeleton. Useful to do more advanced graph operations and algorithms. https://docs.scipy.org/doc/scipy/reference/sparse.csgraph.html\nkdtree: a scipy.spatial.ckdtree.cKDTree containing the vertices of skeleton as a kdtree. Useful for quickly finding points that are nearby. https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.cKDTree.html\n\nMethods\n\npath_length(paths=None): the path length of the whole neuron if no arguments, or pass a list of paths to get the path length of that. A path is just a list of vertices which are connected by edges.\npath_to_root(vertex_index): returns the path to the root from the passed vertex\npath_between(source_index, target_index): the shortest path between the source vertex index and the target vertex index\nchild_nodes(vertex_indices): a list of arrays listing the children of the vertex indices passed in\nparent_nodes(vertex_indices): an array listing the parent of the vertex indices passed in\n\n\n# A better way to plot a neuron is to use cover_paths\n# and plot those as 3d lines\ndef plot_neuron_skeleton(neuron, ax, c='b', linewidth=1):\n\n    for cover_path in neuron.skeleton.cover_paths:\n        path_verts = neuron.skeleton.vertices[cover_path,:]\n        ax.plot(path_verts[:,0], path_verts[:,1], path_verts[:,2], c=c, linewidth=linewidth)\n\nfig = plt.figure(figsize=(6, 6))\nax = plt.axes(projection='3d')\nplot_neuron_skeleton(nrn, ax)"
  },
  {
    "objectID": "quickstart_notebooks/00_skeletons.html#indexing-and-selecting-sets-of-points",
    "href": "quickstart_notebooks/00_skeletons.html#indexing-and-selecting-sets-of-points",
    "title": "Skeletons",
    "section": "Indexing And Selecting Sets of Points",
    "text": "Indexing And Selecting Sets of Points\nThe meshworks object contains a mesh with lots of vertices and a skeleton which holds a subset of these vertices. Therefore, in python these points have different “indices” in the mesh and skeleton. For example, if the mesh contains 10000 vertices, the indexing of those would run from 0 - 9999. The skeleton, which contains a subset of 100 of these would have indexing from 0-99. How would you figure out which of the mesh vertices these correspond to?\nLuckily, we have some really nifty functions that help us distinguish those:\nLet us first look at some attributes in the meshworks objects:\nA few nifty function for subselecting points: downstream points and `path_between``. For a given point, downstream points are defined as points on paths from endpoints to the root which are further than the given point. For example, if the skeleton path is : A-B-C-D-E where A is the root, D and E are downstream points of C. With branching, this can be more complex. To find the downstream points from say the 9th branch point, we can do:\n\n# Downstream points\nnrn.downstream_of(nrn.branch_points[9])\n\npath_between returns the vertices from one point to another. For example, we can get the every mesh vertex from the end point 5 to the root point. As a quick visualization, we can look at the distance to root along the, showing that it is descreasing.\n\nfig, ax = plt.subplots()\nax.plot(\n    nrn.distance_to_root(\n        nrn.path_between(\n            nrn.end_points[5],\n            nrn.root,\n        )\n    ) / 1_000,\n)\n\nax.set_ylabel('Distance from root ($\\mu m$)')\nax.set_ylabel('Vertex along path')\n\n\n\n\nDistance to root along graph path indicated above. Note that this flattens out at zero because “distance” is computed along the skeleton and many graph vertices in the level 2 graph are associated with the soma vertex of the skeleton"
  },
  {
    "objectID": "quickstart_notebooks/00_skeletons.html#masking",
    "href": "quickstart_notebooks/00_skeletons.html#masking",
    "title": "Skeletons",
    "section": "Masking",
    "text": "Masking\nJust like meshes, we can mask the meshwork object. Like all basic meshwork functions, the expected input is in mesh vertices. Importantly, doing so will be synchronized across the mesh, the skeleton, and annotations.\n\n# Let's now use masking to highlight one particular cover path.\n\nfig = plt.figure(figsize=(6, 6))\nax = plt.axes(projection=\"3d\")\n\nplot_neuron_skeleton(nrn, ax, \"b\")\n\nnrn.reset_mask()  # This just makes sure we are working with the same baseline object.\n\n# This will make a mask with only the vertices in the first cover path.\nwith neuron.mask_context(nrn.skeleton.cover_paths[0].to_mesh_mask) as nrnf:\n  plot_neuron_skeleton(nrnf, ax, \"r\", linewidth=2)\n  ax.scatter(\n      nrnf.skeleton.root_position[0],\n      nrnf.skeleton.root_position[1],\n      nrnf.skeleton.root_position[2],\n  )\n\nWhile mask_context acts to mask the skeleton and then unmasks it at the end of operations, you can also just mask a skeleton and let it stay that way. In that case, use the nrn.apply_mask function.\n\n\n\n\n\n\nWarning\n\n\n\nDo not use the nrn.mesh.apply_mask or nrn.skeleton.apply_mask functions, which will not synchronize the mask across the mesh, skeleton, and annotations."
  },
  {
    "objectID": "quickstart_notebooks/00_skeletons.html#annotations",
    "href": "quickstart_notebooks/00_skeletons.html#annotations",
    "title": "Skeletons",
    "section": "Annotations",
    "text": "Annotations\nnrn.anno has set of annotation tables containing some additional information for analysis. Each annotation table has both a Pandas DataFrame object storing data and additional information that allow the rows of the DataFrame to be mapped to mesh and skeleton vertices. For the neurons that have been pre-computed, there is a consisent set of annotation tables:\n\npost_syn: Postsynaptic sites (inputs) for the cell\npre_syn: Presynaptic sites (outputs) for the cell\nis_axon: List of vertices that have been labeled as part of the axon\nlvl2_ids: Gives the PCG level 2 id for each mesh vertex, largely for book-keeping reasons.\nsegment_properties: For each vertex, information about the approximate radius, surface area, volume, and length of the segment it is on.\nvol_prop: For every vertex, information about the volume and surface area of the level 2 id it is associated with.\n\nTo access one of the DataFrames, use the name of the table as an attribute of the anno object and then get the .df property. For example, to get the postsynaptic sites, we can do:\n\nfrom meshparty import meshwork\nnrn = meshwork.load_meshwork('data/864691134940133219_skel.h5')\nnrn.anno.post_syn.df.head()\n\nHowever, in addition to these rows, you can also easily get the mesh vertex index or skeleton vertex index that a row corresponds to with nrn.anno.post_syn.mesh_index or nrn.anno.post_syn.skel_index respectively. This seems small, but it allows you to integrate skeleton-like measurements with annotations trivially.\nFor example, to get the distance from the cell body for each postsynaptic site, we can do:\n\nnrn.distance_to_root(nrn.anno.post_syn.mesh_index)\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the nrn.distance_to_root, like all basic meshwork object functions, expects a mesh vertex index rather than a skeleton vertex index.\n\n\nA common pattern is to copy a synapse dataframe and then add columns to it. For example, to add the distance to root to the postsynaptic sites, we can do:\n\nsyn_df = nrn.anno.pre_syn.df.copy()\nsyn_df['dist_to_root'] = nrn.distance_to_root(nrn.anno.pre_syn.mesh_index)\n\n\nFiltering Annotations by Skeleton Arbor\nEach annotation table has a ‘filter_query’ method that takes a boolean mesh mask and returns only those rows of the dataframe associated with those particular locations on the mesh.\nLet’s use what we learned above in two examples: first, getting all input synapses within 50 microns of the root and second, getting all input synapses on one particular branch off of the soma.\n\ndtr = nrn.distance_to_root() / 1_000   # Convert from nanometers to microns\nnrn.anno.post_syn.filter_query( dtr &lt; 50).df\n\nWe can also use one set of annotations as an input to filter query from another set of annotations. For example, due to errors in segmentation or mistakes in synapse detection, there can be synaptic outputs on the dendrite. However, if we have an is_axon annotation that simply contains a collection of vertices that correspond to the cell’s axon.\nWe can use this annotation to create a mask and filter out all of the synapses that are not on the axon.\n\naxon_mask = nrn.anno.is_axon.mesh_mask\nnrn.anno.pre_syn.filter_query(~axon_mask).df # The \"~\" is a logical not operation that flips True and False\n\nAs a sanity check, we can use nglui to see if these synapses we have labeled as being on the axon are all where we expect.\n\nimport os\nfrom caveclient import CAVEclient\nfrom nglui.statebuilder.helpers import make_synapse_neuroglancer_link\n\nclient = CAVEclient('minnie65_public')\n\nmake_synapse_neuroglancer_link(\n    nrn.anno.pre_syn.filter_query(axon_mask).df,\n    client,\n    return_as=\"html\"\n)\n\n(Click one of the synapse annotations to load the neuron mesh).\nAnother common example might be to pick one of the child nodes of the soma and get all of the synapses on that branch. We can do this by using the nrn.skeleton.get_child_nodes function to get the skeleton vertex indices of the child nodes and then use that to filter the synapses.\n\nbranch_index = 0 # Let's just use the first child vertex of the root node, which is at the soma by default.\nbranch_inds = nrn.downstream_of(nrn.child_index(nrn.root)[branch_index])\nbranch_mask = branch_inds.to_mesh_mask\n\nmake_synapse_neuroglancer_link(\n    nrn.anno.post_syn.filter_query(branch_mask).df,\n    client,\n    return_as=\"html\"\n)"
  },
  {
    "objectID": "python-tools.html",
    "href": "python-tools.html",
    "title": "Python Tools",
    "section": "",
    "text": "Connectome Annotation Versioning Engine (CAVE) a computational infrastructure developed to manage, proofread, and analyze large connetomic datasets efficiently and collaboratively\n\n\nThe MICrONS dataset, spanning one cubic millimeter of mouse cortex, is comprised of petabytes of data from 1) in vivo functional imaging, 2) electron microscopy images at nanometer resolution, 3) dense segmentation of 200,000 brain cells, and 4) half a billion synapses, and 5) hundreds of thousands of other annotations to label the data.\nTo enable systematic analysis without downloading hundreds of gigabytes of data, users can selectively access cloud-based data programmatically through a collection of open source Python clients:\n\nData Access Tools, as of 2025\n\n\n\n\n\n\n\nTool Name\nData Products\nURLs\n\n\n\n\ncloud-volume\nEM imagery, voxel segmentation, and meshes\nhttps://github.com/seung-lab/cloud-volume\n\n\nMeshParty\nEM meshes and skeletons\nhttps://github.com/CAVEconnectome/MeshParty\n\n\nCAVEclient\nEM structural annotations (synapses, cell-types, etc.) segmentation edits, local geometric properties\nhttps://github.com/CAVEconnectome/CAVEclient\n\n\nmicrons-nda\nFunctional imaging DataJoint database\nhttps://github.com/cajal/microns-nda-access\n\n\n\nThe functional data, including calcium traces, stimuli, behavioural measures and more, are available in a DataJoint database that can be accessed using DataJoint’s Python API, or is available as neurodata without borders (NWB) files on the Distributed Archives for Neurophysiology Data Integration (DANDI) Archive.\nEM imagery and segmentation volumes are hosted on Google or AWS servers, and accessed with the python client cloud-volume. Mesh files describing the shape of cells can be downloaded with cloud-volume, which also provides features for convenient mesh analysis, skeletonization and visualization.\nThese meshes can be decomposed and richly annotated for automated proofreading and morphological analysis of processes and spines using NEURD. For more, see reference: (Celii et al. 2025)\nAnnotations on the structural data, such as synapses and cell body locations, can be queried via CAVE client, a Python interface to the Connectome Annotation Versioning Engine (CAVE) APIs. CAVE encompasses a set of microservices for collaborative proofreading and analysis of large-scale volumetric data. For more, see reference: (Dorkenwald et al. 2025)\n\nThe Quickstart Notebooks in this section provide an introduction to CAVE, and examples of how to interact with CAVEclient, MeshParty, and cloud-volume.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to topReferences\n\nCelii, Brendan, Stelios Papadopoulos, Zhuokun Ding, Paul G. Fahey, Eric Wang, Christos Papadopoulos, Alexander B. Kunin, et al. 2025. “NEURD Offers Automated Proofreading and Feature Extraction for Connectomics.” Nature 640 (8058): 487–96. https://doi.org/10.1038/s41586-025-08660-5.\n\n\nDorkenwald, Sven, Casey M. Schneider-Mizell, Derrick Brittain, Akhilesh Halageri, Chris Jordan, Nico Kemnitz, Manual A. Castro, et al. 2025. “CAVE: Connectome Annotation Versioning Engine.” Nature Methods, April. https://doi.org/10.1038/s41592-024-02426-z.",
    "crumbs": [
      "Introduction",
      "Python Tools"
    ]
  },
  {
    "objectID": "programmatic_access/ultrastructure_tutorial/em_ultrastructure.html",
    "href": "programmatic_access/ultrastructure_tutorial/em_ultrastructure.html",
    "title": "Ultrastructure",
    "section": "",
    "text": "Under construction"
  },
  {
    "objectID": "programmatic_access/ultrastructure_tutorial/em_ultrastructure.html#ultrastructure-resources",
    "href": "programmatic_access/ultrastructure_tutorial/em_ultrastructure.html#ultrastructure-resources",
    "title": "Ultrastructure",
    "section": "Ultrastructure Resources",
    "text": "Ultrastructure Resources\nElectron microscopy allows one to see very high resolution features called the “ultrastructure” of the cell, such as organelles and microtubules. However, when first seeing EM images, it can be difficult to know the biological meaning of what you are looking at.\nHere are a couple of resources with several examples of particular structures imaged with EM:\n\nSynapseWeb. This website from the Harris lab at UT Austin offers a number of annotated examples of EM images, in particular the Atlas of Ultrastructural Neurocytology.\nBrain Ultrastructure: Putting the Pieces Together. This short paper has some particularly clear pictures of less common structures like neurites undergoing autophagy.\n\nHowever, note that EM imagery can look very different when imaged with different imaging techniques, sectioning methods or staining protocols, and thus the exact appearance can vary between datasets. In addition, the same structure imaged in different planes can look different, particularly in anisotropic approaches like serial section EM where the x/y resolution is much finer than the z resolution."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "The Machine Intelligence from Cortical Networks (MICrONS) program aimed to close the performance gap between human analysts and automated pattern recognition systems by reverse-engineering the algorithms of the brain. This program mapped the function and connectivity of cortical circuits, using high throughput imaging technologies, with the goal of providing insights into the computational principles that underlie cortical function in order to advance the next generation of machine learning algorithms.\nMICrONS assembled the largest (multi-petabyte) extant dataset of co-registered neurophysiological and neuroanatomical data from the mammalian brain, spanning 1 mm3 and encompassing 100,000 neurons. These data include large scale electron microscopy based reconstructions of cortical circuitry from mouse visual cortex, with corresponding functional imaging data from those same neurons.\nThis multi-year project was conducted by a consortium of laboratories, led by groups at the Allen Institute for Brain Science, Princeton University, and Baylor College of Medicine, with support from a broad array of teams coordinated and funded by the IARPA MICrONS program"
  },
  {
    "objectID": "introduction.html#electron-microscopy-approach-to-connectomics",
    "href": "introduction.html#electron-microscopy-approach-to-connectomics",
    "title": "Introduction",
    "section": "Electron Microscopy approach to Connectomics",
    "text": "Electron Microscopy approach to Connectomics\nThe function of the nervous system arises out of a combination of the properties of individual neurons and the properties of how they are connected into a larger network. The central goal of connectomics is to produce complete maps of the connectivity of the nervous system with synaptic resolution and analyze them to better understand the organization, development, and function of the nervous system. Electron microscopy (EM) has been a central tool in achieving these aims for two key reasons:\n\nEM can easily image nanometer-scale structures such as synapses. This allows one to unambiguously observe chemical synapses and, if pushed to high enough resolution, gap junctions. (Note that the MICrONs dataset is not high enough resolution to unambiguously observe gap junctions).\nDense staining methods allow the complete observation of a of rich collection of cellular features, including membranes, nuclei, mitochondria, and much more. This allows the reconstruction of the complete morphology of cells."
  },
  {
    "objectID": "introduction.html#what-questions-can-be-addressed-by-connectomic-data",
    "href": "introduction.html#what-questions-can-be-addressed-by-connectomic-data",
    "title": "Introduction",
    "section": "What questions can be addressed by connectomic data?",
    "text": "What questions can be addressed by connectomic data?\nFor invertebrates like the fruit fly, the nervous system is highly stereotyped in both the composition of cells and how they connect to one another. Indeed, in the fly many cell types exist as a single pair of cells, one from the left and one from the right hemisphere. Connectomic maps from one individual can thus act as a nearly universal map of the brain and powerfully inform theoretical and experimental studies.\nIn mouse cortex, this same type of stereotypy does not apply. There is no one-to-one match between any given cell in the brain of one mouse and another, and thousands of cells in a cortical area can belong to the same cell type. Because of this, the types of questions one can ask differ, but dense reconstruction of anatomy and connectivity still can be a powerful source of insight into questions like:\n\nWhat morphological or connectivity-based cell types can be found?\nWhat are the properties of the excitatory networks in cortex and how might they relate to learning rules?\nWhat rules dictate how inhibitory cells distribute their synaptic output across target cell types?\n\nIn some datasets, such as the MICrONs dataset, calcium imaging of the same tissue was performed prior to preparing the structural EM volume. In that case, we can address additional questions like:\n\nHow do functional responses relate to the morphology of cells?\nHow do functional responses relate to the connectivity of cells?\n\nIn addition, a whole host of non-neuronal cells are also present, such as astrocytes, oligodendrocytes, microglia, and pericytes. Many people are interested in studying the relationship between these cells and neurons, and connectomic datasets can be a powerful tool to discovery this structural context."
  },
  {
    "objectID": "introduction.html#the-microns-cubic-millimeter",
    "href": "introduction.html#the-microns-cubic-millimeter",
    "title": "Introduction",
    "section": "The MICrONS Cubic Millimeter",
    "text": "The MICrONS Cubic Millimeter\n\n\n\n\n\n\nNote\n\n\n\nFor more complete details about the generation of the volume, please see “Functional connectomics spanning multiple areas of mouse visual cortex” (The MICrONS Consortium et al. 2025).\n\n\nThe MICrONs Cubic Millimeter dataset is an EM volume of mouse visual cortex that spans all cortical layers, and extends approximately 1 mm in width by 0.5 mm in depth, collected as a collaboration between the Allen Institute for Brain Science, Baylor College of Medicine, and Princeton University. The goal of this dataset was to produce a so-called “functional connectomics” volume, where the same population of neurons could be measured with both calcium imaging to record functional properties and EM to describe synapse-resolution structural properties.\nTo link structure and function, excitatory neurons were genetically labeled with GCaMPs and imaged under a battery of natural movies and parametric stimuli by the Tolias lab, then at Baylor College of Medicine. The mouse was shipped to the Allen Institute and the same cortical region was prepared for EM, with more than 25,000 sections cut from the block and imaged across a small fleet of electron microscopes. In order to capture the interactions between different visual regions, the volume location is at the edge of primary visual cortex (VISp) and extends into higher visual areas VISal and VISrl.\nImagery was then aligned and segmented by the team of Sebastian Seung at Princeton, who also automatically detected synapses and nuclei. A team of proofreaders at Johns Hopkins Applied Physics Laboratory helped correct the largest types of errors in the segmentation, leaving virtually every neuron as a distinct object, and coregister tens of thousands of neurons to the functional data. Further analysis at the Allen Institute and elsewhere has focused on curating a map of cell types across the entire dataset, and generating a database to link synapses, cell types, and more to the morphology of individual cells.\nThe result is a cubic millimeter scale dataset with a rich collection of information and the potential for a range of studies, with strengths and limitations due to the current state of proofreading and needs of the functional data collection."
  },
  {
    "objectID": "examples/skeleton_load_and_generate.html",
    "href": "examples/skeleton_load_and_generate.html",
    "title": "Neuron Skeletons",
    "section": "",
    "text": "Created 7/2/2024 by Bethanny Danskin\nMICrONS Tutorial for VORTEX\nDepends on the following two packages: CAVEclient and Skeleton-Plot (version&gt;0.0.9)\nSkeletons and meshworks are loaded as MeshParty meshwork object\npcg-skel and cloudvolume are necessary if generating your own skeletons from the meshes\n\nUsing CAVEclient requires having set up a CAVE auth token. See how to set up your CAVEclient token here.\n\n\n# !pip install caveclient\n# !pip install skeleton-plot\n\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n\nfrom caveclient import CAVEclient\n# Initialize a client for the \"minnie65_public\" datastack.\nclient = CAVEclient(datastack_name='minnie65_public') \n\n# see the available materialization versions\nclient.materialize.get_versions()\n\n# Uncomment to get more details on each release version\n# client.materialize.get_versions_metadata()\n\n[1078, 117, 661, 343, 795, 943]\n\n\n\n# set materialization version, for consistency\nmaterialization = 1078 # current public as of 6/5/2024\n# materialization = 661 # version at which skeletons were pre-generated\nclient.version = materialization\n\n\nimport skeleton_plot as skelplot\nimport skeleton_plot.skel_io as skel_io\n\n\n\nThe skeletons of the meshes are calculated at specific timepoints. The last collection of all neurons in the dataset was at materialization version 661.\nBoth the .swc skeletons and .h5 meshwork objects are available in the BossDB repository\n\n# path to the skeleton .swc files\nskel_path = \"s3://bossdb-open-data/iarpa_microns/minnie/minnie65/skeletons/v661/skeletons/\"\n\n# path to the skeleton and meshwork .h5 files\nmesh_path = \"s3://bossdb-open-data/iarpa_microns/minnie/minnie65/skeletons/v661/meshworks/\"\n\n\n\n\n\n# Skeleton\nnucleus_id = 292685\nsegment_id = 864691135122603047\nskel_filename = f\"{segment_id}_{nucleus_id}.swc\"\n\n# load the .swc skeleton\nsk = skel_io.read_skeleton(skel_path, skel_filename)\n\n\n# load the meshwork (may take minutes to locate, if using public credentials)\nmesh_filename = f\"{segment_id}_{nucleus_id}.h5\"\nmw = skel_io.load_mw(mesh_path, mesh_filename)\n\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_skel(\n    sk,\n    title=nucleus_id,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\n\n\n\nThe meshwork object (h5 file) includes additional information about the input and output synapse positions along the skeleton\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_mw_skel(\n    mw, \n    title=nucleus_id,\n    pull_radius = True,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n    plot_presyn = True,\n    plot_postsyn = True,)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s say you have observed a cell in neuroglancer that you want to work with, like this pyramidal cell (https://neuroglancer.neuvue.io/?json_url=https://global.daf-apis.com/nglstate/api/v1/6320379988541440)\n\nroot_id_v1078 = 864691135356151759\n\nThe segment root_id may change with time, but the 6-digit nucleus id is much more static. Look up the nucleus id from the current root id\n\nnuc_df = client.materialize.tables.nucleus_detection_v0(pt_root_id=root_id_v1078).query()\nnucleus_id = nuc_df.id.item()\nprint(nucleus_id)\n\n490689\n\n\nNow use the nucleus id to look up the previous segmentation root id for v661\n(this is necessary to find the exact filename match. But alternately, you can use directory search to find the matching filenames with just the nucleus id)\n\nsegment_id = client.materialize.tables.nucleus_detection_v0(id=nucleus_id).query(\n    materialization_version=661).pt_root_id.item()\nsegment_id\n\n864691135851524807\n\n\n\n# Load Skeleton\nskel_filename = f\"{segment_id}_{nucleus_id}.swc\"\n\nsk = skel_io.read_skeleton(skel_path, skel_filename)\n\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_skel(\n    sk,\n    title=nucleus_id,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\nNote that this skeleton has no axon compartment. That is because the axon has only been proofread after the skeleton was created. Non-proofread cells do not have their axon statistics generated.\n\n\n\n\nCells that have undergone proofreading in the dataset are annotated in the proofreading_status_and_strategy table. For more information, see the MICrONS-Explorer proofreading documentation\n\n# Load cells that have undergone proofreading\nproof_df = client.materialize.tables.proofreading_status_and_strategy(status_axon=True).query(\n    select_columns = ['pt_root_id','pt_position','status_dendrite','status_axon','strategy_dendrite','strategy_axon'],\n    materialization_version=1078)\n\n# This gives you the list of cells with axonal proofreading, anywhere in the dataset\nproof_df.head()\n\nTable Owner Notice on proofreading_status_and_strategy: NOTE: this table supercedes 'proofreading_status_public_release'. For more details, see: www.microns-explorer.org/manifests/mm3-proofreading.\n\n\n\n\n\n\n\n\n\npt_root_id\nstatus_dendrite\nstatus_axon\nstrategy_dendrite\nstrategy_axon\npt_position\n\n\n\n\n0\n864691135617152361\nt\nt\ndendrite_extended\naxon_partially_extended\n[185152, 185344, 21255]\n\n\n1\n864691136090326071\nt\nt\ndendrite_extended\naxon_fully_extended\n[192080, 190064, 22297]\n\n\n2\n864691135082864887\nt\nt\ndendrite_extended\naxon_interareal\n[303659, 166262, 17349]\n\n\n3\n864691136195284556\nt\nt\ndendrite_extended\naxon_fully_extended\n[173184, 217472, 21929]\n\n\n4\n864691135565870679\nt\nt\ndendrite_clean\naxon_fully_extended\n[184384, 108896, 21755]\n\n\n\n\n\n\n\nNote: for v661, the current preferred proofreading table did not exist. Use proofreading_status_public_release instead\n\n# Load cells that have underwent proofreading for v661\nproof_df = client.materialize.tables.proofreading_status_public_release().query(\n    select_columns = ['pt_root_id','pt_position','status_dendrite','status_axon'],\n    materialization_version=661)\n\n# This gives you the list of cells with axonal proofreading, anywhere in the dataset\nproof_df.head()\n\nTable Owner Notice on proofreading_status_public_release: NOTE: this table is deprecated and no longer receiving updates; please use 'proofreading_status_and_strategy' which is available in datastack version &gt;= 1078 (datastack = minnie65_public or minnie65_phase3_v1).\n\n\n\n\n\n\n\n\n\npt_root_id\nstatus_dendrite\nstatus_axon\npt_position\n\n\n\n\n0\n864691134884807418\nextended\nextended\n[299067, 123129, 22993]\n\n\n1\n864691134885430010\nextended\nnon\n[181280, 223040, 21399]\n\n\n2\n864691134885645050\nextended\nnon\n[172288, 222528, 21607]\n\n\n3\n864691134918370314\nclean\nclean\n[170528, 226848, 20316]\n\n\n4\n864691134918461194\nclean\nclean\n[189760, 127520, 20540]\n\n\n\n\n\n\n\n\n\n\nThis will let you create skeletons for more recently proofread cells that do not exist in the publicly pregenerated .swc files. However, these skeletons will come without apical dendrite labels\npcg-skel documentation\n\n# !pip install pcg-skel\n# !pip install cloud-volume\n\n\nimport cloudvolume\nimport pcg_skel\n\n# specify the materialization version\nclient.materialize.version = 1078\n\n# initialize cloudvolume client\ncv_minnie = cloudvolume.CloudVolume(client.info.segmentation_source(), use_https=True)\n\n\n# pcg-skel pipeline code\ninput_id = 864691135639556411\nid_is_nuc = False\n\n# Cell identification meta\nsynapse_table = client.info.get_datastack_info()['synapse_table']\nif id_is_nuc:\n    id_col = 'id'\nelse:\n    id_col = 'pt_root_id'\nuse_view = True\nif use_view:\n    row = client.materialize.query_view('nucleus_detection_lookup_v1', \n                filter_equal_dict = {id_col: input_id})\nelse:\n    row = client.materialize.query_table('nucleus_detection_v0', \n                filter_equal_dict = {id_col: input_id})\n    \n    row = row.drop('created', axis = 1)\n\nnuc_id = int(row['id']) \nroot_id = int(row['pt_root_id'])\nroot_point = row['pt_position'].values[0]\n\nprint(f'starting on body {nuc_id}, {root_id}')\n\n# create whole neuron with radius info\nresample_spacing = 1510\ncollapse_soma = True\ncollapse_radius = 10_000\nres = [4, 4, 40]\nnrn =  pcg_skel.coord_space_meshwork(root_id,\n                                     client=client,\n                                     root_point=root_point,\n                                     root_point_resolution=res,\n                                     collapse_soma=collapse_soma,\n                                     collapse_radius=collapse_radius,\n                                     synapses='all',\n                                     synapse_table=synapse_table,\n                                     cv = cv_minnie)\n\n# add radius properties df to annotations \npcg_skel.features.add_volumetric_properties(nrn, client)\nprint('adding segment properties')\npcg_skel.features.add_segment_properties(nrn)\nprint('segment properties added')\n\nstarting on body 267207, 864691135639556411\nadding segment properties\nsegment properties added\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\n\nskelplot.plot_tools.plot_verts(nrn.skeleton.vertices, \n                               nrn.skeleton.edges, \n                               nrn.skeleton.radius,\n                               plot_soma=True,\n                               soma_node=nrn.skeleton.root,\n                               x=\"z\",\n                               y=\"y\")\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\nplt.gca().invert_yaxis()\n\n\n\n\n\n\n\n\n\n\n\n# create compartment label\ncompartment_labels = np.zeros(len(nrn.skeleton.vertices)).astype(int)\n\n# add soma label\ncompartment_labels[int(nrn.skeleton.root)]=1\n\n# get the mesh volume properties\nvolume_df = nrn.anno.segment_properties.df\n\n# add column indicating skel index \nvolume_df['skel_index'] = nrn.anno.segment_properties.mesh_index.to_skel_index_padded\nsk_volume_df = volume_df.drop_duplicates('skel_index').sort_values('skel_index').reset_index()\n\n# set map for skel index -&gt; radius\nradius_labels = np.array(sk_volume_df['r_eff']) / 1000\n\n# metadata dictionary for keeping vertex variables together\nskeleton_properties = {}\nskeleton_properties['compartment'] = compartment_labels\nskeleton_properties['vertices'] = nrn.skeleton.vertices\nskeleton_properties['edges'] = nrn.skeleton.edges\nskeleton_properties['radius'] = radius_labels\n\n\nnrn.skeleton.export_to_swc(filename = 'test.swc', \n                           node_labels=compartment_labels, \n                           radius=radius_labels,\n                           avoid_root=True, \n                           resample_spacing = resample_spacing)"
  },
  {
    "objectID": "examples/skeleton_load_and_generate.html#download-visualize-and-generate-neuron-skeletons",
    "href": "examples/skeleton_load_and_generate.html#download-visualize-and-generate-neuron-skeletons",
    "title": "Neuron Skeletons",
    "section": "",
    "text": "Created 7/2/2024 by Bethanny Danskin\nMICrONS Tutorial for VORTEX\nDepends on the following two packages: CAVEclient and Skeleton-Plot (version&gt;0.0.9)\nSkeletons and meshworks are loaded as MeshParty meshwork object\npcg-skel and cloudvolume are necessary if generating your own skeletons from the meshes\n\nUsing CAVEclient requires having set up a CAVE auth token. See how to set up your CAVEclient token here.\n\n\n# !pip install caveclient\n# !pip install skeleton-plot\n\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n\nfrom caveclient import CAVEclient\n# Initialize a client for the \"minnie65_public\" datastack.\nclient = CAVEclient(datastack_name='minnie65_public') \n\n# see the available materialization versions\nclient.materialize.get_versions()\n\n# Uncomment to get more details on each release version\n# client.materialize.get_versions_metadata()\n\n[1078, 117, 661, 343, 795, 943]\n\n\n\n# set materialization version, for consistency\nmaterialization = 1078 # current public as of 6/5/2024\n# materialization = 661 # version at which skeletons were pre-generated\nclient.version = materialization\n\n\nimport skeleton_plot as skelplot\nimport skeleton_plot.skel_io as skel_io\n\n\n\nThe skeletons of the meshes are calculated at specific timepoints. The last collection of all neurons in the dataset was at materialization version 661.\nBoth the .swc skeletons and .h5 meshwork objects are available in the BossDB repository\n\n# path to the skeleton .swc files\nskel_path = \"s3://bossdb-open-data/iarpa_microns/minnie/minnie65/skeletons/v661/skeletons/\"\n\n# path to the skeleton and meshwork .h5 files\nmesh_path = \"s3://bossdb-open-data/iarpa_microns/minnie/minnie65/skeletons/v661/meshworks/\"\n\n\n\n\n\n# Skeleton\nnucleus_id = 292685\nsegment_id = 864691135122603047\nskel_filename = f\"{segment_id}_{nucleus_id}.swc\"\n\n# load the .swc skeleton\nsk = skel_io.read_skeleton(skel_path, skel_filename)\n\n\n# load the meshwork (may take minutes to locate, if using public credentials)\nmesh_filename = f\"{segment_id}_{nucleus_id}.h5\"\nmw = skel_io.load_mw(mesh_path, mesh_filename)\n\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_skel(\n    sk,\n    title=nucleus_id,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\n\n\n\nThe meshwork object (h5 file) includes additional information about the input and output synapse positions along the skeleton\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_mw_skel(\n    mw, \n    title=nucleus_id,\n    pull_radius = True,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n    plot_presyn = True,\n    plot_postsyn = True,)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s say you have observed a cell in neuroglancer that you want to work with, like this pyramidal cell (https://neuroglancer.neuvue.io/?json_url=https://global.daf-apis.com/nglstate/api/v1/6320379988541440)\n\nroot_id_v1078 = 864691135356151759\n\nThe segment root_id may change with time, but the 6-digit nucleus id is much more static. Look up the nucleus id from the current root id\n\nnuc_df = client.materialize.tables.nucleus_detection_v0(pt_root_id=root_id_v1078).query()\nnucleus_id = nuc_df.id.item()\nprint(nucleus_id)\n\n490689\n\n\nNow use the nucleus id to look up the previous segmentation root id for v661\n(this is necessary to find the exact filename match. But alternately, you can use directory search to find the matching filenames with just the nucleus id)\n\nsegment_id = client.materialize.tables.nucleus_detection_v0(id=nucleus_id).query(\n    materialization_version=661).pt_root_id.item()\nsegment_id\n\n864691135851524807\n\n\n\n# Load Skeleton\nskel_filename = f\"{segment_id}_{nucleus_id}.swc\"\n\nsk = skel_io.read_skeleton(skel_path, skel_filename)\n\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_skel(\n    sk,\n    title=nucleus_id,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\nNote that this skeleton has no axon compartment. That is because the axon has only been proofread after the skeleton was created. Non-proofread cells do not have their axon statistics generated.\n\n\n\n\nCells that have undergone proofreading in the dataset are annotated in the proofreading_status_and_strategy table. For more information, see the MICrONS-Explorer proofreading documentation\n\n# Load cells that have undergone proofreading\nproof_df = client.materialize.tables.proofreading_status_and_strategy(status_axon=True).query(\n    select_columns = ['pt_root_id','pt_position','status_dendrite','status_axon','strategy_dendrite','strategy_axon'],\n    materialization_version=1078)\n\n# This gives you the list of cells with axonal proofreading, anywhere in the dataset\nproof_df.head()\n\nTable Owner Notice on proofreading_status_and_strategy: NOTE: this table supercedes 'proofreading_status_public_release'. For more details, see: www.microns-explorer.org/manifests/mm3-proofreading.\n\n\n\n\n\n\n\n\n\npt_root_id\nstatus_dendrite\nstatus_axon\nstrategy_dendrite\nstrategy_axon\npt_position\n\n\n\n\n0\n864691135617152361\nt\nt\ndendrite_extended\naxon_partially_extended\n[185152, 185344, 21255]\n\n\n1\n864691136090326071\nt\nt\ndendrite_extended\naxon_fully_extended\n[192080, 190064, 22297]\n\n\n2\n864691135082864887\nt\nt\ndendrite_extended\naxon_interareal\n[303659, 166262, 17349]\n\n\n3\n864691136195284556\nt\nt\ndendrite_extended\naxon_fully_extended\n[173184, 217472, 21929]\n\n\n4\n864691135565870679\nt\nt\ndendrite_clean\naxon_fully_extended\n[184384, 108896, 21755]\n\n\n\n\n\n\n\nNote: for v661, the current preferred proofreading table did not exist. Use proofreading_status_public_release instead\n\n# Load cells that have underwent proofreading for v661\nproof_df = client.materialize.tables.proofreading_status_public_release().query(\n    select_columns = ['pt_root_id','pt_position','status_dendrite','status_axon'],\n    materialization_version=661)\n\n# This gives you the list of cells with axonal proofreading, anywhere in the dataset\nproof_df.head()\n\nTable Owner Notice on proofreading_status_public_release: NOTE: this table is deprecated and no longer receiving updates; please use 'proofreading_status_and_strategy' which is available in datastack version &gt;= 1078 (datastack = minnie65_public or minnie65_phase3_v1).\n\n\n\n\n\n\n\n\n\npt_root_id\nstatus_dendrite\nstatus_axon\npt_position\n\n\n\n\n0\n864691134884807418\nextended\nextended\n[299067, 123129, 22993]\n\n\n1\n864691134885430010\nextended\nnon\n[181280, 223040, 21399]\n\n\n2\n864691134885645050\nextended\nnon\n[172288, 222528, 21607]\n\n\n3\n864691134918370314\nclean\nclean\n[170528, 226848, 20316]\n\n\n4\n864691134918461194\nclean\nclean\n[189760, 127520, 20540]\n\n\n\n\n\n\n\n\n\n\nThis will let you create skeletons for more recently proofread cells that do not exist in the publicly pregenerated .swc files. However, these skeletons will come without apical dendrite labels\npcg-skel documentation\n\n# !pip install pcg-skel\n# !pip install cloud-volume\n\n\nimport cloudvolume\nimport pcg_skel\n\n# specify the materialization version\nclient.materialize.version = 1078\n\n# initialize cloudvolume client\ncv_minnie = cloudvolume.CloudVolume(client.info.segmentation_source(), use_https=True)\n\n\n# pcg-skel pipeline code\ninput_id = 864691135639556411\nid_is_nuc = False\n\n# Cell identification meta\nsynapse_table = client.info.get_datastack_info()['synapse_table']\nif id_is_nuc:\n    id_col = 'id'\nelse:\n    id_col = 'pt_root_id'\nuse_view = True\nif use_view:\n    row = client.materialize.query_view('nucleus_detection_lookup_v1', \n                filter_equal_dict = {id_col: input_id})\nelse:\n    row = client.materialize.query_table('nucleus_detection_v0', \n                filter_equal_dict = {id_col: input_id})\n    \n    row = row.drop('created', axis = 1)\n\nnuc_id = int(row['id']) \nroot_id = int(row['pt_root_id'])\nroot_point = row['pt_position'].values[0]\n\nprint(f'starting on body {nuc_id}, {root_id}')\n\n# create whole neuron with radius info\nresample_spacing = 1510\ncollapse_soma = True\ncollapse_radius = 10_000\nres = [4, 4, 40]\nnrn =  pcg_skel.coord_space_meshwork(root_id,\n                                     client=client,\n                                     root_point=root_point,\n                                     root_point_resolution=res,\n                                     collapse_soma=collapse_soma,\n                                     collapse_radius=collapse_radius,\n                                     synapses='all',\n                                     synapse_table=synapse_table,\n                                     cv = cv_minnie)\n\n# add radius properties df to annotations \npcg_skel.features.add_volumetric_properties(nrn, client)\nprint('adding segment properties')\npcg_skel.features.add_segment_properties(nrn)\nprint('segment properties added')\n\nstarting on body 267207, 864691135639556411\nadding segment properties\nsegment properties added\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\n\nskelplot.plot_tools.plot_verts(nrn.skeleton.vertices, \n                               nrn.skeleton.edges, \n                               nrn.skeleton.radius,\n                               plot_soma=True,\n                               soma_node=nrn.skeleton.root,\n                               x=\"z\",\n                               y=\"y\")\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\nplt.gca().invert_yaxis()\n\n\n\n\n\n\n\n\n\n\n\n# create compartment label\ncompartment_labels = np.zeros(len(nrn.skeleton.vertices)).astype(int)\n\n# add soma label\ncompartment_labels[int(nrn.skeleton.root)]=1\n\n# get the mesh volume properties\nvolume_df = nrn.anno.segment_properties.df\n\n# add column indicating skel index \nvolume_df['skel_index'] = nrn.anno.segment_properties.mesh_index.to_skel_index_padded\nsk_volume_df = volume_df.drop_duplicates('skel_index').sort_values('skel_index').reset_index()\n\n# set map for skel index -&gt; radius\nradius_labels = np.array(sk_volume_df['r_eff']) / 1000\n\n# metadata dictionary for keeping vertex variables together\nskeleton_properties = {}\nskeleton_properties['compartment'] = compartment_labels\nskeleton_properties['vertices'] = nrn.skeleton.vertices\nskeleton_properties['edges'] = nrn.skeleton.edges\nskeleton_properties['radius'] = radius_labels\n\n\nnrn.skeleton.export_to_swc(filename = 'test.swc', \n                           node_labels=compartment_labels, \n                           radius=radius_labels,\n                           avoid_root=True, \n                           resample_spacing = resample_spacing)"
  },
  {
    "objectID": "examples/skeleton_load_and_generate.html#optional-generate-myelin-labels-from-myelin-table",
    "href": "examples/skeleton_load_and_generate.html#optional-generate-myelin-labels-from-myelin-table",
    "title": "Neuron Skeletons",
    "section": "Optional: generate myelin labels from myelin table",
    "text": "Optional: generate myelin labels from myelin table\nFor the subset of neurons with manual labeling of myelination, see CAVE table and documentaiton for vortex_manual_myelination_v0\n\n# Skeleton utility functions\nfrom tqdm.notebook import tqdm, trange\ntqdm.pandas()\ndef get_myelin_at_vertex(mw, myelin_df, properties_dict, client, cv):\n    # given a df of all myelinated points on the axon, return the corresponding skeleton labels to the properties dictionary\n\n    # get level2 nodes at myelinated positions\n    myelin_lvl2 = myelin_df.progress_apply(pd_get_level2_point, axis=1, client=client, cv=cv)\n\n    # generate index lookup\n    mw_index_lookup = get_meshwork_index_lookup(mw)\n\n    # merge myelin_lvl2 to index lookup\n    myelin_merge = pd.merge(myelin_lvl2, mw_index_lookup, on='lvl2_id', how='inner')\n    \n    # empty myelin labels\n    try:\n        vertices = properties_dict['vertices']\n    except:\n        print('no vertex properties found; call get_skeleton_features_from_meshwork() first') \n        \n    myelin = np.zeros(len(vertices))\n    \n    # where myelin is present, set to 1\n    myelin[myelin_merge.skel_ind.values] = 1\n\n    properties_dict['myelin'] = myelin\n\n    return properties_dict\n\ndef pd_get_level2_point(row, client, cv, voxel_resolution=[4,4,40]):\n    point, root_id = row[['pt_position','valid_id']]\n\n    try:\n        lvl2_id = pcg_skel.chunk_tools.get_closest_lvl2_chunk(point,\n                                                              root_id,\n                                                              client,\n                                                              voxel_resolution=voxel_resolution,\n                                                              radius=200)\n        row['lvl2_id'] = lvl2_id\n    except:\n        row['lvl2_id'] = np.nan\n        \n    return row\n\ndef get_meshwork_index_lookup(mw):\n    # generate index lookup with lvl2, mesh, and skeleton indices\n    mw_index_lookup = mw.anno.lvl2_ids.df\n    mw_index_lookup['skel_ind'] = mw.mesh_indices.to_skel_index_padded\n    mw_index_lookup.set_index('mesh_ind_filt', inplace=True)\n\n    return mw_index_lookup\n\n\n# get myelin-labeled points (exists for a subset of manually annotated cells)\nmyelin_df = client.materialize.tables.vortex_manual_myelination_v0(valid_id=input_id, tag='t').query(\n    select_columns=['valid_id','tag','pt_position'],\n    materialization_version=1078,\n)\n\nprint(len(myelin_df))\n\nTable Owner Notice on vortex_manual_myelination_v0: Myelination status assessed for the axon of the VALID_ID, not the pt_root_id.\n\n\n197\n\n\n\n# Add myelin info (this takes upwards of 10 minutes to convert points to vertices)\nskeleton_properties = get_myelin_at_vertex(nrn, myelin_df, skeleton_properties, client=client, cv=cv_minnie)\n\n\n\n\n\nprint(skeleton_properties)\n\n{'compartment': array([0, 0, 0, ..., 0, 0, 1]), 'vertices': array([[ 395328.,  547312.,  924200.],\n       [ 396992.,  546184.,  923360.],\n       [ 397712.,  545840.,  922760.],\n       ...,\n       [1313160.,  846424.,  797800.],\n       [1314944.,  847008.,  792760.],\n       [ 717824.,  771840.,  863120.]]), 'edges': array([[ 842,  866],\n       [ 866,  867],\n       [ 867,  868],\n       ...,\n       [4602, 4595],\n       [4595, 4596],\n       [4596, 4597]], dtype=int64), 'radius': array([0.12334736, 0.12334736, 0.12334736, ..., 0.13528796, 0.13528796,\n       5.54981555]), 'myelin': array([0., 0., 0., ..., 0., 0., 0.])}"
  },
  {
    "objectID": "examples/skeleton_load_and_generate.html#addendum-useful-chunkedgraph-functions-getting-segments-across-time",
    "href": "examples/skeleton_load_and_generate.html#addendum-useful-chunkedgraph-functions-getting-segments-across-time",
    "title": "Neuron Skeletons",
    "section": "Addendum: useful chunkedgraph functions (getting segments across time)",
    "text": "Addendum: useful chunkedgraph functions (getting segments across time)\nBecause the segment id may have changed since the skeletons were calculated, find the past segment id of this object\nChunkedgraph documentation and functions\n\nclient.materialize.get_versions()\n\n[1078, 117, 661, 343, 795, 943]\n\n\n\n# Get info on version 661\nclient.materialize.get_versions_metadata()[2]\n\n{'datastack': 'minnie65_public',\n 'is_merged': False,\n 'status': 'AVAILABLE',\n 'version': 661,\n 'id': 688,\n 'valid': True,\n 'time_stamp': datetime.datetime(2023, 4, 6, 20, 17, 9, 199182, tzinfo=datetime.timezone.utc),\n 'expires_on': datetime.datetime(2125, 3, 27, 19, 17, 9, 199182, tzinfo=datetime.timezone.utc)}\n\n\n\ntimestamp_v661 = client.materialize.get_versions_metadata()[2]['time_stamp']\ntimestamp_latest = client.materialize.get_versions_metadata()[0]['time_stamp']\n\n\nclient.chunkedgraph.suggest_latest_roots(segment_id, timestamp=timestamp_latest)\n\n864691135356151759\n\n\n\nclient.chunkedgraph.get_root_timestamps(root_id_v1078)\n\narray([datetime.datetime(2024, 4, 10, 4, 7, 54, 947000, tzinfo=&lt;UTC&gt;)],\n      dtype=object)\n\n\n\nclient.chunkedgraph.is_latest_roots(864691135808631069)\n\narray([ True])"
  },
  {
    "objectID": "dash-connectivity.html",
    "href": "dash-connectivity.html",
    "title": "Dash Apps: Connectivity Viewer",
    "section": "",
    "text": "Version update\n\n\n\nThis tutorial has recently been updated to materialization version 1412 (from 1300).\nWe have released a new public version 1412, as part of our quarterly release schedule. See details at Release Manifests: 1412.\nWe have created two webapps to help explore and visualize the data. These apps were created using the Dash framework, hence the name.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Dash Apps: Connectivity Viewer"
    ]
  },
  {
    "objectID": "dash-connectivity.html#connectivity-and-cell-type-viewer",
    "href": "dash-connectivity.html#connectivity-and-cell-type-viewer",
    "title": "Dash Apps: Connectivity Viewer",
    "section": "Connectivity and Cell Type Viewer",
    "text": "Connectivity and Cell Type Viewer\nThe Connectivity Viewer is designed to let you glance at the synaptic output or input of a given cell and group connectivity by cell type or other annotations.\n\n\n\nConnectivity viewer input options.\n\n\nAlong the top, you can enter an Cell ID (either a root ID or a cell ID) and, optionally, a table to use to define cell types for synaptic partners.\n\n\n\n\n\n\nNote\n\n\n\nNote that if you want to use cell IDs from the nucleus detection table, you must select the Nucleus ID option from the dropdown next to the entry box.\n\n\nThe Materialization option allows you to select which version of the dataset to query from. Default is ‘latest’.\nFor the optional Table dropdown, there are several tables to choose from, though the most informative will be one of the cell type classification tables. The aibs_metamodel_celltypes_v661 has the greatest coverage of the dataset, and is described in Perisomatic ultrastructure efficiently classifies cells in mouse cortex” (Elabbady et al. 2025).\nThe table aibs_metamodel_mtypes_v661_v2 is the most current version version of the cell type classification, described in “A connectomic census of mouse visual cortex” (Schneider-Mizell et al. 2025).\nPressing “Submit” will query the database for the cell and its synapses.\nOnce completed, you will see the bar under the input options turn green and report the data that it is displaying. You may get one of several outputs depending on the root id you use.\nFor example, if we query root id 864691135408247241 at version 1412, we will see the following:\n\nMeaning the server looked up the correct root id for the selected materialization version. If you DO want the connectivity for the expired root id, you mus select the materialization version for when it was extant.\nWhen you use a current root id or nucleus id, you will see the following return message\n\nImportantly, the data displayed below will match whatever is in this bar — if your query takes a long time or fails for some reason (i.e.bad internet, server error), the tables displayed might be stale.\nThere are two locations the resulting data is displayed, as we’ll see below.\n\nFirst, a bar of tabs allows you to quickly visualize the data in different ways.\nSecond, the table can present synaptic partners in a more detailed way.\n\n\nVisualization tabs\nThe first and default option in the tabs is Tables Only which doesn’t actually show any data. This is merely a placeholder to let you look directly at the tablular data below.\nThe next tab, Plots lets you visualize the synaptic output of a neuron.\nWe focus on synaptic output only right now, because the data quality is such that automated dendritic reconstructions are reliable, but automated axon reconstructions are not (see Proofreading section). Because of that, if a neuron has been proofread we trust the cell types of its postsynaptic partners as a group but do not trust the cell types of its presynaptic partners.\n\n\n\nPlot view. Left, violin plot of the synaptic distribution of the queried cell. Blue/left is the distribution of the cortical depth of input synapses along, red/right is the distribution of cortical depth of synaptic outputs. Approximate layers are shown. Center, a scatterplot with a dot for each synaptic output of the queried cell that is onto a target with a unique cell body (i.e. no dendritic fragments and no cells erroneously merged into other cells). The y-axis is the depth of the synapse itself, while the x-axis is the depth of the cell body of the target neuron. If a cell body categroy is selected, the dots are colored by the cell type of the target neuron. Right, a bar plot showing of the number of synapses onto target cell types.\n\n\nThese plots are designed to give you a quick overview of a cell’s synaptic outputs.\nTo add a particularly cell type column, use the Color by value: dropdown below the row of plots to select a column to color by.\nThese plots are handled in Plotly, so you can hover over the plots to see more information, and you can click and drag to zoom in on a particular region and save by clicking on the camera icon. For the scatterplot, you can also turn on and off individual types by clicking on the legend.\nThe next tab, Neuroglancer Links lets you visualize the synapses of a neuron in Neuroglancer.\n\n\n\nNeuroglancer links. From left to right: Show all synaptic inputs to the cell, show all synaptic inputs to the cell grouped into different layers by value in the associated table (e.g. cell type), show all synaptic outputs of the cell, show all synaptic outputs of the cell grouped into different layers by value in the associated table (e.g. cell type). Below the dropdown specifies which column to use if grouping annotations. The Include No Type toggle specifies whether to show synapses with cells that have no cell type annotation in the grouped cases.\n\n\nThe Neuroglancer links generated by these buttons will span all synapses of the queried cell, and do not reflect the filtering in the table below. The synapse annotations are also associated with the synaptic partners, making it easy to browse through a broad sampling of synaptic connectivity.\n\n\nConnectivity Table\nThe table below shows a table of all synaptic partners.\n\n\n\nTable view options.\n\n\nOne of the first things you can see in the table viewer is that it is split into two tabs, Input and Output, with the tab names for each showing the total number of synapses in each category.\nTo move between them, just click the tab you want.\nAbove the Input/Ouput tabs is a link to the Neuroglancer view of the synapses show in the table, after filtering and/or selecting a subset of rows.\nYou also have the drop-down option to select which branch of neuroglancer to use: Seung-lab or Spelunker. If the annotations you expect are not rendering, try the other neuroglancer type.\nThe table view here is similar to in the Annotation Table Viewer described above. You can sort the table by clicking column headers, filter the table using the row underneath the headers, and select individual rows. If you select individual rows, the Neuroglancer link will group all synapses from each partner together, making it particularly easy to see how synaptic connectivity relates to neuronal anatomy.\nThe default sorting of the table is by total number of synapses, but note that both summed total synapse size (net_size) and average synapse size (mean_size) are also shown. These can be particularly important values when thinking about connectivity between excitatory neurons.\nA CSV file of the entire table can be downloaded by clicking the “Export to CSV” button.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Dash Apps: Connectivity Viewer"
    ]
  },
  {
    "objectID": "annotation-tables.html",
    "href": "annotation-tables.html",
    "title": "Annotation Tables",
    "section": "",
    "text": "Version update\n\n\n\nThis tutorial has recently been updated to materialization version 1412 (from 1300).\nWe have released a new public version 1412, as part of our quarterly release schedule. See details at Release Manifests: 1412.\nThe minnie65_public data release includes a number of annotation tables that help label the dataset. This section describes the content of each of these tables — see here for instructions for how to query and filter tables.\nUnless otherwise specificied (i.e. via desired_resolution), all positions are in units of 4,4,40 nm/voxel resolution.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "annotation-tables.html#common-fields",
    "href": "annotation-tables.html#common-fields",
    "title": "Annotation Tables",
    "section": "Common Fields",
    "text": "Common Fields\nSeveral fields (or column names) are common to many tables. These fall into two main classes: the spatial point columns that are how we assign annotations to cells via points in the 3d space and book-keeping columns, that are used internally to track the state of the data.\n\nSpatial Point Columns\nMost tables have one or more Bound Spatial Points, which is a location in the 3d space that tells the annotation to remain associated with the root id at that location.\nBound spatial points have will have one prefix, usually pt (i.e. “point”) and three associated columns with different suffixes: _position, _supervoxel_id, and _root_id.\nFor a given prefix {pt}, the three columns are as follows:\n\nThe {pt}_position indicates the location of the point in 3d space.\nThe {pt}_supervoxel_id indicates a unique identifier in the segmentation, and is mostly internal bookkeeping.\nThe {pt}_root_id indicates the root id of the annotation at that location.\n\n\n\nBook-keeping Columns\nSeveral columns are common to many or all tables, and mostly used as internal book-keeping. Rather than describe these for every table, they will just be mentioned briefly here:\n\nCommon columns\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nA unique ID specific to the annotation within that table\n\n\ncreated\nInternal bookkeeping column, should always be t for data you can download\n\n\nvalid\nA unique ID specific to the annotation within that table\n\n\ntarget_id\nSome tables reference other tables, particularly the nucleus table. If present, this column will be the same as id\n\n\ncreated_ref / valid_ref / id_ref (optional)\nFor reference tables, the data shows both the created/valid/id of the reference annotation and the target annotation. The values with the _ref suffix are those of the reference table (usually something like proofreading state or cell type) and the values without a suffix ar ethose of the target table (usually a nucleus)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "annotation-tables.html#synapse-table",
    "href": "annotation-tables.html#synapse-table",
    "title": "Annotation Tables",
    "section": "Synapse Table",
    "text": "Synapse Table\nTable name: synapses_pni_2\nThe only synapse table is synapses_pni_2. This is by far the largest table in the dataset with 337 million entries, one for each synapse. It contains the following columns:\n\nSynapse table column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npre_pt_position / pre_pt_supervoxel_id / pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse.\n\n\npost_pt_position / post_pt_supervoxel_id / post_pt_root_id\nIThe bound spatial point data for the postsynaptic side of the synapse.\n\n\nsize\nThe size of the synapse in voxels. This correlates well, but not perfectly, with the surface area of synapse.\n\n\nctr_pt_position\nA position in the center of the detected synaptic junction. Of all points in the synapse table, this is usually the closest point to the surface (and thus mesh) of both neurons. Because it is at the edge of cells, it is not associated with a root id.\n\n\n\n# Synapse query: outputs\nclient.materialize.synapse_query(pre_ids=example_root_id)\n\n# Synapse query: inputs\nclient.materialize.synapse_query(post_ids=example_root_id)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "annotation-tables.html#nucleus-tables",
    "href": "annotation-tables.html#nucleus-tables",
    "title": "Annotation Tables",
    "section": "Nucleus tables",
    "text": "Nucleus tables\nThe ‘nucleus centroid’ of a cell is unlikely to change with proofreading, and so is a useful static identifier for a given cell. The results of automatic nucleus segmentation and neuron-detection are avialable in the following tables. These tables are often the ‘reference’ table for other annotations.\n\nNucleus Detection Table\nTable name: nucleus_detection_v0\nNucleus detection has been used to define unique cells in the dataset. Distinct from the neuronal segmentation, a convolutional neural network was trained to segment nuclei. Each nucleus detection was given a unique ID, and the centroid of the nucleus was recorded as well as its volume. Many other tables in the dataset are reference tables on nucleus_detection_v0, meaning they are linked by the same annotation id. The id of the segmented nucelus, a 6-digit integer, is static across data versions and for this reason is the preferred method to identify the same ‘cell’ across time.\nThe key columns of nucleus_detection_v0 are:\n\nNucleus table column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\n6-digit number of the segmentation for that nucleus; ‘nucleus ID’\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\n\nNote that the id column is the nucleus ID, also called the ‘soma ID’ or the ‘cell ID’.\n# Standard query\nclient.materialize.query_table('nucleus_detection_v0')\n\n# Content-aware query\nclient.materialize.tables.nucleus_detection_v0(id=example_nucleus_id).query()\n\n\nNucleus brain area assignment\nTable name: nucleus_functional_area_assignment\nGiven the nucleus detection table nucleus_detection_v0 and the transformation of 2-photon in vivo imaging to the EM structural space (see functional-coregistration-tables ), each cell in the volume has been assigned to one of four visual cortical areas.\nThe inferred functional brain area based on the position of the nucleus in the EM volume. Area boundaries estimated from the area-membership assignments of the 2P recorded cells, after transformation to EM space.\nThe table nucleus_functional_area_assignment is a reference table on nucleus_detection_v0 and adds the following columns:\n\nFunctional area assignment\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\ntag\nthe brain area label (one of V1, AL, RL, LM)\n\n\nvalue\nthe distance to the area boundary (in um), calculated as the mean-distance of the 10 nearest neighbors in a non-matching brain area. A larger value is further from the area boundaries, and can be interpretted as higher confidence in area assignment.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('nucleus_functional_area_assignment')\n\n# Content-aware query\nclient.materialize.tables.nucleus_functional_area_assignment(tag='V1').query()\n\n\n\n\n\n\nNeuron-Nucleus Table\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis table is superseded by other cell typing below, but remains here for convenient reference as it is relevant to other tables such as aibs_metamodel_celltypes_v661.\n\n\nTable name: nucleus_ref_neuron_svm\nWhile the table of centroids for all nuclei is nucleus_detection_v0, this includes neuronal nuclei, non-neuronal nuclei, and some erroneous detections. The table nucleus_ref_neuron_svm shows the results of a classifier that was trained to distinguish neuronal nuclei from non-neuronal nuclei and errors. For the purposes of analysis, we recommend using the nucleus_ref_neuron_svm table to get the most broad collection of neurons in the dataset.\nThe key columns of nucleus_ref_neuron_svm are:\n\nNucleus table column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\nclassification-system\nDescribes how the classification was done. All values will be is_neuron for this table\n\n\ncell_type\nThe output of the classifier. All values will be either neuron or not-neuron (glia or error) for this table\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('nucleus_ref_neuron_svm')\n\n# Content-aware query\nclient.materialize.tables.nucleus_ref_neuron_svm(target_id=example_nucleus_id).query()",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "annotation-tables.html#cell-type-tables",
    "href": "annotation-tables.html#cell-type-tables",
    "title": "Annotation Tables",
    "section": "Cell Type Tables",
    "text": "Cell Type Tables\nThere are several tables that contain information about the cell type of neurons in the dataset, with each table representing a different method of doing the classificaiton. Because each method requires a different kind of information, not all cells are present in all tables. Each of the cell types tables has the same format and in all cases the id column references the nucleus id of the cell in question.\n\nManual Cell Types (V1 Column)\nTable name: allen_v1_column_types_slanted_ref\nA subset of nucleus detections in a 100 um column (n=2204) in VISp were manually classified by anatomists at the Allen Institute into categories of cell subclasses, first distinguishing cells into classes of non-neuronal, excitatory and inhibitory; then into subclasses.\nFor the non-neuronal subclasses, see aibs_column_nonneuronal_ref\nThe key columns are:\n\nAIBS Manual Cell Types, V1 Column\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nOne of aibs_coarse_excitatory or aibs_coarse_inhibitory for detected neurons, or aibs_coarse_nonneuronal for non-neurons (glia/pericytes).\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\nThe cell types in the table are:\n\n\n\n\n\n\nManual Cell Types (neurons)\n\n\n\n\n\n\nAIBS Manual Cell Type definitions (neurons)\n\n\n\n\n\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\n23P\nExcitatory\nLayer 2/3 cells\n\n\n4P\nExcitatory\nLayer 4 cells\n\n\n5P-IT\nExcitatory\nLayer 5 intratelencephalic cells\n\n\n5P-ET\nExcitatory\nLayer 5 extratelencephalic cells\n\n\n5P-NP\nExcitatory\nLayer 5 near-projecting cells\n\n\n6P-IT\nExcitatory\nLayer 6 intratelencephalic cells\n\n\n6P-CT\nExcitatory\nLayer 6 corticothalamic cells\n\n\nBC\nInhibitory\nBasket cell\n\n\nBPC\nInhibitory\nBipolar cell. In practice, this was used for all cells thought to be VIP cell, not only those with a bipolar dendrite\n\n\nMC\nInhibitory\nMartinotti cell. In practice, this label was used for all inhibitory neurons that appeared to be Somatostatin cell, not only those with a Martinotti cell morphology\n\n\nUnsure\nInhibitory\nUnsure. In practice, this label also is used for all likely-inhibitory neurons that did not match other types\n\n\n\n\n\n\n# Standard query\nclient.materialize.query_table('allen_v1_column_types_slanted_ref')\n\n# Content-aware query\nclient.materialize.tables.allen_v1_column_types_slanted_ref(id=example_nucleus_id).query()\n\n\n\n\n\n\nManual Cell Types (non-neurons)\n\n\n\n\n\n\nAIBS Manual Cell Type definitions (non-neurons)\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\nOPC\nNon-neuronal\nOligodendrocyte precursor cell\n\n\nastrocyte\nNon-neuronal\nAstrocyte\n\n\nmicroglia\nNon-neuronal\nMicroglia\n\n\npericyte\nNon-neuronal\nPericyte\n\n\noligo\nNon-neuronal\nOligodendrocyte\n\n\n\n\n\n\n# Standard query\nclient.materialize.query_table('aibs_column_nonneuronal_ref')\n\n# Content-aware query\nclient.materialize.tables.aibs_column_nonneuronal_ref(id=example_nucleus_id).query()\n\n\nPredictions from soma/nucleus features\nTable name: aibs_metamodel_celltypes_v661\nThis table contains the results of a hierarchical classifier trained on features of the cell body and nucleus of cells. This was applied to most cells in the dataset that had complete cell bodies (e.g. not cut off by the edge of the data), detected by nucleus_detection_v0, with small-objects and multi-soma errors removed. The model was run with cell-based features as of version 661 of the dataset. For more details, see (Elabbady et al. 2025). In general, this does a good job, but sometimes confuses layer 5 inhibitory neurons as being excitatory:\nThe key columns are:\n\nAIBS Soma Nuc Metamodel Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nOne of excitatory_neuron or inhibitory_neuron for detected neurons, or nonneuron for non-neurons (glia/pericytes).\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\nThe cell types in the table are:\n\n\n\n\n\n\nSoma Nuc Metamodel Cell types\n\n\n\n\n\n\nAIBS Soma Nuc Metamodel: Cell Type definitions\n\n\n\n\n\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\n23P\nExcitatory\nLayer 2/3 cells\n\n\n4P\nExcitatory\nLayer 4 cells\n\n\n5P-IT\nExcitatory\nLayer 5 intratelencephalic cells\n\n\n5P-ET\nExcitatory\nLayer 5 extratelencephalic cells\n\n\n5P-NP\nExcitatory\nLayer 5 near-projecting cells\n\n\n6P-IT\nExcitatory\nLayer 6 intratelencephalic cells\n\n\n6P-CT\nExcitatory\nLayer 6 corticothalamic cells\n\n\nBC\nInhibitory\nBasket cell\n\n\nBPC\nInhibitory\nBipolar cell. In practice, this was used for all cells thought to be VIP cell, not only those with a bipolar dendrite\n\n\nMC\nInhibitory\nMartinotti cell. In practice, this label was used for all inhibitory neurons that appeared to be Somatostatin cell, not only those with a Martinotti cell morphology\n\n\nNGC\nInhibitory\nNeurogliaform cell. In practice, this label also is used for all inhibitory neurons in layer 1, many of which may not be neurogliaform cells although they might be in the same molecular family\n\n\nOPC\nNon-neuronal\nOligodendrocyte precursor cell\n\n\nastrocyte\nNon-neuronal\nAstrocyte\n\n\nmicroglia\nNon-neuronal\nMicroglia\n\n\npericyte\nNon-neuronal\nPericyte\n\n\noligo\nNon-neuronal\nOligodendrocyte\n\n\n\n\n\n\n# Standard query\nclient.materialize.query_table('aibs_metamodel_celltypes_v661')\n\n# Content-aware query\nclient.materialize.tables.aibs_metamodel_celltypes_v661(id=example_nucleus_id).query()\nPrevious versions of this table include: aibs_soma_nuc_metamodel_preds_v117 (run on a subset of data, the V1 column) and aibs_soma_nuc_exc_mtype_preds_v117 (using training data labeled by another classifier: see mtypes below).\n\n\nCoarse prediction from spine detection\nTable name: baylor_log_reg_cell_type_coarse_v1\nThis table contains the results of a logistic regression classifier trained on properties of neuronal dendrites. This was applied to many cells in the dataset, but required more data than soma and nucleus features alone and thus more cells did not complete the pipeline. It has very good performance on excitatory vs inhibitory neurons because it focuses on dendritic spines, a characteristic property of excitatory neurons. It is a good table to double check E/I classifications if in doubt.\nFor details, see (Celii et al. 2025).\nThe key columns are:\n\nBaylor Coarse Cell Type Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nbaylor_log_reg_cell_type_coarse for all entries\n\n\ncell_type\nexcitatory or inhibitory\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('baylor_log_reg_cell_type_coarse_v1')\n\n# Content-aware query\nclient.materialize.tables.baylor_log_reg_cell_type_coarse_v1(id=example_nucleus_id).query()\n\n\nFine prediction from dendritic features\nTable name: aibs_metamodel_mtypes_v661_v2\nExcitatory neurons and inhibitory neurons were distinguished with the soma-nucleus model (Elabbady et al. 2025), and subclasses were assigned based on a data-driven clustering of the neuronal features. Inhibitory neurons were classified based on how they distributed they synaptic outputs onto target cells, while exictatory neurons were classified based on a collection of dendritic features.\nThis was applied to most detected neurons in the dataset that had complete cell bodies (e.g. not cut off by the edge of the data), detected by nucleus_detection_v0, with small-objects and multi-soma errors removed. The model was run with cell-based features as of version 661 of the dataset.For more details, see (Schneider-Mizell et al. 2025).\nNote that all cell-type labels in this table come from a clustering specific to this paper, and while they are intended to align with the broader literature they are not a direct mapping or a well-established convention.\nFor a more conventional set of labels on the same set of cells, look at the manual table allen_v1_column_types_slanted_ref. Cell types in that table align with those in the aibs_metamodel_celltypes_v661.\nThe key columns are:\n\nAllen Motif-type (mtype) Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nexcitatory or inhibitory\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\nThe cell types in the table are:\n\n\n\n\n\n\nM-type Cell Type definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\nL2a\nExcitatory\nA cluster of layer 2 (upper layer 2/3) excitatory neurons\n\n\nL2b\nExcitatory\nA cluster of layer 2 (upper layer 2/3) excitatory neurons\n\n\nL3a\nExcitatory\nA cluster of excitatory neurons transitioning between upper and lower layer 2/3\n\n\nL3b\nExcitatory\nA cluster of layer 3 (upper layer 2/3) excitatory neurons\n\n\nL3c\nExcitatory\nA cluster of layer 3 (upper layer 2/3) excitatory neurons\n\n\nL4a\nExcitatory\nThe largest cluster of layer 4 excitatory neurons\n\n\nL4b\nExcitatory\nAnother cluster of layer 4 excitatory neurons\n\n\nL4c\nExcitatory\nA cluster of layer 4 excitatory neurons along the border with layer 5\n\n\nL5a\nExcitatory\nA cluster of layer 5 IT neurons at the top of layer 5\n\n\nL5b\nExcitatory\nA cluster of layer 5 IT neurons throughout layer 5\n\n\nL5ET\nExcitatory\nThe cluster of layer 5 ET neurons\n\n\nL5NP\nExcitatory\nThe cluster of layer 5 NP neurons\n\n\nL6a\nExcitatory\nA cluster of layer 6 IT neurons at the top of layer 6\n\n\nL6b\nExcitatory\nA cluster of layer 6 IT neurons throughout layer 6. Note that this is different than the label “Layer 6b” which refers to a narrow band at the border between layer 6 and white matter\n\n\nL6c\nExcitatory\nA cluster of tall layer 6 cells (unsure if IT or CT)\n\n\nL6CT\nExcitatory\nA cluster of tall layer 6 cells matching manual CT labels\n\n\nL6wm\nExcitatory\nA cluster of layer 6 cells along the border with white matter\n\n\nPTC\nInhibitory\nPerisomatic targeting cells, a cluster of inhibitory neurons that target the soma and proximal dendrites of excitatory neurons. Approximately corresponds to basket cell\n\n\nDTC\nInhibitory\nDendrite targeting cells, a cluster of inhibitory neurons that target the distal dendrites of excitatory neurons. Most SST cells would be DTC\n\n\nSTC\nInhibitory\nSparsely targeting cells, a cluster of inhibitory neurons that don’t concentrate multiple synapses onto the same target neurons. Many neurogliaform cells and layer 1 interneurons fall into this category\n\n\nITC\nInhibitory\nInhibitory targeting cells, a cluster of inhibitory neurons that preferntially target other inhibitory neurons. Most VIP cells would be ITCs\n\n\n\n\n\n\n# Standard query\nclient.materialize.query_table('aibs_metamodel_mtypes_v661_v2')\n\n# Content-aware query\nclient.materialize.tables.aibs_metamodel_mtypes_v661_v2(id=example_nucleus_id).query()\nPrevious versions of this table include: aibs_soma_nuc_exc_mtype_preds_v117 (run at v117), allen_column_mtypes_v1 and allen_column_mtypes_v2 (run on a subset of data, the V1 column)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "annotation-tables.html#proofreading-tables",
    "href": "annotation-tables.html#proofreading-tables",
    "title": "Annotation Tables",
    "section": "Proofreading Tables",
    "text": "Proofreading Tables\n\nProofreading Status and Strategy\nTable name: proofreading_status_and_strategy\nThe table proofreading_status_and_strategy describes the status of cells that have undergone manual proofreading.\nBecause of the inherent difference in the difficulty and time required for different kinds of proofreading, we describe the status of axons and dendrites separately.\nEach compartment status may be either:\n\nFALSE: indicates no comprehensive proofreading has been performed, or is not applicable.\n\nTRUE: indicates that false merges have been comprehensively removed, and the compartment is at least ‘clean’. Consult the strategy column if completeness of the compartment is relevant to your research.\n\nAn axon or dendrite labeled as status=TRUE can be trusted to be correct, but may not be complete. The degree of completion can be read from the strategy column. For more information, please see Proofreading and Data Quality; or also the microns-explorer page on proofreading strategies.\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made. NOTE: if this does not match the pt_root_id then the cell has undergone further changes. This is usually and improvement in proofreading, but proceed with caution.\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. May be TRUE or FALSE\n\n\nstatus_axon\nThe status of the axon proofreading. May be TRUE or FALSE\n\n\nstrategy_dendrite\nThe strategy employed to proofread the dendrite. See strategy table below for details\n\n\nstrategy_axon\nThe strategy employed to proofread the axon. See strategy table below for details\n\n\n\nThe specific strategies are as follows (and will update over time):\n\nProofreading Strategies\n\nProofreading Strategy Table\n\n\n\n\n\n\nStrategy\nDescription\n\n\n\n\nnone\nNo cleaning, and no extension. Indicates an entry in proofreading_status that is FALSE for that compartment\n\n\ndendrite_clean\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. The dendrite may be incorrectly truncated by segmentation error. Not all dendrite tips have been checked for extension. No comprehensive attempt was made to re-attach spine heads.\n\n\ndendrite_extended\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. Every tip was identified, manually inspected, and extended if possible. No comprehensive attempt was made to re-attach spine heads.\n\n\naxon_column_truncated\nAThe axon was extended within the V1 cortical column, with a preference for local connections. In some cases the axon was cut at the column boundary and/or the layer boundary, especially the boundary between layers 2/3 and layer 4. Output synapses represent a sampling of potential partners\n\n\naxon_interareal\nThe axon was extended with a preference for branches that projected to other brain areas. Some axon branches were fully extended, but local connections may be incomplete. Output synapses represent a sampling of potential partners.\n\n\naxon_partially_extended\nThe axon was extended outward from the soma, following each branch to its termination. Output synapses represent a sampling of potential partners.\n\n\naxon_fully_extended\nAxon was extended outward from the soma, following each branch to its termination. After initial extension, every endpoint was identified, manually inspected, and extended again if possible. Output synapses represent a largely complete sampling of partners.\n\n\n\nThis table, proofreading_status_and_strategy, supercedes proofreading_status_public_release.\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()\n\n\n\n\n\n\nProofreading status at public release\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis table is out-of-date, and remains here for convenient reference\n\n\nTable name: proofreading_status_public_release\nThe table proofreading_status_public_release describes the status of cells selected for manual proofreading.\nBecause of the inherent difference in the challenge and time required for different kinds of proofreading, we describe the status of axons and dendrites separately. Further, we distinguish three different categories of proofreading:\n\nnon: No proofreading has been comprehensively performed.\nclean: Proofreading has comprehensively removed false merges, but not necessarily added missing parts.\nextended: Proofreading has comprehensively removed false merges and attempted to add all or most missing parts.\n\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. One of the three categories described above\n\n\nstatus_axon\nThe status of the axon proofreading. One of the three categories described above\n\n\n\nThis table is deprecated and superseded by proofreading_status_and_strategy.\n# Set version to 1078 or before\nclient.version = 1078\n\n# Standard query\nclient.materialize.query_table('proofreading_status_public_release')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_public_release(status_axon='extended').query()",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "annotation-tables.html#functional-coregistration-tables",
    "href": "annotation-tables.html#functional-coregistration-tables",
    "title": "Annotation Tables",
    "section": "Functional Coregistration Tables",
    "text": "Functional Coregistration Tables\nTo relate the structural data to functional data, cell bodies must be coregistered between the functional imaging and EM volumes. As with cell-typing, there are manual and automated methods for doing this, with the former having higher accuracy but lower throughput.\n\ncoregistration_manual_v4 : The results of manually verified coregistration. This table is well-verified, but contains fewer ROIs (N=15,352 root ids, 19,181 ROIs).\ncoregistration_auto_phase3_fwd_apl_vess_combined_v2 : The results of automated functional matching between the EM and 2-p functional data. This table is not manually verified, but contains more ROIs (N=35,466 root ids, N=83,046 ROIs).\n\n\nManual coregistration\nTable name: coregistration_manual_v4\nA table of EM nucleus centroids manually matched to Baylor functional units. A unique functional unit is identified by its session, scan_idx and unit_id. An EM nucleus centroid may have matched to more than one functional unit if it was scanned on more than one imaging field.\nThe key columns are:\n\nCoregistration table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nsession\nThe session index from functional imaging\n\n\nscan_idx\nThe scan index from functional imaging\n\n\nunit_id\nThe functional unit index from imaging. Only unique within scan and session\n\n\nfield\nThe field index from functional imaging\n\n\nresidual\nThe residual distance between the functional and the assigned structural points after transformation, in microns\n\n\nscore\nA separation score, measuring the difference between the residual distance to the assigned neuron and the distance to the nearest non-assigned neuron, in microns. This can be negative if the non-assigned neuron is closer than the assigned neuron. Larger values indicate fewer nearby neurons that could be confused with the assigned neuron.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis table includes duplicate entries for the same ‘pt_root_id’ and nucleus id if the coregistered cell has multiple unit recordings\n\n\n# Standard query\nclient.materialize.query_table('coregistration_manual_v4')\n\n# Content-aware query\nclient.materialize.tables.coregistration_manual_v4(id=example_nucleus_id).query()\nThis table coregistration_manual_v4 supercedes previous iterations of this table:\n\ncoregistration_manual_v3\ncoregistration_manual\n\n\n\nAutomated coregistration\nTable name: coregistration_auto_phase3_fwd_apl_vess_combined_v2\nA table of EM nucleus centroids automatically matched to Baylor functional units. This table reconciles the following two tables that both make a best match of the of registration using different techniques: coregistration_auto_phase3_fwd_v2 and apl_functional_coreg_vess_fwd. A unique functional unit is identified by its session, scan_idx and unit_id. An EM nucleus centroid may have matched to more than one functional unit if it was scanned on more than one imaging field.\nThe key columns are:\n\nCoregistration table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nsession\nThe session index from functional imaging\n\n\nscan_idx\nThe scan index from functional imaging\n\n\nunit_id\nThe functional unit index from imaging. Only unique within scan and session\n\n\nfield\nThe field index from functional imaging\n\n\nresidual\nThe residual distance between the functional and the assigned structural points after transformation, in microns\n\n\nscore\nA separation score, measuring the difference between the residual distance to the assigned neuron and the distance to the nearest non-assigned neuron, in microns. This can be negative if the non-assigned neuron is closer than the assigned neuron. Larger values indicate fewer nearby neurons that could be confused with the assigned neuron.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis table includes duplicate entries for the same pt_root_id and nucleus id if the coregistered cell has multiple unit recordings\n\n\n# Standard query\nclient.materialize.query_table('coregistration_auto_phase3_fwd_apl_vess_combined_v2')\n\n# Content-aware query\nclient.materialize.tables.coregistration_auto_phase3_fwd_apl_vess_combined_v2(id=example_nucleus_id).query()\nThis table coregistration_auto_phase3_fwd_apl_vess_combined_v2 supercedes previous iterations of this table:\n\ncoregistration_auto_phase3_fwd_apl_vess_combined_v2\napl_functional_coreg_forward_v5\n\n\n\nFunctional properties\nTable name: functional_properties_v3_bcm\nA summary of the functional properties for each of the coregistered neurons (as of coregistration_manual_v3). For details, see (Ding et al. 2025)\nThe key columns are:\n\nFuctional properties of coregistered neurons\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\nsession\nThe session index from functional imaging\n\n\nscan_idx\nThe scan index from functional imaging\n\n\nunit_id\nThe functional unit index from imaging. Only unique within scan and session\n\n\npref_ori\npreferred orientation in radians (0 - pi), horizontal bar moving upward is 0 and orientation increases clockwise, extracted from model responses to oriented noise stimuli\n\n\npref_dir\npreferred direction in radians (0 - 2pi), horizontal bar moving upward is 0 and orientation increases clockwise, extracted from model responses to oriented noise stimuli\n\n\ngOSI\nglobal orientation selectivity index\n\n\ngDSI\nglobal direction selectivity index\n\n\ncc_abs\nprediction performance of the model, higher is better\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis table includes duplicate entries for the same ‘pt_root_id’ and nucleus id if the coregistered cell has multiple unit recordings\n\n\n# Standard query\nclient.materialize.query_table('functional_properties_v3_bcm')\n\n# Content-aware query\nclient.materialize.tables.functional_properties_v3_bcm(id=example_nucleus_id).query()",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "annotation-tables.html#overview-of-relevant-tables",
    "href": "annotation-tables.html#overview-of-relevant-tables",
    "title": "Annotation Tables",
    "section": "Overview of relevant tables",
    "text": "Overview of relevant tables\n\nheard you like tables–here’s a table for your tables\n\n\n\n\n\n\n\nTable Name\nNumber of Annotations\nDescription\n\n\n\n\nsynapses_pni_2\n337,312,429\nThe locations of synapses and the segment ids of the pre and post-synaptic automated synapse detection\n\n\nnucleus_detection_v0\n144,120\nThe locations of nuclei detected via a fully automated method\n\n\nnucleus_alternative_points\n8,388\nA reference annotation table marking alternative segment_id lookup locations for a subset of nuclei in nucleus_detection_v0 that is more accurate than the centroid location listed there\n\n\nnucleus_ref_neuron_svm\n144,120\nreference annotation indicating the output of a model detecting which nucleus detections are neurons versus which are not 1\n\n\ncoregistration_manual_v4\n19,181\nA table indicating the association between individual units in the functional imaging data and nuclei in the structural data, derived from human powered matching. Includes residual and separation scores to help assess confidence\n\n\ncoregistration_auto_phase3_fwd_apl_vess_combined_v2\n83,046\nA table indicating the association between individual units in the functional imaging data and nuclei in the structural data, derived from the automated procedure. Includes residuals and separation scores to help assess confidence\n\n\nproofreading_status_and_strategy\n2020\nA table indicating which neurons have been proofread on their axons or dendrites\n\n\naibs_column_nonneuronal_ref\n542\nCell type reference annotations from a human expert of non-neuronal cells located amongst the Minnie Column\n\n\nallen_v1_column_types_slanted_ref\n1,357\nNeuron cell type reference annotations from human experts of neuronal cells located amongst the Minnie Column\n\n\nallen_column_mtypes_v1\n1,357\nNeuron cell type reference annotations from data driven unsupervised clustering of neuronal cells\n\n\naibs_metamodel_mtypes_v661_v2\n72,158\nReference annotations indicating the output of a model predicting cell types across the dataset based on the labels from allen_column_mtypes_v1.1\n\n\naibs_metamodel_celltypes_v661\n94,014\nReference annotations indicating the output of a model predicting cell classes based on the labels from allen_v1_column_types_slanted_ref and aibs_column_nonneuronal_ref\n\n\nbaylor_log_reg_cell_type_coarse_v1\n55,063\nReference annotations indicated the output of a logistic regression model predicting whether the nucleus is part of an excitatory or inhibitory cell\n\n\nbaylor_gnn_cell_type_fine_model_v2\n49,051\nReference annotations indicated the output of a graph neural network model predicting the cell type based on the human labels in allen_v1_column_types_slanted_ref\n\n\nvortex_astrocyte_proofreading_status\n126\nThis table reports the status of a manually selected subset of astrocytes within the VISP column. Astrocyte seelection and proofreading performed as part of VORTEX.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "analysis_gallery/index.html",
    "href": "analysis_gallery/index.html",
    "title": "Analysis Gallery",
    "section": "",
    "text": "Code from Research Publications\n\nCoding examples and github repositories for the Nature 2025 research publications\n\n\n\n\n  \n    \n      \n        The MICrONS Consortium\n        \n        Functional connectomics spanning multiple areas of mouse visual cortex\n      \n    \n    \n      \n    \n    \n    \n    \n  \n\n  \n    \n      \n        Schneider-Mizell et al.\n        \n        Inhibitory specificity from a connectomic census of mouse visual cortex\n      \n    \n    \n      \n    \n    \n    \n    \n  \n\n  \n    \n      \n        Ding et al.\n        \n        Functional connectomics reveals general wiring rule in mouse visual cortex\n      \n    \n    \n      \n    \n    \n    \n    \n  \n\n  \n    \n      \n        Wang et al.\n        \n        Foundation model of neural activity predicts response to new stimulus types\n      \n    \n    \n      \n    \n    \n    \n    \n  \n\n  \n    \n      \n        Elabbady et al.\n        \n        Perisomatic ultrastructure efficiently classifies cells in mouse cortex\n      \n    \n    \n      \n    \n    \n    \n    \n  \n\n  \n    \n      \n        Gamlin et al.\n        \n        Connectomics of predicted Sst transcriptomic types in mouse visual cortex\n      \n    \n    \n      \n    \n    \n    \n    \n  \n\n  \n    \n      \n        Celii et al.\n         \n        \n          \n        \n        \n        NEURD offers automated proofreading and feature extraction for connectomics\n      \n    \n    \n      \n    \n    \n    \n    \n  \n\n  \n    \n      \n        Dorkenwald et al.\n         \n        \n          \n        \n        \n        CAVE Connectome Annotation Versioning Engine\n      \n    \n    \n      \n    \n    \n    \n    \n  \n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "Background",
    "section": "",
    "text": "A 3D rendering of cells in the MICrONs dataset.\n\n\nThis IARPA MICrONS dataset spans a 1.4mm x .87mm x .84 mm volume of cortex in a P87 mouse. It is 400x larger than the previously released Phase 1 dataset, which can be found here. The dataset was imaged using two-photon microscopy, microCT, and serial electron microscopy, and then reconstructed using a combination of AI and human proofreading.\nFunctional imaging data of an estimated 75,000 pyramidal neurons in the volume contains the single cell responses to a variety of visual stimuli. That same tissue was imaged with high resolution electron microscopy and processed through a machine learning driven reconstruction pipeline to generate a large scale anatomical connectome. The anatomical data contains more than an estimated 200,000 cells, and 120,000 neurons (details). Automated synapse detection measured more than 523 million synapses.\nThis makes this dataset remarkable in a few key ways\n\nIt was the largest multi-modal connectomics dataset released and the largest connectomics dataset by minimum dimension, number of cells, and number of connections detected (as of releas in July 2021)\nIt is the first EM reconstruction of a mammalian circuit across multiple functional brain areas.\n\nOn this set of websites, you can find visualization tools, as well as instructions tutorials for how to query and download portions of the dataset programmatically. To “git started” analyzing data, checkout the MICrONS Programmatic Tutorials for anatomical data and the MICrONS Neural Data Access (NDA) repository for functional data.\nThe pre-print with details about the dataset is:\n\nMICrONs Consortium et al. Functional connectomics spanning multiple areas of mouse visual cortex. bioRxiv 2021.07.28.454025; doi: https://doi.org/10.1101/2021.07.28.454025",
    "crumbs": [
      "Introduction",
      "Background"
    ]
  },
  {
    "objectID": "background.html#a-functional-connectomics-dataset-spanning-multiple-visual-areas",
    "href": "background.html#a-functional-connectomics-dataset-spanning-multiple-visual-areas",
    "title": "Background",
    "section": "",
    "text": "A 3D rendering of cells in the MICrONs dataset.\n\n\nThis IARPA MICrONS dataset spans a 1.4mm x .87mm x .84 mm volume of cortex in a P87 mouse. It is 400x larger than the previously released Phase 1 dataset, which can be found here. The dataset was imaged using two-photon microscopy, microCT, and serial electron microscopy, and then reconstructed using a combination of AI and human proofreading.\nFunctional imaging data of an estimated 75,000 pyramidal neurons in the volume contains the single cell responses to a variety of visual stimuli. That same tissue was imaged with high resolution electron microscopy and processed through a machine learning driven reconstruction pipeline to generate a large scale anatomical connectome. The anatomical data contains more than an estimated 200,000 cells, and 120,000 neurons (details). Automated synapse detection measured more than 523 million synapses.\nThis makes this dataset remarkable in a few key ways\n\nIt was the largest multi-modal connectomics dataset released and the largest connectomics dataset by minimum dimension, number of cells, and number of connections detected (as of releas in July 2021)\nIt is the first EM reconstruction of a mammalian circuit across multiple functional brain areas.\n\nOn this set of websites, you can find visualization tools, as well as instructions tutorials for how to query and download portions of the dataset programmatically. To “git started” analyzing data, checkout the MICrONS Programmatic Tutorials for anatomical data and the MICrONS Neural Data Access (NDA) repository for functional data.\nThe pre-print with details about the dataset is:\n\nMICrONs Consortium et al. Functional connectomics spanning multiple areas of mouse visual cortex. bioRxiv 2021.07.28.454025; doi: https://doi.org/10.1101/2021.07.28.454025",
    "crumbs": [
      "Introduction",
      "Background"
    ]
  },
  {
    "objectID": "background.html#electron-microscopy-methods",
    "href": "background.html#electron-microscopy-methods",
    "title": "Background",
    "section": "Electron Microscopy methods",
    "text": "Electron Microscopy methods\nThe general approach to EM involves staining tissue with heavy metals and using electrons to imaging thin sections of tissue. Sections can either be created by cutting tissue into slices (“serial section”) followed by imaging, or by imaging the face of a block and shaving off a thin layer of tissue after each run (“block face”). In both approaches, each section corresponds to only 30–50 nanometers of thickness (although it can be a millimeter wide), and thus assembling a substantial volumes involves thousands or tens of thousands of sections in a row. Finally, computational methods are used to align and assemble the sections into a single volume and then to segment the volume into individual cells and synapses.\nWhile the resolution afforded by EM has been used to study the nervous system since 1959, the first true “connectomics” project that merged dense mapping of neurons and synaptic connectivity was the study of the nematode C. elegans led by Sydney Brenner, John White, and others. Advances in EM methods and computational methods in the last ten years have allowed tremendous scaling of connectomics datasets. The largest current mammalian datasets, including the MICrONs volume will consider here, now span more than a cubic millimeter, and invertebrate datasets now can cover the entire brain of a fruit fly.",
    "crumbs": [
      "Introduction",
      "Background"
    ]
  },
  {
    "objectID": "background.html#data-use-cases",
    "href": "background.html#data-use-cases",
    "title": "Background",
    "section": "Data Use Cases",
    "text": "Data Use Cases\n\nCell Types\n\n\n\n\n\nCell types: pyramidal cell, astrocyte, neurogliaform cell, bipolar cell\n\n\n\n\nYou can use this dataset to explore the morphology and connectivity of a very diverse variety of excitatory, inhibitory, and non-neuronal cell types throughout the volume.\nClick these links for example cells: bipolar cells, basket cells, a chandelier cell, martinotti cells, a layer 5 thick tufted, a layer 5 NP cell, layer 4 cells, layer 2/3 cells, blood vessels, astrocytes, microglia. For programmatic access of more systematic cell type information released thus far, explore MICrONS Tutorials.",
    "crumbs": [
      "Introduction",
      "Background"
    ]
  },
  {
    "objectID": "background.html#synaptic-connectivity",
    "href": "background.html#synaptic-connectivity",
    "title": "Background",
    "section": "Synaptic Connectivity",
    "text": "Synaptic Connectivity\n\n\n\n\n\nSynapses, inset\n\n\n\n\nQuery from hundreds of millions of synapses in the volume from any of the segmented cells. To see an example visualization with one cell’s inputs and outputs annotated click the button below. We are releasing an app that you can use to query this data non-programmatically and generate a synaptic visualization for any cell. To learn how to query the data programmatically yourself, look at this notebook on MICrONS Tutorials.\nSynaptic Visualization (neuroglancer)\nLaunch Connectivity Viewer (web app)\n\n\n\nFunctional Data\n\n\n\n\n\nDeconvolved activity traces\n\n\n\n\nAll the functional recordings from 115,372 regions of interest and 75,902 estimated neurons are available, including the response of those units to a variety of visual stimuli. You can download the database of functional recordings to explore how cells responded to visual stimuli. For a subset of the neurons, we have generated a correspondence between their functional recording and their anatomical reconstruction.\nTo learn how to query the functional databases and link them to the anatomical data, you can view this notebook on MICrONS NDA, or access a hosted database and interactive notebooks by following the instructions at this repository.",
    "crumbs": [
      "Introduction",
      "Background"
    ]
  },
  {
    "objectID": "dash-table-viewer.html",
    "href": "dash-table-viewer.html",
    "title": "Dash Apps: Table Viewer",
    "section": "",
    "text": "Version update\n\n\n\nThis tutorial has recently been updated to materialization version 1412 (from 1300).\nWe have released a new public version 1412, as part of our quarterly release schedule. See details at Release Manifests: 1412.\nWe have created two webapps to help explore and visualize the data. These apps were created using the Dash framework, hence the name.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Dash Apps: Table Viewer"
    ]
  },
  {
    "objectID": "dash-table-viewer.html#table-viewer",
    "href": "dash-table-viewer.html#table-viewer",
    "title": "Dash Apps: Table Viewer",
    "section": "Table Viewer",
    "text": "Table Viewer\nThe Table Viewer app allows you to query individual tables in the database, filter them, and visualize the results in Neuroglancer.\n\n\n\nTable viewer input options\n\n\nIn the Table Viewer, you first select one of the tables from a dropdown in the upper left side.\nThe Materialization option allows you to select which version of the dataset to query from. Default is ‘latest’.\nFilter options allow you to limit the query to a subset of the data, if desired. Note that filtering can occur in the tables afterward as well.\nThe Cell IDs field lets you put in one or more root IDs, cell IDs (i.e. “Nucleus ID”), or IDs from the annotation table being queried. Note that if you use this option, you havet to set the dropdown to the appropriate ID type.\nThe Value Search field lets you search the Search Column for a particular value, such as a specific cell type.\nOnce you have selected your table and set any pre-query filters, click the Submit button to run the query. When the query is done, a green bar will appear below the query input stating that it’s showing the state of the table you queried.\nThe first set of controls allow you to build a Neuroglancer link for the whole table using the “Generate Link” button.\nOptionally, if you would like to make a link where different values of a given column — example, different cell types — are shown in different layers, you can turn on the “Group Annotations” toggle on the right and select the column you want to use for groupings.\nYou have the drop-down option to select which branch of neuroglancer to use: Seung-lab or Spelunker. If the annotations you expect are not rendering, try the other neuroglancer type.\nAfter setting these controls, click the “Generate Link” button to create the Neuroglancer link.\n\n\n\nTable viewer query results.\n\n\nThe table itself is shown below the controls. You can sort the table by clicking on the column headers, and filter the table by typing in the filter box below the column headers.\nFor example, if you want to find only cells with cell type “5P-NP” in a cell type column, simply type 5P-NP in the box under the column header. Similarly, to find only cells whose nucleus volume is greater than 500, type &gt;500 in the box under the volume column header in a table with nucleus information.\nIn addition, individual rows can be selected by clicking on the checkbox to the left of the row.\nThe left button along the top is a Neuroglancer link specific to the selected rows, if present, or the table as filtered if no rows are selected.\nThe orange button will deselect all checkboxes, and the “Export to CSV” will download the table as filtered to a CSV.\n\n\n\n\n\n\nWarning\n\n\n\nIf the table is too large (well beyond 10,000 rows), there are too many annotations to show in Neuroglancer and no link will be automatically generated.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Dash Apps: Table Viewer"
    ]
  },
  {
    "objectID": "dataset-basics.html",
    "href": "dataset-basics.html",
    "title": "Dataset Basics",
    "section": "",
    "text": "One of the more complicated things about using connectomics data is that it’s both highly multimodal — there are images, tables, meshes, and more — and that it has been designed to be dynamic, as the segmentation changes with proofeading and carries changes to synaptic connectivity and more. This section will cover what types of data exist and how to access them.\nThe types of data we will describe here are:\nIn addition, there are three primary ways to access the data:",
    "crumbs": [
      "Introduction",
      "Dataset Basics"
    ]
  },
  {
    "objectID": "dataset-basics.html#dataset-information-hub",
    "href": "dataset-basics.html#dataset-information-hub",
    "title": "Dataset Basics",
    "section": "Dataset Information Hub",
    "text": "Dataset Information Hub\nIf you need to look up any particular values or links for the dataset — how to open a Neuroglancer link for it, what annotation tables exist, how to find Dash Apps — the public MICrONs datastack has a useful information page. Go there if you need to look up basic details about the dataset.\nParticularly useful links here are:\n\nthe Spelunker neuroglancer link\nthe public Dash Apps\nthe list of all materialization versions\nand the annotation tables associated with the most recent release version (v1300).",
    "crumbs": [
      "Introduction",
      "Dataset Basics"
    ]
  },
  {
    "objectID": "dataset-basics.html#how-to-reference-a-neuron",
    "href": "dataset-basics.html#how-to-reference-a-neuron",
    "title": "Dataset Basics",
    "section": "How to reference a neuron",
    "text": "How to reference a neuron\nThe dynamic nature of the data means that there is more than one way to reference a neuron, and each way has a slightly different meaning.\nThe most frequent way to reference a specific neuron is its root id, a unique integer that identifies a specific segmentation, and can be used in numerous places. However, as proofreading occurs, the segmentation of a cell can change, and each edit creates a new “version” of a given neuron that is given its own unique root id. Root ids thus refer to not only a specific cell, but a specific version of that cell.\nWhen downloading meshes, querying the database, or browsing in Neuroglancer, we use the root id of a cell.\nHowever, when using data from different points in time, however, root ids can change and the data might not be consistent across two time points. One way to make this easier is to only use data from one moment in time.\nThis is the case for the public database that will be used in the course — all data from the public database is synched up to a single moment in time, but it does mean that the analysis cannot respond to any changes in segmentation. In some cases, in some cases, you might see the root id called a “segment id” or an “object id,” but they all refer to the same thing.\nFor segmentations with a cell body in the volume, we also have a cell id (or soma id), which is a unique integer that identifies each nucleus in the dataset. The advantage of the soma id is that it remains constant across all versions of a cell, so it can be used to track a cell across time. The disadvantage is that to look up the id at any given time or to load into Neuroglancer, you need to look up the root id.\nCell ids can be found in the nucleus_detection_v0 table in the database, as well as many of the tables that reference cell types.",
    "crumbs": [
      "Introduction",
      "Dataset Basics"
    ]
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Version update\n\n\n\nThis tutorial has recently been updated to materialization version 1412 (from 1300).\nWe have released a new public version 1412, as part of our quarterly release schedule. See details at Release Manifests: 1412."
  },
  {
    "objectID": "faq.html#data-access",
    "href": "faq.html#data-access",
    "title": "Frequently Asked Questions",
    "section": "Data Access",
    "text": "Data Access\n\n\n\n\n\n\nHow do I download the data?\n\n\n\n\n\nThe MICrONS dataset is massive. The imagery and segmentation are on the order of petabytes. There are hundreds of millions of synapses, and even downloading a static table of the synaptic conenctivity is tens of gigabytes.\n\nIn general, you do not want to download the entire dataset.\n\nInstead, we have tools that allow you to visualize, locate, and download only the data you are most interested in.\nFor downloading the imagery, segmentation, and meshes, you will use the the python package cloud-volume. CloudVolume is a serverless Python client for random access reading and writing of Neuroglancer volumes in “Precomputed” format, a set of representations for arbitrarily large volumetric images, meshes, and skeletons. See Download Imagery and Segmentation and Download Meshes\nFor downloading neuron skeletons in .swc format, see Download Skeletons.\nFor connectivity information, you can use the brower-based Connectivity Viewer which will: access the connectivity for cells of interest, generate neuroglancer links for interactive visualization, and generate connectivity summaries on demand. For large scale connectivity analysis (between many cells, or populations of cells), you will want to access the data programmatically. See the Python tools quickstart notebooks, specifically CAVE Query Synaptic Connectivity.\nFor all other annotations – such as cell types, synapse compartment types, and axon myelination – you will want to access the annotation tables programmatically through the python tool CAVEclient. See the Python tools quickstart notebooks, specifically CAVE Query Cell Types and CAVE Query Proofread Cells for practical examples.\n\nIf you really do still want to download the entire dataset:\n\nWe have bulk downloads available from our cloud service providers. See:\n\nStatic Repositories for a list of available data, sizes, and cloud paths. This includes flat segmentation, meshes, or skeletons at some major versions.\nBossdb repository for MICrONS aligned volume data\nDANDI for functional NWB files\n\nDirectly downloading these resources may speed up processing time, and you can point cloud-volume to a local filepath just as easily as a cloud source.\n\n\n\n\n\n\n\n\n\nHow can I access the connectivity information?\n\n\n\n\n\n\nBrowser-based method\n\nYou can use the Connectivity Viewer to: access the connectivity for cells of interest, generate neuroglancer links for interactive visualization, and generate connectivity summaries on demand.\n\nProgrammatic access\n\nFor large scale connectivity analysis (between many cells, or populations of cells), you will want to access the data programmatically. See the Python tools quickstart notebooks, specifically CAVE Query Synaptic Connectivity.\n\nBulk download of connectivity grpah\n\n\n\n\n\n\n\nImportant\n\n\n\nThis connectivity table represents the original v117 segmentation. Proofreading has improved the accuracy and added millions of high-quality synaptic connections. The programmatic access above is our recommended method to perform connectivity analysis\n\n\nThe synaptic connectivity map at version 117 is available for bulk download from Bossdb. See Static Repositories - Synapse Graph for details.\n\n\n\n\n\n\n\n\n\nHow can I do this analysis?\n\n\n\n\n\nThere are many, many things worth analyzing in the dataset, and a myriad of ways to do that analysis. On this website we host Quickstart tutorials that provide minimal examples of what is possible. We are also expanding the gallery of Analysis Examples. And we outlink to relevant repositories from Publications using the MICrONS dataset.\nConsider posting to the Tutorials Forum for feedback and discussion.\nWe are also available for individual analysis consultation and support. Access begins with submitting a VORTEX request\nmicrons-explorer.org/requests | Scientific Request Form\n\n\n\n\n\n\n\n\n\nHow do I cite the dataset?\n\n\n\n\n\nFor any general use of the MICrONS Cubic Millimeter data and resources, please cite the primary flagship publication:\n\nThe MICrONS Consortium, J. Alexander Bae, Mahaly Baptiste, Maya R. Baptiste, Caitlyn A. Bishop, Agnes L. Bodor, Derrick Brittain, et al. 2025. “Functional Connectomics Spanning Multiple Areas of Mouse Visual Cortex.” Nature 640 (8058): 435–47. https://doi.org/10.1038/s41586-025-08790-w.\n\nFor use of the MICrONS Layer 2/3 dataset (Phase 1), please cite the primary flagship publication\n\nNicholas L. Turner, Thomas Macrina, J. et al. 2022. “Reconstruction of neocortex: Organelles, compartments, cells, circuits, and activity.” Cell 185 (6): 1082-1100. https://doi.org/10.1016/j.cell.2022.01.023.\n\nFor any primary research publication, cite the publication (if released) or preprint (if unreleased). See microns-explorer.org/publications for more.\nWhen integrating our data into a website, cite the source in a manner that is clear, accurate and easily discoverable and link to www.microns-explorer.org. Also, be sure that our Content is never displayed in the absence of such citation. For images obtained from the Sites, use the following citation: www.microns-explorer.org."
  },
  {
    "objectID": "faq.html#data-curation",
    "href": "faq.html#data-curation",
    "title": "Frequently Asked Questions",
    "section": "Data Curation",
    "text": "Data Curation\n\n\n\n\n\n\nMy cell disappeared\n\n\n\n\n\nThe MICrONS dataset is public and open access, ready for analysis, but manual edits to the segmentation continue to improve data quality. The automatic segmentation from EM imagery to 3D reconstruction was largely effective, and the only way to process data at this scale (The MICrONS Consortium et al. 2025). However, due to imaging defects and the nature of thin, branching axons, the automated methods do make mistakes that have large impacts on the biological accuracy of the reconstructions.\nThe data is regularly versioned; that is, a long-term copy of the dataset is made available for users. We highly recommend setting the version or timestamp in your analysis for consistency.\nHowever, even if you do not set the version, there is a lineage graph of changes to the dataset. Meaning, you can find the past version of your cell, annotation table, mesh, skeleton etc. as long as you know the root id of the object you are interested in – or the date at which you performed an analysis.\n\nSee the Materialization and Versioning section for detailed examples of how to 1) find your missing cell, and 2) set the version of your analysis so you do not encounter this issue again.\n\n\n\n\n\n\n\n\n\n\nThis cell is incorrect\n\n\n\n\n\nAutomated methods for cellular segmentation, synapse detection, and classification are fantastic for working with data at this scale (The MICrONS Consortium et al. 2025). However, each of these processes makes mistakes. We have methods of maunal correction or proofreading to improve the quality of the data.\n\nProofreading\n\nIt is useful to distinguish between two kinds of proofreading errors: splits and merges.\nSplit errors occur when a process incorrectly appears to stop in the segmentation, but actually could be continued. Merge errors occur when processes from two different segments are incorrectly merged together. These two kinds of errors impact analysis differently. For example for neurons: while split errors reduce the number of correct synaptic connections, merge errors add incorrect connections.\nThe frequency of errors is roughly related to the size of the process, and thus axons and dendrites have extremely different error profiles. Because they are thicker, dendrites are largely well-segmented, with only a few split errors and most merge errors being with small axons rather than other dendrites.\nSegmentation errors are also more common in places where the image quality was lower, or the image sections interupted. Such as at the EM volume boundaries (pial surface, white matter, and lateral edges.\nWe provide ongoing proofreading to the dataset through the VORTEX program. We also support and train scientists who want to make their own proofreading edits to the dataset. For either object, see VORTEX below\n\nVirtual Observatory of the Cortex VORTEX\n\nThe VORTEX program is a grant funded mechanism designed to assist you in precisely this situation. We provide the labor and experience to make corrections and annotations in the data. The results of these efforts will be integrated with an evolving version of the dataset, and shared with you and the larger scientific community.\nDepending on the scale of errors in your cells of interest, we may provide:\n\nanalysis support to interpret the data despite the errors\nlimited correction of small errors\nlarge-scale proofreading support to answer scientific questions\nprovide you direct proofreading access to the dataset and training in how to make edits\n\nAny of these solutions begins with submitting a VORTEX request\nmicrons-explorer.org/requests | Scientific Request Form\n\nAutomated classifiers\n\nAutomated classifiers such as the synapse detection and cell type tables can also make mistakes. In practice, it is easiest to exclude data from your analysis if it does not match expectations or quality metrics. If you have specific feedback about performance of the automated classifiers, reach out of the maintainers of the Annotation table in question. This information is included in the table metadata\nclient.materialize.tables.get_table_metadata(table_name)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MICrONs Tutorials",
    "section": "",
    "text": "Welcome to the MICrONs-Explorer data tutorial ecosystem.\nThe MICrONS open-access dataset is a functional connectome containing 200,000 cells, 75,000 neurons with physiology, and 523 million synapses.\nThis resource provides tools and tutorials for how to access the anatomical and functional data that span all 6 layers of mouse primary visual cortex and 3 higher visual areas (LM, AL, RL) within a cubic millimeter volume.\n\nHighlights\n\n\nDataset overview\n\n\n\n\nIntroduction: motivation of and introduction to the MICrONS dataset\nBackground: Overview of dataset collection, methods, and outputs\n\n\nData curation and improvements\n\n\nVORTEX program: for ongoing proofreading serving the scienfitic community\nProofreading: description of proofreading efforts and how they impact data interpretability\nMaterialization and Versioning: description of proofreading efforts and how they impact data interpretability\n\n\nData analysis tools\n\n\nNeuroglancer: for interactive visualization, exploration, and annotation\nTable Viewer: a Dash App, browser-based tool, for querying the CAVEclient annotation tables\nConnectivity Viewer: a Dash App, browser-based tool, for querying and visualizing synaptic connectivity\nPython Tools: introduction and quickstart to the Connectome Annotation Versioning Engine (CAVE) and associated python packages\n\n\nData Release information:\n\n\nAnnotation Tables: semantic description of the annotation tables available throug CAVEclient\nRelease Manifests: describing major materialization versions, which are periodic updates to the publicly available data\nStatic Repositories: for bulk download of data\n\n\n\n\n\n\nMICrONS dataset overivew. Figure from Nature, 2025 (DOI https://doi.org/10.1038/d41586-025-00908-4)\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "materialization-version.html",
    "href": "materialization-version.html",
    "title": "Materialization and Versioning",
    "section": "",
    "text": "Version update\n\n\n\nThis tutorial has recently been updated to materialization version 1412 (from 1300).\nWe have released a new public version 1412, as part of our quarterly release schedule. See details at Release Manifests: 1412.\nWhen beginning your analysis in the MICrONS dataset, it is important to understand:\nThe data is regularly versioned; that is, a long-term copy of the dataset is made available for users. We highly recommend setting the version or timestamp in your analysis for future consistency.\nHowever, even if you do not set the version, there is a lineage graph of changes to the dataset. Meaning, you can find the past version of your cell, annotation table, mesh, skeleton etc. as long as you know the root id of the object you are interested in–or the date at which you performed an analysis.",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Materialization and Versioning"
    ]
  },
  {
    "objectID": "materialization-version.html#why-the-data-changes",
    "href": "materialization-version.html#why-the-data-changes",
    "title": "Materialization and Versioning",
    "section": "Why the data changes",
    "text": "Why the data changes\nThe automatic segmentation from EM imagery to 3D reconstruction was largely effective, and the only way to process data at this scale (The MICrONS Consortium et al. 2025). However, due to imaging defects and the nature of thin, branching axons, the automated methods do make mistakes that have large impacts on the biological accuracy of the reconstructions.\nManual Proofreading, or the correction of segmentation and adding of annotations, is an ongoing effort.\nDifferent aspects of the data require different level of manual intervention. For example, the segmentation methods produced highly accurate dendritic arbors before proofreading, enabling morphological identification of broad cell types. Most dendritic spines are properly associated with their dendritic trunk. Recovery of larger-caliber axons, those of inhibitory neurons, and the initial portions of excitatory neurons was also typically successful. Owing to the high frequency of imaging defects in the shallower and deeper portions of the dataset, processes near the pia and white matter often contain errors. Many non-neuronal objects are also well-segmented, including astrocytes, microglia and blood vessels. The two subvolumes of the dataset were segmented separately, but the alignment between the two is sufficient for manually tracing between them.\nChanges to the dataset represent an improvement in accuracy, and reflect an investment in the long-term usefulness of this open-access resource.",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Materialization and Versioning"
    ]
  },
  {
    "objectID": "materialization-version.html#what-types-of-data-change-with-time",
    "href": "materialization-version.html#what-types-of-data-change-with-time",
    "title": "Materialization and Versioning",
    "section": "What types of data change with time",
    "text": "What types of data change with time\nProofreading edits to the segmentation change what supervoxels (groups of locally aggregated voxels) are associated with what segmented object. Any time the supervoxel is associated with a different segmented object, all of the ids upstream of that supervoxel will update. In practice, this means the 18-digit segmentation id or pt_root_id of your neuron or microglia or axon etc. will change every time it is proofread.\n\n\n\na, Automated segmentation overlaid on EM data. Each color represents an individual putative cell. b, Different colors represent supervoxels that make up putative cells. c, Supervoxels belonging to a particular neuron, with an overlaid cartoon of its supervoxel graph. These data corresponds to the framed square in a and the full panel in b. d, One-dimensional representation of the supervoxel graph. The ChunkedGraph data structure adds an octree structure to the graph to store the connected component information. Each abstract node (black nodes in levels &gt;1) represents the connected component in the spatially underlying graph.\n\n\n\n\nFigure from (Dorkenwald et al. 2025)\n\n\nThe pt_root_id is always associated with the same collection of supervoxels, and therefore the same mesh and same skeleton. But if that pt_root_id is expired, then you may not find that object in current Annotation Tables, Synapse Connectivity Tables, and Neuroglancer views of the current version of the dataset (default).\nCreating a new pt_root_id for an edited object is the only way to have the flexibility of both merging two or more segments that should be connected (for example: extending an axon) and splitting an object into two, as in the following example:\n\n\n\nf, To submit a split operation, users place labels for each side of the split (top right). The backend system first connects each set of labels on each side by identifying supervoxels between them in the graph (left). The extended sets are used to identify the edges needed to be cut with a maximum-flow minimum-cut algorithm.\n\n\n\n\nFigure from (Dorkenwald et al. 2025)\n\n\nBut this also means we can track the histories of what ids used to be part of which segmented objects, which helps for finding the same cell, axon, or arbitrary segment across time. See Lineage Graphs below for details.",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Materialization and Versioning"
    ]
  },
  {
    "objectID": "materialization-version.html#how-to-set-the-version-of-your-analysis",
    "href": "materialization-version.html#how-to-set-the-version-of-your-analysis",
    "title": "Materialization and Versioning",
    "section": "How to set the version of your analysis",
    "text": "How to set the version of your analysis\nMost programmatic access to the CAVE services occurs through CAVEclient, a Python client to access various types of data from the online services.\nFull documentation for CAVEclient is available here.\n\n\n\n\n\n\nInitial Setup\n\n\n\nBefore using any programmatic access to the data, you first need to set up your CAVEclient token.\n\n\nTo initialize a caveclient, we give it a datastack, which is a name that defines a particular combination of imagery, segmentation, and annotation database. For the MICrONs public data, we use the datastack name minnie65_public.\n\nfrom caveclient import CAVEclient\nfrom datetime import datetime, timezone\n\n# initialize cave client\nclient = CAVEclient('minnie65_public')\n\n# see the available materialization versions\nclient.materialize.get_versions()\n\n[1300, 1078, 117, 661, 343, 1181, 795, 943, 1412]\n\n\nAnd these are their associated timestamps (all timestamps are in UTC):\n\nfor version in client.materialize.get_versions():\n    print(f\"Version {version}: {client.materialize.get_timestamp(version)}\")\n\nVersion 1300: 2025-01-13 10:10:01.286229+00:00\nVersion 1078: 2024-06-05 10:10:01.203215+00:00\nVersion 117: 2021-06-11 08:10:00.215114+00:00\nVersion 661: 2023-04-06 20:17:09.199182+00:00\nVersion 343: 2022-02-24 08:10:00.184668+00:00\nVersion 1181: 2024-09-16 10:10:01.121167+00:00\nVersion 795: 2023-08-23 08:10:01.404268+00:00\nVersion 943: 2024-01-22 08:10:01.497934+00:00\nVersion 1412: 2025-04-29 10:10:01.200893+00:00\n\n\nYou can set the overall materialization version for the dataset using client.version. This will ensure all of the subsequent CAVE queries are performed at the same materialization, so you will get consistency between, for example, a cell type query and a synapse query.\n\n# set materialization version, for consistency\nclient.version = 1412 # current public as of 4/29/2025\n\nHowever, you can also set individual queries to a different version with optional argument materialization_version. For more about table queries, see CAVE Query Cell Types.\n\nnuc_v661 = client.materialize.tables.nucleus_detection_v0().query(materialization_version=661)\n\nnuc_v661.sample(3)\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n27095\n717226\n2020-09-28 22:41:36.353256+00:00\nNaN\nt\n340.089078\n118380363788982290\n864691135386337877\n[389712, 210880, 24904]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n143388\n164389\n2020-09-28 22:45:24.957212+00:00\nNaN\nt\n454.651412\n81435535339497985\n864691135475705920\n[121056, 201904, 18463]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n34048\n126135\n2020-09-28 22:41:48.617493+00:00\nNaN\nt\n158.299423\n78694108986133068\n864691135625910878\n[101216, 223936, 16893]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\nYou can even do the same thing with an arbitrary timestamp, using optional argument timestamp. However, due to how the ChunkedGraph operates, this will be more time-intensive than looking up a specific materialized version.\n\n%%time\nnuc_v661 = client.materialize.tables.nucleus_detection_v0().query(timestamp=datetime(2023,4,6,20))\n\nnuc_v661.sample(3)\n\nCPU times: total: 812 ms\nWall time: 59.7 s\n\n\n\n\n\n\n\n\n\nid\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n99694\n500984\nt\n245.666775\n103042450989740874\n864691136287472323\n[278528, 229328, 21380]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n110157\n563160\nt\n333.836780\n107894390181643519\n864691136974411804\n[313680, 203072, 25566]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n120527\n612800\nt\n291.528704\n113866387252299373\n864691135763026486\n[357008, 133440, 22939]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\n\nHow to set the timestamp to an expired version\nMaterialization versions expire at regular intervals. Indeed, every version between our major long-term public releases existed at some point, but has since expired.\nThis does not mean the data from those versions is gone.\nIt does mean it takes longer to materialize data from that date, because the chunkedgraph has to calculate differences between an extant materialized version and the requested time. In order to materialize data from an expired version, you must set the optional timestamp argument in every query:\n\n# set the timestamp of a version that may or may not exist\nexample_timestamp = datetime(2022, 2, 24, 8, 10, 0, 184668, tzinfo=timezone.utc)\n\n# example table query\nnuc_timestamp = client.materialize.tables.nucleus_detection_v0().query(timestamp=example_timestamp, limit=100)\nnuc_timestamp.head(3)\n\n201 - \"Limited query to 100 rows\n\n\n\n\n\n\n\n\n\nid\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n11294\nt\n49.112842\n73556435283294116\n864691135269406572\n[63968, 218032, 20683]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n11300\nt\n323.577446\n73626116832723681\n864691135151717168\n[64400, 213072, 20630]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n11301\nt\n277.511864\n0\n0\n[64496, 219104, 20408]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\n\n# example synapse query\nsyn_timestamp = client.materialize.synapse_query(post_ids=nuc_timestamp.pt_root_id, timestamp=example_timestamp)\nsyn_timestamp.head(3)\n\n\n\n\n\n\n\n\nid\nvalid\npre_pt_supervoxel_id\npre_pt_root_id\npost_pt_supervoxel_id\npost_pt_root_id\nsize\npre_pt_position\npost_pt_position\nctr_pt_position\n\n\n\n\n0\n507533913\nt\n120060554860852929\n864691132931221450\n120060554860853553\n0\n420\n[402320, 146520, 23899]\n[402354, 146594, 23900]\n[402350, 146558, 23899]\n\n\n1\n512239447\nt\n107692559937453733\n864691134764563389\n107692560004163297\n0\n1300\n[312342, 272446, 16895]\n[312272, 272510, 16899]\n[312298, 272460, 16898]\n\n\n4\n416272231\nt\n110295859874434808\n864691135385181117\n110295859874440074\n0\n1884\n[330842, 269600, 16654]\n[330878, 269532, 16662]\n[330854, 269538, 16663]\n\n\n\n\n\n\n\n\n\nTimestamps for all public release versions\nLong-term releases are made available for analysis, but are not permanent. You can lookup the timestamp associated with any version here.\n\nCAVEclient materialization version timestamps (as datetime.datetime objects)\n\n\n\n\n\n\nVersion\nTimestamp\n\n\n\n\n117\ndatetime(2021, 6, 11, 8, 10, 0, 215114, tzinfo=datetime.timezone.utc)\n\n\n343\ndatetime(2022, 2, 24, 8, 10, 0, 184668, tzinfo=datetime.timezone.utc)\n\n\n661\ndatetime(2023, 4, 6, 20, 17, 9, 199182, tzinfo=datetime.timezone.utc)\n\n\n795\ndatetime(2023, 8, 23, 8, 10, 1, 404268, tzinfo=datetime.timezone.utc)\n\n\n943\ndatetime(2024, 1, 22, 8, 10, 1, 497934, tzinfo=datetime.timezone.utc)\n\n\n1078\ndatetime(2024, 6, 5, 10, 10, 1, 203215, tzinfo=datetime.timezone.utc)\n\n\n1181\ndatetime(2024, 9, 16, 10, 10, 1, 121167, tzinfo=datetime.timezone.utc)\n\n\n1300\ndatetime(2025, 1, 13, 10, 10, 1, 286229, tzinfo=datetime.timezone.utc)\n\n\n1412\ndatetime(2025, 4, 29, 10, 10, 1, 200893, tzinfo=datetime.timezone.utc)",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Materialization and Versioning"
    ]
  },
  {
    "objectID": "materialization-version.html#how-to-cross-reference-across-time",
    "href": "materialization-version.html#how-to-cross-reference-across-time",
    "title": "Materialization and Versioning",
    "section": "How to cross-reference across time",
    "text": "How to cross-reference across time\n\nLineage Graphs\nCAVEclient combines materialized snapshots with ChunkedGraph-based tracking of neuron edit histories to facilitate analysis queries for arbitrary time points. The ChunkedGraph tracks the edit lineage of neurons as they are being proofread, allowing us to map any segment used in a query to the closest available snapshot time point. This produces an overinclusive set of segments with which we query the snapshot database.\n\n\n\na, Edits change the assignment of synapses to segment IDs. Each of the four synapses is assigned to the segment IDs (colors) according to the presynaptic and postsynaptic points (point, bar). The identity of the segments changes through proofreading (time passed: ΔT) indicated by different colors. The lineage graph shows the current segment ID (color) for each point in time.\n\n\n\n\nFigure from (Dorkenwald et al. 2025)\n\n\nWhen we query the ‘live’ database for all changes to annotations since the used materialization snapshot and add them to the set of annotations. The resulting set of annotations is then mapped back to the query timestamp using the lineage graph and supervoxel to root lookups and finally reduced to only include the queried set of root IDs.\n\n\n\nb, Analysis queries are not necessarily aligned to exported snapshots. Queries for other time points are supported by on-the-fly delta updates from both the annotations and segmentation through the use of the lineage graph.\n\n\n\n\nFigure from (Dorkenwald et al. 2025)\n\n\n\nExample querying the ChunkedGraph history for a root id\n\n\n\n\n\n\n\nInitial Setup\n\n\n\nBefore using any programmatic access to the data, you first need to set up your CAVEclient token.\n\n\n\nfrom caveclient import CAVEclient\nfrom datetime import datetime\n\n# initialize cave client\nclient = CAVEclient('minnie65_public')\n\n# return the current timestamp\nclient.materialize.get_timestamp()\n\ndatetime.datetime(2025, 4, 29, 10, 10, 1, 200893, tzinfo=datetime.timezone.utc)\n\n\nMost commonly, what you will want is to look-up the current root id for a pt_root_id in a previous analysis. This is not always a trivial thing to do, for example in the case of a multi-soma object that has been manually split. Which of the two new cells was your original cell of interest?\nThe ChunkedGraph will make its best guess, given supervoxel overlap, with the function suggest_latest_roots()\n\nexample_id = 864691135919440816\n\n# Access the ChunkedGraph service of caveclient\nclient.chunkedgraph.suggest_latest_roots(example_id, timestamp = client.materialize.get_timestamp())\n\nnp.int64(864691135970572133)\n\n\nNow we have updated pt_root_id for our cell, at the current materialized version.\nIf you want to run this for a large number of root ids, you can first check if the pt_root_ids are current to your CAVEclient materialization version using is_latest_roots(), and then only update the ids that have expired:\n\n# Check if roots are current\nprint(client.chunkedgraph.is_latest_roots(example_id))\n\n# See when the id was generated (when the segment was last edited)\nclient.chunkedgraph.get_root_timestamps(example_id)\n\n[False]\n\n\narray([datetime.datetime(2023, 2, 1, 8, 47, 29, 891000, tzinfo=&lt;UTC&gt;)],\n      dtype=object)\n\n\nUsing the timestamp argument, you can also lookup the suggested root at any arbitrary time. Here we use the timestamp for a different materialization, verion 943:\n\nclient.chunkedgraph.suggest_latest_roots(example_id, \n                                         timestamp=client.materialize.get_version_metadata(943)['time_stamp']\n                                        )\n\nnp.int64(864691135808631069)\n\n\nSometimes you may want to check the lineage graph for a cell of interest, to better understand what was edited and why. You can access this and more advanced features from the get_lineage_graph(). See the ChunkedGraph documentation for more use cases.\nclient.chunkedgraph.get_lineage_graph(example_id)\n\n\nStatic annotations\nThe CAVEclient Materialization Engine updates segmentation data and creates databases that combine spatial annotation points and segmentation information.\nThe live database is written to by the Annotation service and is actively managed by the Materialization service to keep root IDs up to date for all BoundSpatialPoints in all tables. Snapshotted databases are copies of a time-locked state of the ‘live’ database’s segmentation and annotation information used to facilitate consistent querying.\nThis means if you use a static annotation label to index your analysis, for example a nucleus_id or a synapse_id which do not undergo proofreading, you can look up the current pt_root_id at any time by asking CAVEclient to materialize the new segmentation under the static point.\nUsing our example cell from above, let’s find its nucleus id. If we try to query the nucleus table with the expired id, we will return no result:\n\nexample_id = 864691135919440816\n\nclient.materialize.tables.nucleus_detection_v0(pt_root_id=example_id).query()\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n\n\n\n\n\n\n\nThis is expected, since the example id is expired at the time of this materialization. Instead, let’s query the version we know this id existed at: version 661\n\nclient.materialize.tables.nucleus_detection_v0(pt_root_id=example_id).query(materialization_version=661)\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n260620\n2020-09-28 22:44:52.109195+00:00\nNaN\nt\n292.154409\n88605588438852384\n864691135919440816\n[173584, 145120, 21127]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\nThis returns both the expected pt_root_id 864691135919440816, and the id from the nucleus_detection_v0 – better known as the nucleus_id.\nGiven the nucleus_id, we can now query the current materialized version for the current root id.\n\nnuc_id = client.materialize.tables.nucleus_detection_v0(pt_root_id=example_id).query(materialization_version=661)['id']\n\nclient.materialize.tables.nucleus_detection_v0(id=nuc_id).query()\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n260620\n2020-09-28 22:44:52.109195+00:00\nNaN\nt\n292.154409\n88605588438852384\n864691135970572133\n[173584, 145120, 21127]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\nThis returns the same pt_root_id as the lineage graph example above. But, it has the benefit of guaranteeing the id belongs to your cell of interest and not an arbitrary chunk of the previous segmented object.\n\nIf you are working with a neuron, glial cell, or any cell that has a nucleus detection, we recommend using the nucleus_id as your identifier rather than the pt_root_id.\n\n\nIf you do use pt_root_id, be sure to note the dataset materialization version in your analysis.",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Materialization and Versioning"
    ]
  },
  {
    "objectID": "neuroglancer-basic.html",
    "href": "neuroglancer-basic.html",
    "title": "Neuroglancer",
    "section": "",
    "text": "Note\n\n\n\nNeuroglancer works best in Chrome and Firefox, and does not always work as expected in Safari.\nNeuroglancer is a WebGL-based viewer developed by Jeremy Maitin-Shephard at the Google Connectomics team to visualize very large volumetric data, designed in large part for connectomics. We often use Neuroglancer to quickly explore data, visualize results in context, and share data.\nTo look at the MICrONS data in Neuroglancer, click this link. Note that you will need to authenticate with the same Google-associated account that you use to set up CAVEclient.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "neuroglancer-basic.html#interface-basics",
    "href": "neuroglancer-basic.html#interface-basics",
    "title": "Neuroglancer",
    "section": "Interface Basics",
    "text": "Interface Basics\nThe Neuroglancer interface is divided into panels. In the default view, one panel shows the imagery in the X/Y plane (left), one shows a 3d view centered at the same location, and the narrow third panel provides information about the specific layer. Note that at the center of the each panel is a collection of axis-aligned red, blue and, green lines. The intersection and direction of each of these lines is consistent across all panels.\nAlong the top left of the view, you can see tabs with different names. Neuroglancer organizes data into layers, where each layer tells Neuroglancer about a different aspect of the data. The default view has three layers:\n\nimg describes how to render imagery.\nseg describes how to render segmentation and meshes.\nann is a manual annotation layer, allowing the user to add annotations to the data.\n\nYou can switch between layers by right clicking on the layer tab. You will see the panel at the right change to provide controls for each layer as you click it.\nThe collection of all layers, the user view, and all annotations is stored as a JSON object called the state.\nThe basic controls for navigation are:\n\nsingle click/drag slides the imagery in X/Y and rotates the 3d view.\nscroll wheel up/down moves the imagery in Z.\nright click jumps the 3d view to the clicked location in either the imagery or on a segmented object.\ndouble click selects a segmentation and loads its mesh into the 3d view. Double clicking on a selected neuron deselects it.\ncontrol-scrool zooms the view under the cursor in or out.\nz snaps the view to the closest right angle.\n\nYou can paste a position into Neuroglancer by clicking the x, y, z coordinate in the upper left corner and pasting a space or comma-separated list of numbers and hitting enter. Note that Neuroglancer always works in voxel units, and you can see the resolution of the voxels in the extreme upper left corner.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "neuroglancer-basic.html#selecting-objects",
    "href": "neuroglancer-basic.html#selecting-objects",
    "title": "Neuroglancer",
    "section": "Selecting objects",
    "text": "Selecting objects\nThe most direct way to select a neuron is to double click in the imagery to select the object under your cursor. This will load all the voxels associated with that object and also display its mesh in the 3d view.\nTo see the list of selected objects, you can select the segmentation tab (right click on the seg tab). Underneath the list of options, there is a list of selected root ids and the color assigned to them in the view. You can change colors of all neurons randomly by pressing l or individually change colors as desired. In addition, you can press the checkbox to hide a selected object while keeping it in the list, or deselect it by clicking on the number itself. You can also copy a root id by pressing the clipboard icon next to its number, or copy all selected root ids by pressing the clipboard icon above the list.\nThis selection list also allows you to select objects by pasting one or more root ids into the text box at the top of the list and pressing enter.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "neuroglancer-basic.html#annotations",
    "href": "neuroglancer-basic.html#annotations",
    "title": "Neuroglancer",
    "section": "Annotations",
    "text": "Annotations\nAnnotations are stored in an annotation layer. The default state has an annotation layer called ann, but you can always add new annotation layers by command-clicking the + button to the right of the layer tabs.\nTo create an annotation, select the layer (right click on the tab), and then click the icon representing the type of annotation you want to use. The most basic annotation is a point, which is the icon to the left of the list. The icon will change to having a green background when selected.\nNow if you control-click in either the imagery or the 3d view, you will create a point annotation at the clicked location. The annotation will appear in the list to the right, with its coordinate (in voxels, not nanometers) displayed. Clicking any annotation in the list will jump to that annotation in 3d space. Each annotation layer can have one color, which you can change with the color picker to the left of the annotation list.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "neuroglancer-basic.html#navigating-annotations",
    "href": "neuroglancer-basic.html#navigating-annotations",
    "title": "Neuroglancer",
    "section": "Navigating Annotations",
    "text": "Navigating Annotations\nAnnotations in an annotation layer can be right-clicked on to jump to them, but can also be navigated as a list.\n\nspelunkerneuroglancer.neuvuengl.microns-explorer\n\n\nTo see the list of annotations, select the annotation layer (right click on the tab).\nEach annotation is listed as its location in Neuroglancer voxel coordinates.\nClicking on an row in this annotation list will not only jump to it in the view, but also select it. You can see the information about the selected neuron in the lower right corner.\n\n\nTo see the list of annotations, select the annotation layer (right click on the tab).\nEach annotation is listed as its location in Neuroglancer voxel coordinates.\nClicking on an row in this annotation list will not only jump to it in the view, but also select it. You can see the information about the selected neuron in the lower right corner.\n\n\n\n\n\n\nImportant\n\n\n\nThe following sections are not accurate for spelunker version of neuroglancer, but do represent planned feature addtions.\n\n\nOnce an annotation is selected, any associated root ids are loaded. The keys [ and ] will jump to the previous and next annotations in the list, respectively.\nEach annotation can have a full-text description associated with it for adding notes. This can be added in the lower right corner.\n\n\n\nAnnotation tags after populating them manually.\n\n\nHowever, the most convenient way to label data quickly is through Tags.\nTo add tags, click on the Shortcuts tab within the Annotation widget on the right, and then click on the + button to add a new tag.\nEach tag gets a text label and a key command to activate or deactivate it for a given annotation.\nBy default, the first tag is activated by pressing shift-q, the second by pressing shift-w, and so on down the qwerty line.\n\n\n\nAnnotation tags applied to annotations. The labels with with the # symbol indicate a tag is present.\n\n\nNow when you select an annotation, you can press the key command to attach that tag to it.\nPressing the same key command will remove the tag.\nAny number of tags can be added to each annotation.\nTogether with the [ and ] keys to navigate the list, this allows you to quickly label a large number of annotations.\n\n\nTo see the list of annotations, select the annotation layer (right click on the tab).\nEach annotation is listed as its location in Neuroglancer voxel coordinates.\nClicking on an row in this annotation list will not only jump to it in the view, but also select it. You can see the information about the selected neuron in the lower right corner.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "neuroglancer-basic.html#saving-and-sharing-states",
    "href": "neuroglancer-basic.html#saving-and-sharing-states",
    "title": "Neuroglancer",
    "section": "Saving and sharing states",
    "text": "Saving and sharing states\nLike many other websites that require logins, you cannot simply send your URL ot another person to have them see the view. Instead, to save the current state and make it available to yourself or others in the future, you need to save the state with the Share button at the top right corner. This will then give you a URL that you can copy and share with others or paste yourself. A typical sharing URL looks like the following:\nhttps://neuroglancer.neuvue.io/?json_url=https://global.daf-apis.com/nglstate/api/v1/4684616269037568\nThe first part is the URL for the Neuroglancer viewer, while the part after the ?json_url= is a URL that points to a JSON file that contains the state. The number at the end of the URL is used to uniquely identify the state and can be used programatically to retrieve information.\n\n\n\n\n\n\nWarning\n\n\n\nIf a URL contains ?local_id= instead of ?json_url, that means that it cannot be viewed by anyone else or even in another browser on your own computer.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "programmatic_access/functional_tutorial/em_functional_intro.html",
    "href": "programmatic_access/functional_tutorial/em_functional_intro.html",
    "title": "MICrONs Tutorial",
    "section": "",
    "text": "(em:functional-data)= # MICrONS Functional Data\nThe MICrONs mouse went through a battery of functional imaging experiments before be prepared for electron microscopy. Excitatory neurons were imaged with 2-photon calcium imaging using GCaMP6s. Because of the size of the volume, different populations of neurons were imaged in different sessions, spanning several days. Each neuronal {term}`ROI`` from the functional data is uniquely determined based on the combination of image session, scan index, and ROI unit id. During each imaging session, the mouse was head-fixed and presented a variety of visual stimuli to the left visual field, including both natural movies and synthetic parametric movies. In addition to functional 2p imaging, the treadmill rotation, left eye position, and left pupil diameter were recorded to provide behavioral context."
  },
  {
    "objectID": "programmatic_access/functional_tutorial/em_functional_intro.html#stimuli-and-digital-twin",
    "href": "programmatic_access/functional_tutorial/em_functional_intro.html#stimuli-and-digital-twin",
    "title": "MICrONs Tutorial",
    "section": "Stimuli and “Digital twin”",
    "text": "Stimuli and “Digital twin”\nThe visual stimuli presented to the mouse included many natural movies, as well as parametric stimuli that were designed to emphasize specific visual features, such as orientation and spatial frequency. Because imaging sessions were performed over several days and imaging time in any one location was limited, comparisons between cells in different sessions are not straightforward. To get around this, the principle goal of this collection of stimuli was to be used as training data for deep neural networks that were trained to predict the response of each cell to an arbitrary stimulus. These so-called “digital twins” were designed to be used to probe functional responses to cells to stimuli that were outside the original training set. More details about this digital twin approach can be found in Wang et al. 2023.\nAs a measure of the reliability of visual responses, a collection of six movies totalling one minute were presented to the mouse ten times per imaging session. An oracle score was computed based on the signal correlation of a given cell to the oracle stimuli across the imaging session, where higher numbers indicate more reliable responses. Specifically, the oracle score is the mean signal correlation of the response of each presentation to the average of the other nine presentations."
  },
  {
    "objectID": "programmatic_access/functional_tutorial/em_functional_intro.html#coregistration",
    "href": "programmatic_access/functional_tutorial/em_functional_intro.html#coregistration",
    "title": "MICrONs Tutorial",
    "section": "Coregistration",
    "text": "Coregistration\nThe process of aligning the location of cells from the functional imaging with the same cells in the EM imaging is called coregistration. It is a challenging problem due to the need for micron-scale alignment of image volumes, despite signifiant differences in the imaging modalities, tissue deformations under different conditions and potential distortions introduced by sample preparation.\nThe dataset contains two approaches to coregistration, one semi-manual and one fully automatic. In the semi-manual approach, a transform between the EM data and the 2p data was generated based on fiducual points such as blood vessels and cell bodies. This transform was then applied to the functional data to identify a location in the EM space, and a human annotator then identified the cell body in the EM data that was a best match to the functional ROI based on location and context. In the fully automatic approach, blood vessels were segmented in both 2p and EM volumes, and a transform was generated based on matching this 3d structure. More details can be found in the MICrONS dataset preprint.\n\nCoregistration Quality Metrics\n\n\n\n```piwqsqds img/coreg-metrics.png\n\n\n\n\nalign: center\n\n\n\nCartoon illustrating the coregistration metrics.\n\nTwo values are available to help assess the quality of the coregistration.\nThe **residual** indicates the distance between the location of an ROI after transformation to the EM space and the location of the matched cell body in the EM data.\nThe **score** (or separation score) is the distance between the matched cell body and the nearest other cell body in the EM data.\nThis attempts to measure how the residual compares with the distance to other potential matches in the data.\nLarger values indicate fewer potential matches and therefore a more confident match, in general.\n\nWhen using the automated coregistration, it is important to filter the data based on assignment confidence.\nA guide for this can be found by comparing the subset of cells matched by both the automated and manual coregistration methods.\n\n```{figure} img/coreg-agreement.png\n---\nalign: center\n---\nRelationship between separation threshold (left) and residual (right) and the accuracy of automated coregistration compared to manual coregistration. Orange curves depict the fraction of cells that remain after filtering out those matches beyond the threshold indicated on the x-axis.\n\n\nMatching EM to Function\nThe combination of session index, scan index, and ROI unit id uniquely identifies each ROI in the functional data. The annotation database contains tables with the results of each of the coregistration methods. Each row in each table contains the nucleus id, centroid, and root ID of an EM cell as well as the scan/session/unit indices required to match it. In additoin, the residual and score metrics for each match are provided to filter by quality. For manual coregistration, the table is called coregistration_manual_v3 and for automated coregistration, the table is called apl_functional_coreg_forward_v5.\nFull column definitions can be found on Annotation Tables page."
  },
  {
    "objectID": "programmatic_access/functional_tutorial/em_functional_intro.html#functional-data",
    "href": "programmatic_access/functional_tutorial/em_functional_intro.html#functional-data",
    "title": "MICrONs Tutorial",
    "section": "Functional data",
    "text": "Functional data\nA collection of in silico model responses of neurons to a variety of visual stimuli has been precomputed. The data can be found in a collection of files:\n\n:header-rows: 1 * - Filename - Format - Info * - nat_movie.npy - numpy.ndarray - dimensions=[clip, frame, height, width] * - nat_resp.npy - numpy.ndarray - dimensions=[unit, bin] * - nat_unit.csv - importable into pandas.DataFrame - columns=[animal_id, scan_session, scan_idx, unit_id, row_idx] * - monet_resp.npy - numpy.ndarray - dimensions=[unit, direction, trial] * - monet_dir.npy - numpy.ndarray - dimensions=[direction] * - monet_unit.csv - importable into pandas.DataFrame - columns=[animal_id, scan_session, scan_idx, unit_id]\n\nThe model response data is organized into two sets of files, one for natural movies and one for Monet stimuli, a type of parametric stimulus that measures orientation tuning. The .npy files can be read with the numpy function np.load and the .csv files with the pandas function pd.read_csv.\n\n\n\n```piwqsqds img/function-stimulus.png\n\n\n\n\nalign: center\n\n\n\nExample images from a variety fo the stimuli used to probe functional responses. Natural movies include scenes from cinema, POV nature videos, and rendered 3d scenes. Monet stimuli are a parametic textured stimulus that varies in orientation and spatial frequency of correlated motion. ```\nThe natural movie data is organized into three files:\n\nnat_movie contains 250 10-sec clips of natural movies. Movies were shown to the model at a resolution of 72 * 128 pixels at 30 hz. This is downsampled from the in vivo presentation resolution of 144 * 256 pixels.\nnat_resp contains 104,171 units’ binned responses to natural movies. Responses were binned into 500 msec non-overlapping bins. This results in 250 clips * 10 sec / 500 msec = 5000 bins, with every binned response corresponding to 15 frames of natural movies.\nnat_unit contains the mapping from rows in nat_resp to functional unit keys. Since there is only a single animal for the EM data, use the session, scan, and unit id values to match to the EM data via one of the coregistration tables described above.\n\nThe Monet stimulus data is similarly organized into three files:\n\nmonet_resp contains 104,171 units’ mean responses to Monet stimuli. 120 trials of 16-direction Monet stimuli were shown to the model. The 120 trials were generated with different random seeds.\nmonet_dir contains the direction (in units of degrees) for the Monet stimulus shown to the model. The order of directions matches the order in the direction dimension of monet_resp. Here, 0 degrees is vertical feature moving leftwards, with degrees increases in the anti-clockwise direction.\nmonet_unit contains the functional unit keys of the 104,171 units. The order of the unit keys matches the order in the unit dimension of monet_resp. Since there is only a single animal for the EM data, use the session, scan, and unit id values to match to the EM data via one of the coregistration tables described above."
  },
  {
    "objectID": "proofreading.html",
    "href": "proofreading.html",
    "title": "Proofreading and Data Quality",
    "section": "",
    "text": "Version update\n\n\n\nThis tutorial has recently been updated to materialization version 1412 (from 1300).\nWe have released a new public version 1412, as part of our quarterly release schedule. See details at Release Manifests: 1412.\nAutomated segmentation of neuronal processes in dense EM imaging is challenging at the size of entire neurons, which can have millimeters of axons and dendrites. The automated segmentation algorithms used in the EM data for this project are not perfect, and so proofreading is necessary to correct errors in the segmentation.\nHowever, while automated proofreading methods are starting to appear, in general the proofreading process is time consuming and involves a lot of manual attention. A concerted effort was made to do proofreading on a subset of neurons within the dataset, and so some cells can be understood to be more biologically accurate and complete than others. Some cells in the MICrONs data are thus very well proofread, others are virtually untouched, and many are somewhere in between.\nEach kind of cell can be useful, but different approaches must be taken for each.\nIt is useful to distinguish between two kinds of proofreading errors: splits and merges.\nThese two kinds of errors impact analysis differently. While split errors reduce the number of correct synaptic connections, merge errors add incorrect connections.\nThe frequency of errors is roughly related to the size of the process, and thus axons and dendrites have extremely different error profiles. Because they are thicker, dendrites are largely well-segmented, with only a few split errors and most merge errors being with small axons rather than other dendrites.\nEven without proofreading a neuron’s dendrites, we thus expect that most of the input synapses onto the dendrites are correct (since the vast majority of merge errors are with axons that only have output synapses), though some may be missing due to split errors at tips or missing dendritic spines.\nIn contrast, axons, because they are thinner, have many more errors of both kinds. We thus do not trust axonal connectivity at all without additional proofreading.",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Proofreading and Data Quality"
    ]
  },
  {
    "objectID": "proofreading.html#proofreading-categories",
    "href": "proofreading.html#proofreading-categories",
    "title": "Proofreading and Data Quality",
    "section": "Proofreading categories",
    "text": "Proofreading categories\nProofreading is not an all-or-nothing process. Because of the reasons described above, we distinguish between the proofread status of the axon and the dendrite for each neuron separately. In addition, we consider three levels of proofreading:\n\nUnproofread: The arbor (dendrite or axon) has not been comprehensively proofread, although edits may have happened here and there.\nClean: The arbor (dendrite or axon) has been comprehensively proofread to remove all merge errors, but may still have split errors. This means that the synapses that are there are correct, but incomplete.\nExtended: The arbor (dendrite or axon) has been comprehensively proofread to remove all merge and split errors. This means that the synapses that are there are correct and as-complete-as-possible by expert proofreaders. Note that even a well-proofread neuron often has some tips that cannot be followed due to the limitations of the underlying imagery, and of course processes may leave the segmented volume.\n\nThe upshot is that unproofread axons are not of a quality suitable for analysis, you can trust the targets of a clean or extended axon because the default dendrite quality is good. However, if your analysis truly demands knowing if a particular neuron is connected to another neuron or not (rather than connecting to a population), the proofreading standards are particularly high and possibly require additional checks.\n\nProofreading status\n\n\n\n\n\n\n\nPre Axon Status\nPost Dendrite Status\nAnalyzability\n\n\n\n\nUnproofread\nUnproofread\nNot analyzable\n\n\nUnproofread\nClean / Extended\nNot analyzable\n\n\nClean\nUnproofread / Clean / Extended\nAnalyzable but connectivity could be highly biased by limited axonal extent\n\n\nExtended\nUnproofread\nAnalyzable and connectivity is close to complete\n\n\n\nSince a single proofread axon can have many hundreds to ten thousand synaptic outputs, the vast majority onto local dendrites, each single axon provides a tremendous amount of data about that cell’s postsynaptic targets.",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Proofreading and Data Quality"
    ]
  },
  {
    "objectID": "proofreading.html#proofreading-efforts",
    "href": "proofreading.html#proofreading-efforts",
    "title": "Proofreading and Data Quality",
    "section": "Proofreading efforts",
    "text": "Proofreading efforts\nThere have been several different proofreading efforts in the MICrONS data with different goals and levels of comprehensiveness. Importantly, neurons were not selected for proofreading randomly, but rather chosen based on various criteria. A few of the most signifcant proofreading efforts are described below.\n\nExcitatory neuron proofreading for functional connectomics. Excitatory neurons in retinotopically matched ares of VISp and RL were proofread to enable functional connectomics analysis, looking for the relationship between functional similarity and synaptic connectivity.\nMartinotti cell proofreading. Extremely extensive reconstructions of several layer 5 Martinotti cells in VISp were performed to enable analysis of the relationship between morphology and transcriptomics.\nMinnie V1 Column. A 100 micron square area of VISp for a comprehensive census of its neuronal population across all layers. Originally, excitatory neurons in this column had dendritic cleaning and extension and inhibitory neurons had comprehensive cleaning and substantail but incomplete extension. However, this population has become a hub for proofreading and cell typing, and will be discussed further in the section below.\n\nThe NIH-funded Virtual Observatory of the Cortex (VORTEX) program supports continued proofreading efforts. Supported efforts include proofreading of: of excitatory functionally-coregistered cells, inhibitory neurons, and glial cell types including astrocytes and oligodendrocyte precursor cells.\nTo learn more about our supported efforts, see: Virtual Observatory of the Cortex on MICrONS Tutorials.\nAccess our request submission system at:\nmicrons-explorer.org/requests | Scientific Request Form\nWe are also conducting a community survey. If you are using, have used, or plan to use the MICrONS data, give us some feedback!\nMICrONS Community Survey",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Proofreading and Data Quality"
    ]
  },
  {
    "objectID": "proofreading.html#proofreading-tables",
    "href": "proofreading.html#proofreading-tables",
    "title": "Proofreading and Data Quality",
    "section": "Proofreading tables",
    "text": "Proofreading tables\nTo keep track of the state of proofreading, the table proofreading_status_and_strategy in the annotation database has a row for each proofread neuron and provides a summary of the proofreading status of the axon and dendrite.\nTable name: proofreading_status_and_strategy\nThe table proofreading_status_and_strategy describes the status of cells that have undergone manual proofreading.\nBecause of the inherent difference in the difficulty and time required for different kinds of proofreading, we describe the status of axons and dendrites separately.\nEach compartment status may be either:\n\nFALSE: indicates no comprehensive proofreading has been performed, or is not applicable.\n\nTRUE: indicates that false merges have been comprehensively removed, and the compartment is at least ‘clean’. Consult the strategy column if completeness of the compartment is relevant to your research.\n\nAn axon or dendrite labeled as status=TRUE can be trusted to be correct, but may not be complete. The degree of completion can be read from the strategy column. For more information, please see Proofreading and Data Quality; or also the microns-explorer page on proofreading strategies.\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made. NOTE: if this does not match the pt_root_id then the cell has undergone further changes. This is usually and improvement in proofreading, but proceed with caution.\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. May be TRUE or FALSE\n\n\nstatus_axon\nThe status of the axon proofreading. May be TRUE or FALSE\n\n\nstrategy_dendrite\nThe strategy employed to proofread the dendrite. See strategy table below for details\n\n\nstrategy_axon\nThe strategy employed to proofread the axon. See strategy table below for details\n\n\n\nThe specific strategies are as follows (and will update over time):\n\nProofreading Strategies\n\nProofreading Strategy Table\n\n\n\n\n\n\nStrategy\nDescription\n\n\n\n\nnone\nNo cleaning, and no extension. Indicates an entry in proofreading_status that is FALSE for that compartment\n\n\ndendrite_clean\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. The dendrite may be incorrectly truncated by segmentation error. Not all dendrite tips have been checked for extension. No comprehensive attempt was made to re-attach spine heads.\n\n\ndendrite_extended\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. Every tip was identified, manually inspected, and extended if possible. No comprehensive attempt was made to re-attach spine heads.\n\n\naxon_column_truncated\nAThe axon was extended within the V1 cortical column, with a preference for local connections. In some cases the axon was cut at the column boundary and/or the layer boundary, especially the boundary between layers 2/3 and layer 4. Output synapses represent a sampling of potential partners\n\n\naxon_interareal\nThe axon was extended with a preference for branches that projected to other brain areas. Some axon branches were fully extended, but local connections may be incomplete. Output synapses represent a sampling of potential partners.\n\n\naxon_partially_extended\nThe axon was extended outward from the soma, following each branch to its termination. Output synapses represent a sampling of potential partners.\n\n\naxon_fully_extended\nAxon was extended outward from the soma, following each branch to its termination. After initial extension, every endpoint was identified, manually inspected, and extended again if possible. Output synapses represent a largely complete sampling of partners.\n\n\n\nThis table, proofreading_status_and_strategy, supercedes proofreading_status_public_release.\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Proofreading and Data Quality"
    ]
  },
  {
    "objectID": "proofreading.html#minnie-column",
    "href": "proofreading.html#minnie-column",
    "title": "Proofreading and Data Quality",
    "section": "Minnie Column",
    "text": "Minnie Column\nThe Minnie Column is a collection of more than 1,300 neurons whose nucleus centroid fell within a 100 x 100 micron square (viewed from the top) and extending across all layers.\n\n\n\n\n\nLocation of the Minnie column (yellow) within the dataset. Top view.\n\n\n\n\n\n\n\nReconstruction of the V1 column cells\n\n\n\n\nThis was originally considered a region of interest for “Inhibitory specificity from a connectomic census of mouse visual cortex” (Schneider-Mizell et al. 2025), and thus all neurons in this region were proofread to some extent and manually evaulated.\nAll excitatory neurons had their dendrites cleaned and extended to assess dendritic morphology and synaptic inputs, while inhibitory neurons had both their dendrites cleaned and extended, and then their axons were extensively cleaned and extended to capture their targeting properties. Efforts were made to extend inhibitory axons to every major region of their arbor, but not every individual tip was followed and the resulting analysis focused on population-level connectivity rather than individual connections.\nMorphological and connectivity-based cell typing was performed on these cells in terms of both classical and novel categories, and considerable expert attention ensured that the baseline data quality was high. Cell type results can be found in several of the cell type tables available. These cell types were used for training the various whole-dataset cell typing classifiers.\nIn addition, the proofreading and multiple levels of cell typing applied to this dense and diverse population has made it a useful reference, and spawned further analysis. Subsequent work has cleaned many of the excitatory axons from layers 2–5, albeit to a variety of degrees of extension.",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Proofreading and Data Quality"
    ]
  },
  {
    "objectID": "quickstart_notebooks/00_cave_quickstart.html",
    "href": "quickstart_notebooks/00_cave_quickstart.html",
    "title": "CAVE Quickstart",
    "section": "",
    "text": "The Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data."
  },
  {
    "objectID": "quickstart_notebooks/00_cave_quickstart.html#caveclient",
    "href": "quickstart_notebooks/00_cave_quickstart.html#caveclient",
    "title": "CAVE Quickstart",
    "section": "CAVEclient",
    "text": "CAVEclient\nMost programmatic access to the CAVE services occurs through CAVEclient, a Python client to access various types of data from the online services.\nFull documentation for CAVEclient is available here.\nTo initialize a caveclient, we give it a datastack, which is a name that defines a particular combination of imagery, segmentation, and annotation database. For the MICrONs public data, we use the datastack name minnie65_public.\n\nfrom caveclient import CAVEclient\ndatastack_name = 'minnie65_public'\nclient = CAVEclient(datastack_name)\n\n# Show the description of the datastack\nclient.info.get_datastack_info()['description']\n\n'This is the publicly released version of the minnie65 volume and segmentation. '\n\n\n\nMaterialization versions\nData in CAVE is timestamped and periodically versioned - each (materialization) version corresponds to a specific timestamp. Individual versions are made publicly available. The materialization service provides annotation queries to the dataset. It is available under client.materialize.\nPeriodic updates are made to the public datastack, which will include updates to the available tables. Some cells will have different pt_root_id because they have undergone proofreading.\nIt is worth checking the version of the data you are using, and specifying the version for analysis consistency.\n\n# see the available materialization versions\nclient.materialize.get_versions()\n\n[1078, 117, 661, 343, 1181, 795, 943]\n\n\nAnd these are their associated timestamps (all timestamps are in UTC):\n\nfor version in client.materialize.get_versions():\n    print(f\"Version {version}: {client.materialize.get_timestamp(version)}\")\n\nVersion 1078: 2024-06-05 10:10:01.203215+00:00\nVersion 117: 2021-06-11 08:10:00.215114+00:00\nVersion 661: 2023-04-06 20:17:09.199182+00:00\nVersion 343: 2022-02-24 08:10:00.184668+00:00\nVersion 1181: 2024-09-16 10:10:01.121167+00:00\nVersion 795: 2023-08-23 08:10:01.404268+00:00\nVersion 943: 2024-01-22 08:10:01.497934+00:00\n\n\n\n# set materialization version, for consistency\nmaterialization = 1181 # current public as of 9/16/2024\nclient.version = materialization"
  },
  {
    "objectID": "quickstart_notebooks/00_cave_quickstart.html#caveclient-basics",
    "href": "quickstart_notebooks/00_cave_quickstart.html#caveclient-basics",
    "title": "CAVE Quickstart",
    "section": "CAVEclient Basics",
    "text": "CAVEclient Basics\nThe most frequent use of the CAVEclient is to query the database for annotations like synapses. All database functions are under the client.materialize property. To see what tables are available, use the get_tables function:\n\nclient.materialize.get_tables()\n\n['nucleus_alternative_points',\n 'allen_column_mtypes_v2',\n 'bodor_pt_cells',\n 'aibs_metamodel_mtypes_v661_v2',\n 'allen_v1_column_types_slanted_ref',\n 'aibs_column_nonneuronal_ref',\n 'nucleus_ref_neuron_svm',\n 'apl_functional_coreg_vess_fwd',\n 'vortex_compartment_targets',\n 'baylor_log_reg_cell_type_coarse_v1',\n 'functional_properties_v3_bcm',\n 'l5et_column',\n 'pt_synapse_targets',\n 'coregistration_auto_phase3_fwd_apl_vess_combined',\n 'coregistration_manual_v4',\n 'vortex_manual_myelination_v0',\n 'synapses_pni_2',\n 'nucleus_detection_v0',\n 'vortex_manual_nodes_of_ranvier',\n 'vortex_astrocyte_proofreading_status',\n 'bodor_pt_target_proofread',\n 'nucleus_functional_area_assignment',\n 'coregistration_auto_phase3_fwd',\n 'synapse_target_structure',\n 'proofreading_status_and_strategy',\n 'aibs_metamodel_celltypes_v661']\n\n\nFor each table, you can see the metadata describing that table. For example, let’s look at the nucleus_detection_v0 table:\n\nclient.materialize.get_table_metadata('nucleus_detection_v0')\n\n{'aligned_volume': 'minnie65_phase3',\n 'created': '2020-11-02T18:56:35.530100',\n 'id': 45664,\n 'schema': 'nucleus_detection',\n 'table_name': 'nucleus_detection_v0',\n 'valid': True,\n 'schema_type': 'nucleus_detection',\n 'user_id': '121',\n 'description': 'A table of nuclei detections from a nucleus detection model developed by Shang Mu, Leila Elabbady, Gayathri Mahalingam and Forrest Collman. Pt is the centroid of the nucleus detection. id corresponds to the flat_segmentation_source segmentID. Only included nucleus detections of volume&gt;25 um^3, below which detections are false positives, though some false positives above that threshold remain. ',\n 'notice_text': None,\n 'reference_table': None,\n 'flat_segmentation_source': 'precomputed://https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/nuclei',\n 'write_permission': 'PRIVATE',\n 'read_permission': 'PUBLIC',\n 'last_modified': '2022-10-25T19:24:28.559914',\n 'segmentation_source': '',\n 'pcg_table_name': 'minnie3_v1',\n 'last_updated': '2024-10-24T22:00:00.145632',\n 'voxel_resolution': [4.0, 4.0, 40.0]}\n\n\nYou get a dictionary of values. Two fields are particularly important: the description, which offers a text description of the contents of the table and voxel_resolution which defines how the coordinates in the table are defined, in nm/voxel.\n\n\n\n\n\n\nAnnotation tables\n\n\n\nYou can also find a semantic description of the most commonly used tables at the Annotation Tables page."
  },
  {
    "objectID": "quickstart_notebooks/00_cave_quickstart.html#querying-tables",
    "href": "quickstart_notebooks/00_cave_quickstart.html#querying-tables",
    "title": "CAVE Quickstart",
    "section": "Querying Tables",
    "text": "Querying Tables\nTo get the contents of a table, use the query_table function. This will return the whole contents of a table without any filtering, up to for a maximum limit of 200,000 rows. The table is returned as a Pandas DataFrame and you can immediately use standard Pandas function on it.\n\ncell_type_df = client.materialize.query_table('nucleus_detection_v0')\ncell_type_df.head()\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n730537\n2020-09-28 22:40:41.780734+00:00\nNaN\nt\n32.307937\n0\n0\n[381312, 273984, 19993]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n373879\n2020-09-28 22:40:41.781788+00:00\nNaN\nt\n229.045043\n96218056992431305\n864691136090135607\n[228816, 239776, 19593]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n601340\n2020-09-28 22:40:41.782714+00:00\nNaN\nt\n426.138010\n0\n0\n[340000, 279152, 20946]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n201858\n2020-09-28 22:40:41.783784+00:00\nNaN\nt\n93.753836\n84955554103121097\n864691135373893678\n[146848, 213600, 26267]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n600774\n2020-09-28 22:40:41.785273+00:00\nNaN\nt\n135.189791\n0\n0\n[339120, 276112, 19442]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nWhile most tables are small enough to be returned in full, the synapse table has hundreds of millions of rows and is too large to download this way\n\n\nTables have a collection of columns, some of which specify point in space (columns ending in _position), some a root id (ending in _root_id), and others that contain other information about the object at that point. Before describing some of the most important tables in the database, it’s useful to know about a few advanced options that apply when querying any table.\n\ndesired_resolution : This parameter allows you to convert the columns specifying spatial points to different resolutions. Many tables are stored at a resolution of 4x4x40 nm/voxel, for example, but you can convert to nanometers by setting desired_resolution=[1,1,1].\nsplit_positions : This parameter allows you to split the columns specifying spatial points into separate columns for each dimension. The new column names will be the original column name with _x, _y, and _z appended.\nselect_columns : This parameter allows you to get only a subset of columns from the table. Once you know exactly what you want, this can save you some cleanup.\nlimit : This parameter allows you to limit the number of rows returned. If you are just testing out a query or trying to inspect the kind of data within a table, you can set this to a small number to make sure it works before downloading the whole table. Note that this will show a warning so that you don’t accidentally limit your query when you don’t mean to.\n\nFor example, using all of these together:\n\ncell_type_df = client.materialize.query_table('nucleus_detection_v0', split_positions=True, desired_resolution=[1,1,1], select_columns=['pt_position', 'pt_root_id'], limit=10)\ncell_type_df\n\n\n\n\n\n\n\n\npt_position_x\npt_position_y\npt_position_z\npt_root_id\n\n\n\n\n0\n241856.0\n374464.0\n838720.0\n0\n\n\n1\n227200.0\n389120.0\n797160.0\n0\n\n\n2\n230144.0\n422336.0\n795320.0\n0\n\n\n3\n239488.0\n386432.0\n794120.0\n0\n\n\n4\n239744.0\n423488.0\n803120.0\n864691136050815731\n\n\n5\n245888.0\n384512.0\n800120.0\n0\n\n\n6\n249792.0\n391680.0\n807080.0\n0\n\n\n7\n243328.0\n403008.0\n794280.0\n0\n\n\n8\n247872.0\n386816.0\n805320.0\n0\n\n\n9\n260352.0\n416640.0\n802360.0\n864691135013273238"
  },
  {
    "objectID": "quickstart_notebooks/00_cave_quickstart.html#filtering-queries",
    "href": "quickstart_notebooks/00_cave_quickstart.html#filtering-queries",
    "title": "CAVE Quickstart",
    "section": "Filtering Queries",
    "text": "Filtering Queries\nFiltering tables so that you only get data about certain rows back is a very common operation. While there are filtering options in the query_table function (see documentation for more details), a more unified filter interface is available through a “table manager” interface.\nRather than passing a table name to the query_table function, client.materialize.tables has a subproperty for each table in the database that can be used to filter that table.\nThe general pattern for usage is\nclient.materialize.tables.{table_name}({filter options}).query({format and timestamp options})\nwhere {table_name} is the name of the table you want to filter, {filter options} is a collection of arguments for filtering the query, and {format and timestamp options} are those parameters controlling the format and timestamp of the query.\nFor example, let’s look at the table aibs_metamodel_celltypes_v661, which has cell type predictions across the dataset. We can get the whole table as a DataFrame:\n\ncell_type_df = client.materialize.tables.aibs_metamodel_celltypes_v661().query()\ncell_type_df.head()\n\n\n\n\n\n\n\n\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\nid_ref\ncreated_ref\nvalid_ref\ntarget_id\nclassification_system\ncell_type\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n336365\n2020-09-28 22:42:48.966292+00:00\nt\n272.488202\n93606511657924288\n864691136274724621\n36916\n2023-12-19 22:47:18.659864+00:00\nt\n336365\nexcitatory_neuron\n5P-IT\n[209760, 180832, 27076]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n110648\n2020-09-28 22:45:09.650639+00:00\nt\n328.533443\n79385153184885329\n864691135489403194\n1070\n2023-12-19 22:38:00.472115+00:00\nt\n110648\nexcitatory_neuron\n23P\n[106448, 129632, 25410]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n112071\n2020-09-28 22:43:34.088785+00:00\nt\n272.929423\n79035988248401958\n864691136147292311\n1099\n2023-12-19 22:38:00.898837+00:00\nt\n112071\nexcitatory_neuron\n23P\n[103696, 149472, 15583]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n197927\n2020-09-28 22:43:10.652649+00:00\nt\n91.308851\n84529699506051734\n864691136050858227\n13259\n2023-12-19 22:41:14.417986+00:00\nt\n197927\nnonneuron\noligo\n[143600, 186192, 26471]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n198087\n2020-09-28 22:41:36.677186+00:00\nt\n161.744978\n83756261929388963\n864691135809440972\n13271\n2023-12-19 22:41:14.685474+00:00\nt\n198087\nnonneuron\nastrocyte\n[137952, 190944, 27361]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\nand we can add similar formatting options as in the last section to the query function:\n\ncell_type_df = client.materialize.tables.aibs_metamodel_celltypes_v661().query(split_positions=True, desired_resolution=[1,1,1], select_columns=['pt_position', 'pt_root_id', 'cell_type'], limit=10)\ncell_type_df\n\n\n\n\n\n\n\n\ncell_type\npt_position_x\npt_position_y\npt_position_z\npt_root_id\n\n\n\n\n0\n23P\n257600.0\n487936.0\n802760.0\n864691135724233643\n\n\n1\n23P\n260992.0\n493568.0\n801560.0\n864691136436395166\n\n\n2\nNGC\n256256.0\n466432.0\n831040.0\n864691135462260637\n\n\n3\n23P\n255744.0\n480640.0\n833200.0\n864691136723556861\n\n\n4\n23P\n262144.0\n505856.0\n824880.0\n864691135776658528\n\n\n5\n23P\n257536.0\n521728.0\n804440.0\n864691135941166708\n\n\n6\n23P\n251840.0\n552896.0\n832320.0\n864691135545065768\n\n\n7\n23P\n251136.0\n546048.0\n821320.0\n864691135479369926\n\n\n8\n23P\n256000.0\n626368.0\n814000.0\n864691135697633557\n\n\n9\nastrocyte\n324096.0\n417920.0\n658880.0\n864691135937358133\n\n\n\n\n\n\n\nHowever, now we can also filter the table to get only cells that are predicted to have cell type \"BC\" (for “basket cell”).\n\nmy_cell_type = \"BC\"\nclient.materialize.tables.aibs_metamodel_celltypes_v661(cell_type=my_cell_type).query()\n\n\n\n\n\n\n\n\nid_ref\ncreated_ref\nvalid_ref\ntarget_id\nclassification_system\ncell_type\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n43009\n2023-12-19 22:48:53.577191+00:00\nt\n369908\ninhibitory_neuron\nBC\n369908\n2020-09-28 22:40:41.814964+00:00\nt\n332.862751\n96002690286851358\n864691136522768017\n[227104, 207840, 20841]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n12051\n2023-12-19 22:40:57.133228+00:00\nt\n193846\ninhibitory_neuron\nBC\n193846\n2020-09-28 22:40:41.897904+00:00\nt\n306.148966\n82838443188669165\n864691135684976823\n[131568, 168496, 16452]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n83044\n2023-12-19 22:58:50.269173+00:00\nt\n615735\ninhibitory_neuron\nBC\n615735\n2020-09-28 22:40:41.957345+00:00\nt\n314.539540\n112181247505371364\n864691136311774525\n[344880, 161104, 17084]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n48718\n2023-12-19 22:50:21.192138+00:00\nt\n401681\ninhibitory_neuron\nBC\n401681\n2020-09-28 22:40:42.066718+00:00\nt\n497.801462\n98465046644219429\n864691136052141043\n[245232, 203952, 21268]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n82324\n2023-12-19 22:58:39.896999+00:00\nt\n613047\ninhibitory_neuron\nBC\n613047\n2020-09-28 22:40:41.982376+00:00\nt\n242.159780\n113234168401651200\n864691136065413528\n[352688, 141616, 25312]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3360\n8968\n2023-12-19 22:40:09.246333+00:00\nt\n170777\ninhibitory_neuron\nBC\n170777\n2020-09-28 22:45:25.310708+00:00\nt\n499.103662\n81230957054577082\n864691135065994564\n[119600, 250560, 15373]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3361\n15548\n2023-12-19 22:41:48.382554+00:00\nt\n208056\ninhibitory_neuron\nBC\n208056\n2020-09-28 22:45:25.401800+00:00\nt\n521.621668\n84540007091735344\n864691135801456226\n[143472, 262944, 23693]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3362\n79472\n2023-12-19 22:57:53.993099+00:00\nt\n591219\ninhibitory_neuron\nBC\n591219\n2020-09-28 22:45:25.526753+00:00\nt\n567.517839\n110216764830845707\n864691135279126177\n[330320, 204752, 25060]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3363\n55791\n2023-12-19 22:52:02.582669+00:00\nt\n438586\ninhibitory_neuron\nBC\n438586\n2020-09-28 22:45:25.430745+00:00\nt\n529.501389\n99807894274485381\n864691135395662581\n[254912, 247440, 23680]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3364\n50504\n2023-12-19 22:50:48.576826+00:00\nt\n419363\ninhibitory_neuron\nBC\n419363\n2020-09-28 22:45:25.436862+00:00\nt\n530.642698\n99716496901116512\n864691136691390838\n[254416, 90336, 20469]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n3365 rows × 15 columns\n\n\n\nor maybe we just want the cell types for a particular collection of root ids:\n\nmy_root_ids = [864691135771677771, 864691135560505569, 864691136723556861]\nclient.materialize.tables.aibs_metamodel_celltypes_v661(pt_root_id=my_root_ids).query()\n\n\n\n\n\n\n\n\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\nid_ref\ncreated_ref\nvalid_ref\ntarget_id\nclassification_system\ncell_type\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n19116\n2020-09-28 22:41:51.767906+00:00\nt\n301.426115\n74737997899501359\n864691135771677771\n11282\n2023-12-19 22:40:43.249642+00:00\nt\n19116\nexcitatory_neuron\n23P\n[72576, 108656, 20291]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n21783\n2020-09-28 22:41:59.966574+00:00\nt\n263.637074\n75795590176519004\n864691135560505569\n15681\n2023-12-19 22:41:50.365399+00:00\nt\n21783\nexcitatory_neuron\n23P\n[80128, 124000, 16563]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n4074\n2020-09-28 22:42:41.341179+00:00\nt\n313.678234\n73543309863605007\n864691136723556861\n50080\n2023-12-19 22:50:42.474168+00:00\nt\n4074\nexcitatory_neuron\n23P\n[63936, 120160, 20830]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\nYou can get a list of all parameters than be used for querying with the standard IPython/Jupyter docstring functionality, e.g. client.materialize.tables.aibs_metamodel_celltypes_v661.\n\n\n\n\n\n\nCaution\n\n\n\nUse of this functionality will show a brief warning that the interface is experimental. This is because the interface is still being developed and may change in the near future in response to user feedback."
  },
  {
    "objectID": "quickstart_notebooks/00_cave_quickstart.html#querying-proofread-neurons",
    "href": "quickstart_notebooks/00_cave_quickstart.html#querying-proofread-neurons",
    "title": "CAVE Quickstart",
    "section": "Querying Proofread neurons",
    "text": "Querying Proofread neurons\n\nProofread neurons\nProofreading is necessary to obtain accurate reconstructions of a cell. In the MICrONS dataset, the general rule is that dendrites onto cells with a single cell body are sufficiently proofread to trust synaptic connections onto a cell. Axons on the other hand require so much proofread that only ~1,000 cells have axons that were proofread to various degrees such that their outputs can be used for analysis.\nThe table proofreading_status_and_strategy contains proofreading information about ~1,300 neurons. This website provides the most detailed overview. In brief, axons annotated with any strategy_axon were cleaned of false mergers but not all were fully extended. The most important distinction is axons annotated with axon_column_truncated were only proofread within a certain volume wheras others were proofread without such bias.\n\nproof_all_df = client.materialize.query_table(\"proofreading_status_and_strategy\", desired_resolution=[1, 1, 1], split_positions=True)\n\n\nproof_all_df[\"strategy_axon\"].value_counts()\n\nstrategy_axon\naxon_partially_extended    979\naxon_column_truncated      233\nnone                       185\naxon_interareal            144\naxon_fully_extended         80\nName: count, dtype: int64\n\n\nWe can filter our query to only return rows that match a condition by adding a filter to our query:\n\nproof_df = client.materialize.query_table(\"proofreading_status_and_strategy\", filter_in_dict={\"strategy_axon\": [\"axon_partially_extended\", \"axon_fully_extended\", \"axon_interareal\", \"axon_column_truncated\"]}, desired_resolution=[1, 1, 1], split_positions=True)\n\n\nproof_df[\"strategy_axon\"].value_counts()\n\nstrategy_axon\naxon_column_truncated      598\naxon_partially_extended    341\naxon_interareal            146\naxon_fully_extended         77\nName: count, dtype: int64"
  },
  {
    "objectID": "quickstart_notebooks/01-caveclient-setup.html",
    "href": "quickstart_notebooks/01-caveclient-setup.html",
    "title": "CAVEclient Setup",
    "section": "",
    "text": "The Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data.\nThe CAVEclient is the main way to programmatically access the MICrONS data using Python.\nIn particular, the CAVEclient provides an interface to query the CAVE database for annotations such as synapses, as well as to get a variety of other kinds of information.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVEclient Setup"
    ]
  },
  {
    "objectID": "quickstart_notebooks/01-caveclient-setup.html#installation",
    "href": "quickstart_notebooks/01-caveclient-setup.html#installation",
    "title": "CAVEclient Setup",
    "section": "Installation",
    "text": "Installation\nTo install caveclient, use pip: pip install caveclient.\nOnce you have installed caveclient, to use it you need to set up your user token in one of two ways:",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVEclient Setup"
    ]
  },
  {
    "objectID": "quickstart_notebooks/01-caveclient-setup.html#setting-up-credentials",
    "href": "quickstart_notebooks/01-caveclient-setup.html#setting-up-credentials",
    "title": "CAVEclient Setup",
    "section": "Setting up credentials",
    "text": "Setting up credentials\nTo access the data programmatically, you need to set up a user token. This token is assigned by the server and functions as a both a username and password to access any aspect of the data. You will need to save this token to your computer using the tools.\n\nScenario 1: New User, No Previous Account\nIf you have never interacted with CAVE before, you will need to both create an account and get a token. Note that you can only have one token at a time, and thus if you create a new token any computer running a previous one will no longer have access.\nStep 1: Log into a CAVE site to set up a new account with a GMail-associated email address. To do this, click this link and acknowledge the terms of service associated with the MICrONS dataset. Once you have done this, your account is automatically created.\nStep 2: Generate a token. To generate a token, run the following code in a Jupyter notebook:\n\nfrom caveclient import CAVEclient\nclient = CAVEclient()\nclient.auth.setup_token(make_new=True)\n\nThis will open a new browser window and ask you to log in.\nYou will show you a web page with an alphanumeric string that is your token.\nCopy your token, and save it to your computer using the following:\n\nclient.auth.save_token(token=YOUR_TOKEN)\n\nNote that the token must be formatted as a string.\nTo check if your setup works, run the following:\n\nclient = CAVEclient('minnie65_public')\n\nIf you don’t get any errors, your setup has worked and you can move on!\n\n\nScenario 2: Existing user, New computer\nIf you have already created an account and token but not set up your computer yet, you can use the same token on a new computer.\nStep 1) Find your token by running the following code in a Jupyter notebook:\n\nfrom caveclient import CAVEclient\nclient = CAVEclient()\nclient.auth.setup_token(make_new=False)\n\nThis will open a new browser window and ask you to log in.\nAfter lgging in, the page will display your current token (the value after the token:).\nCopy this token, and save it to your computer using the following:\n\nclient.auth.save_token(token=YOUR_TOKEN, overwrite=True)\n\nNote that the token must be formatted as a string.\nTo check if your setup works, run the following:\n\nclient = CAVEclient('minnie65_public')\n\nIf you don’t get any errors, your setup has worked and you can move on!\n\n\nSomething went wrong?\nIf you are having trouble with authentication or permissions, it is probably because the token you are trying to use is not the one CAVE expects.\nTo find your current token, run the following code in a Jupyter notebook:\n\nclient = CAVEclient()\nclient.auth.token\n\nThe notebook will print the token your computer is sending to the server.\nIf this token is not the one you find from running the code in Scenario 2, follow the steps there to set up your computer with the correct token.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVEclient Setup"
    ]
  },
  {
    "objectID": "quickstart_notebooks/03-cave-query-proofread-cells.html",
    "href": "quickstart_notebooks/03-cave-query-proofread-cells.html",
    "title": "CAVE Query: Proofread Cells",
    "section": "",
    "text": "Version update\n\n\n\nThis tutorial has recently been updated to materialization version 1412 (from 1300).\nWe have released a new public version 1412, as part of our quarterly release schedule. See details at Release Manifests: 1412.\nThe Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Proofread Cells"
    ]
  },
  {
    "objectID": "quickstart_notebooks/03-cave-query-proofread-cells.html#caveclient",
    "href": "quickstart_notebooks/03-cave-query-proofread-cells.html#caveclient",
    "title": "CAVE Query: Proofread Cells",
    "section": "CAVEclient",
    "text": "CAVEclient\nMost programmatic access to the CAVE services occurs through CAVEclient, a Python client to access various types of data from the online services.\nFull documentation for CAVEclient is available here.\nTo initialize a caveclient, we give it a datastack, which is a name that defines a particular combination of imagery, segmentation, and annotation database. For the MICrONs public data, we use the datastack name minnie65_public.\n\nimport numpy as np\nimport pandas as pd\n\nfrom caveclient import CAVEclient\ndatastack_name = 'minnie65_public'\nclient = CAVEclient(datastack_name)\n\n# Show the description of the datastack\nclient.info.get_datastack_info()['description']\n\n'This is the publicly released version of the minnie65 volume and segmentation. '\n\n\n\nMaterialization versions\nData in CAVE is timestamped and periodically versioned - each (materialization) version corresponds to a specific timestamp. Individual versions are made publicly available. The Materialization client allows one to interact with the materialized annotation tables that were posted to the annotation service. These are called queries to the dataset, and available from client.materialize. For more, see the CAVEclient Documentation.\nPeriodic updates are made to the public datastack, which will include updates to the available tables. Some cells will have different pt_root_id because they have undergone proofreading.\n\n\n\n\n\n\nTip\n\n\n\nFor analysis consistency, is worth checking the version of the data you are using, and consider specifying the version with client.version = your_version\nRead more about setting the version of your analysis\n\n\n\n# see the available materialization versions\nclient.materialize.get_versions()\n\n[1300, 1078, 117, 661, 343, 1181, 795, 943, 1412]\n\n\nAnd these are their associated timestamps (all timestamps are in UTC):\n\nfor version in client.materialize.get_versions():\n    print(f\"Version {version}: {client.materialize.get_timestamp(version)}\")\n\nVersion 1300: 2025-01-13 10:10:01.286229+00:00\nVersion 1078: 2024-06-05 10:10:01.203215+00:00\nVersion 117: 2021-06-11 08:10:00.215114+00:00\nVersion 661: 2023-04-06 20:17:09.199182+00:00\nVersion 343: 2022-02-24 08:10:00.184668+00:00\nVersion 1181: 2024-09-16 10:10:01.121167+00:00\nVersion 795: 2023-08-23 08:10:01.404268+00:00\nVersion 943: 2024-01-22 08:10:01.497934+00:00\nVersion 1412: 2025-04-29 10:10:01.200893+00:00\n\n\n\n# set materialization version, for consistency\nclient.version = 1412 # current public as of 4/29/2025",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Proofread Cells"
    ]
  },
  {
    "objectID": "quickstart_notebooks/03-cave-query-proofread-cells.html#querying-proofread-neurons",
    "href": "quickstart_notebooks/03-cave-query-proofread-cells.html#querying-proofread-neurons",
    "title": "CAVE Query: Proofread Cells",
    "section": "Querying Proofread neurons",
    "text": "Querying Proofread neurons\n\nProofread neurons\nProofreading is necessary to obtain accurate reconstructions of a cell. In the MICrONS dataset, the general rule is that dendrites onto cells with a single cell body are sufficiently proofread to trust synaptic connections onto a cell. Axons on the other hand require so much proofreading that only ~1800 cells have axons such that their outputs should be used for analysis.\nTable name: proofreading_status_and_strategy\nThe table proofreading_status_and_strategy describes the status of cells that have undergone manual proofreading.\nBecause of the inherent difference in the difficulty and time required for different kinds of proofreading, we describe the status of axons and dendrites separately.\nEach compartment status may be either:\n\nFALSE: indicates no comprehensive proofreading has been performed, or is not applicable.\n\nTRUE: indicates that false merges have been comprehensively removed, and the compartment is at least ‘clean’. Consult the strategy column if completeness of the compartment is relevant to your research.\n\nAn axon or dendrite labeled as status=TRUE can be trusted to be correct, but may not be complete. The degree of completion can be read from the strategy column. For more information, please see Proofreading and Data Quality; or also the microns-explorer page on proofreading strategies.\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made. NOTE: if this does not match the pt_root_id then the cell has undergone further changes. This is usually and improvement in proofreading, but proceed with caution.\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. May be TRUE or FALSE\n\n\nstatus_axon\nThe status of the axon proofreading. May be TRUE or FALSE\n\n\nstrategy_dendrite\nThe strategy employed to proofread the dendrite. See strategy table below for details\n\n\nstrategy_axon\nThe strategy employed to proofread the axon. See strategy table below for details\n\n\n\nThe specific strategies are as follows (and will update over time):\n\nProofreading Strategies\n\nProofreading Strategy Table\n\n\n\n\n\n\nStrategy\nDescription\n\n\n\n\nnone\nNo cleaning, and no extension. Indicates an entry in proofreading_status that is FALSE for that compartment\n\n\ndendrite_clean\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. The dendrite may be incorrectly truncated by segmentation error. Not all dendrite tips have been checked for extension. No comprehensive attempt was made to re-attach spine heads.\n\n\ndendrite_extended\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. Every tip was identified, manually inspected, and extended if possible. No comprehensive attempt was made to re-attach spine heads.\n\n\naxon_column_truncated\nAThe axon was extended within the V1 cortical column, with a preference for local connections. In some cases the axon was cut at the column boundary and/or the layer boundary, especially the boundary between layers 2/3 and layer 4. Output synapses represent a sampling of potential partners\n\n\naxon_interareal\nThe axon was extended with a preference for branches that projected to other brain areas. Some axon branches were fully extended, but local connections may be incomplete. Output synapses represent a sampling of potential partners.\n\n\naxon_partially_extended\nThe axon was extended outward from the soma, following each branch to its termination. Output synapses represent a sampling of potential partners.\n\n\naxon_fully_extended\nAxon was extended outward from the soma, following each branch to its termination. After initial extension, every endpoint was identified, manually inspected, and extended again if possible. Output synapses represent a largely complete sampling of partners.\n\n\n\nThis table, proofreading_status_and_strategy, supercedes proofreading_status_public_release.\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()\nHere we query and return the table as of version 1300.\nFor the commands used for querying tables, see the previous quickstart notebook on CAVE queries\n\nproof_all_df = client.materialize.query_table(\"proofreading_status_and_strategy\", \n                                              desired_resolution=[1, 1, 1], \n                                              split_positions=True)\n\n\nproof_all_df[\"strategy_axon\"].value_counts()\n\nstrategy_axon\naxon_partially_extended    1702\naxon_fully_extended         156\naxon_interareal             124\nnone                         38\nName: count, dtype: int64\n\n\n\n\n\nFiltering Queries by proofreading status\nWe can filter our query to only return rows that match a condition by adding a filter to our query:\n\nproof_axon_df = client.materialize.query_table(\"proofreading_status_and_strategy\", \n                                               filter_in_dict={\"strategy_axon\": [\"axon_partially_extended\", \"axon_fully_extended\", \"axon_interareal\"]}, \n                                               desired_resolution=[1, 1, 1], \n                                               split_positions=True)\nproof_axon_df.tail()\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\npt_position_x\npt_position_y\npt_position_z\nvalid_id\nstatus_dendrite\nstatus_axon\nstrategy_dendrite\nstrategy_axon\npt_supervoxel_id\npt_root_id\n\n\n\n\n1977\n3639\n2025-04-26 15:26:01.884071+00:00\nNaN\nt\n768192.0\n722624.0\n865560.0\n864691135124248359\nt\nt\ndendrite_extended\naxon_partially_extended\n91213973684426061\n864691135124248359\n\n\n1978\n3640\n2025-04-26 15:26:01.897693+00:00\nNaN\nt\n742784.0\n743936.0\n823560.0\n864691135447653010\nt\nt\ndendrite_extended\naxon_partially_extended\n90299867070611957\n864691135447653010\n\n\n1979\n3641\n2025-04-26 15:26:01.911083+00:00\nNaN\nt\n726848.0\n730944.0\n864320.0\n864691135849699166\nt\nt\ndendrite_extended\naxon_partially_extended\n89736504934533504\n864691135849699166\n\n\n1980\n3642\n2025-04-26 15:26:01.924637+00:00\nNaN\nt\n792832.0\n936128.0\n952600.0\n864691135396485877\nt\nt\ndendrite_extended\naxon_partially_extended\n92065545708721615\n864691135396485877\n\n\n1981\n3643\n2025-04-26 15:36:38.403644+00:00\nNaN\nt\n738176.0\n727168.0\n893440.0\n864691135646493679\nt\nt\ndendrite_extended\naxon_fully_extended\n90158580028006378\n864691135646493679\n\n\n\n\n\n\n\nA more unified filter interface is available through a “table manager” interface.\nRather than passing a table name to the query_table function, client.materialize.tables has a subproperty for each table in the database that can be used to filter that table.\nThe general pattern for usage is\nclient.materialize.tables.{table_name}({filter options}).query({format and timestamp options})\nwhere {table_name} is the name of the table you want to filter, {filter options} is a collection of arguments for filtering the query, and {format and timestamp options} are those parameters controlling the format and timestamp of the query.\n\n\n\n\n\n\nCaution\n\n\n\nUse of this functionality will show a brief warning that the interface is experimental. This is because the interface is still being developed and may change in the near future in response to user feedback.\n\n\nWith this, we can easily query all proofread cells with proofread axons:\n\nproof_axon_df = client.materialize.tables.proofreading_status_and_strategy(strategy_axon=[\"axon_partially_extended\", \"axon_fully_extended\", \"axon_interareal\"]).query(\n    select_columns=['pt_root_id','status_axon','status_dendrite','strategy_axon','strategy_dendrite'],\n)\nproof_axon_df.tail()\n\n\n\n\n\n\n\n\npt_root_id\nstatus_axon\nstatus_dendrite\nstrategy_axon\nstrategy_dendrite\n\n\n\n\n1977\n864691135124248359\nt\nt\naxon_partially_extended\ndendrite_extended\n\n\n1978\n864691135447653010\nt\nt\naxon_partially_extended\ndendrite_extended\n\n\n1979\n864691135849699166\nt\nt\naxon_partially_extended\ndendrite_extended\n\n\n1980\n864691135396485877\nt\nt\naxon_partially_extended\ndendrite_extended\n\n\n1981\n864691135646493679\nt\nt\naxon_fully_extended\ndendrite_extended",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Proofread Cells"
    ]
  },
  {
    "objectID": "quickstart_notebooks/03-cave-query-proofread-cells.html#combining-proofread-cells-and-cell-types",
    "href": "quickstart_notebooks/03-cave-query-proofread-cells.html#combining-proofread-cells-and-cell-types",
    "title": "CAVE Query: Proofread Cells",
    "section": "Combining proofread cells and cell types",
    "text": "Combining proofread cells and cell types\nFor analysis, often you are interested in neurons that are at the intersection of two or more groups. For example: proofread cells that are also layer 2/3 pyramidal cells. The general workflow for this type of analysis is to:\n\nQuery from one table, for example the proofreading_status_and_strategy table\nQuery from another table, for example the aibs_metamodel_celltypes_v661\nMerge the two tables on the shared index, in this case pt_root_id\n\nWe covered querying cell types in the previous quickstart notebook. Now lets put that together with the proofreading query:\n\n# Query proofread cells with status_axon==True\nproof_df = client.materialize.tables.proofreading_status_and_strategy(status_axon=\"t\").query(\n    select_columns=['pt_root_id','status_axon','status_dendrite','strategy_axon','strategy_dendrite'],\n)\n\n# Query cell types\ncell_type_df = client.materialize.tables.aibs_metamodel_celltypes_v661().query(\n    select_columns = {'nucleus_detection_v0': ['pt_root_id', 'id'],\n                      'aibs_metamodel_celltypes_v661': ['classification_system','cell_type'],\n                     },\n)\n\n\n\n\n\n\n\nTip\n\n\n\nNote the ‘select_columns’ argument differs between the two tables. Thay is because the second table, aibs_metamodel_celltypes_v661 is itself a reference on nucleus_detection_v0. That means the id column returned here is the same as the nucleus id of the cell. This is handy for referencing the same cell across materialization versions as the nucleus id does not change, whereas the pt_root_id will change with proofreading.\n\n\nNow we can merge the two tables together on the shared index!\nBut it is worth checking if there are duplicates in either of the tables. How you handle duplicates will depend on your question, and the table you are using. Here we might see duplicates from multi-soma merges in the cell type table\n\ncell_type_df.value_counts('pt_root_id')\n\npt_root_id\n864691135891586697    350\n0                     175\n864691137020183406    102\n864691136974041116     60\n864691135455264362     43\n                     ... \n864691135568905862      1\n864691135568905350      1\n864691135568904326      1\n864691135568904172      1\n864691137199312705      1\nName: count, Length: 91446, dtype: int64\n\n\nFor analytical simplicity, we will drop any multi-soma objects. We will also rename the id column for clarity\n\n# Drop duplicate pt_root_id and rename the nucleus_id\ncell_type_df = (cell_type_df\n                .drop_duplicates('pt_root_id', keep=False)\n                .rename(columns={'id': 'nucleus_id'})\n               )\n                        \ncell_type_df.head()\n\n\n\n\n\n\n\n\npt_root_id\nnucleus_id\nclassification_system\ncell_type\n\n\n\n\n0\n864691136274724621\n336365\nexcitatory_neuron\n5P-IT\n\n\n1\n864691135489403194\n110648\nexcitatory_neuron\n23P\n\n\n2\n864691136147292311\n112071\nexcitatory_neuron\n23P\n\n\n3\n864691135655940290\n197927\nnonneuron\noligo\n\n\n4\n864691135809440972\n198087\nnonneuron\nastrocyte\n\n\n\n\n\n\n\nNow we can merge the two tables with pandas.merge, on index pt_root_id. We will keep the inner join of the two tables: cells that 1) are proofread, and 2) have a cell type\n\nproof_cell_type_df = pd.merge(proof_df, cell_type_df, on='pt_root_id', how='inner')\nproof_cell_type_df.tail()\n\n\n\n\n\n\n\n\npt_root_id\nstatus_axon\nstatus_dendrite\nstrategy_axon\nstrategy_dendrite\nnucleus_id\nclassification_system\ncell_type\n\n\n\n\n1929\n864691135124248359\nt\nt\naxon_partially_extended\ndendrite_extended\n301119\nexcitatory_neuron\n5P-IT\n\n\n1930\n864691135447653010\nt\nt\naxon_partially_extended\ndendrite_extended\n300956\nexcitatory_neuron\n5P-IT\n\n\n1931\n864691135849699166\nt\nt\naxon_partially_extended\ndendrite_extended\n301083\nexcitatory_neuron\n5P-IT\n\n\n1932\n864691135396485877\nt\nt\naxon_partially_extended\ndendrite_extended\n342334\nexcitatory_neuron\n6P-IT\n\n\n1933\n864691135646493679\nt\nt\naxon_fully_extended\ndendrite_extended\n301203\ninhibitory_neuron\nBPC\n\n\n\n\n\n\n\nAnd we have the list of all proofread cells, by their cell type!\nWe can do this same kind of query more simply by: querying the second table by BOTH the root ids of interest and the cell type of interest. If we wanted only the proofread 23P cells, we could do:\n\n# Query the proofread 23P cells, and merge the proofreading status\nproof_23p_df = (client.materialize.tables.aibs_metamodel_celltypes_v661(pt_root_id=proof_df.pt_root_id, cell_type='23P').query(\n    select_columns = {'nucleus_detection_v0': ['pt_root_id', 'id'],\n                      'aibs_metamodel_celltypes_v661': ['classification_system','cell_type'],\n                     },  )\n                .rename(columns={'id': 'nucleus_id'})\n                .merge(proof_df, on='pt_root_id', how='inner')\n               )\n\nproof_23p_df.head()\n\n\n\n\n\n\n\n\npt_root_id\nnucleus_id\nclassification_system\ncell_type\nstatus_axon\nstatus_dendrite\nstrategy_axon\nstrategy_dendrite\n\n\n\n\n0\n864691135473477426\n258375\nexcitatory_neuron\n23P\nt\nt\naxon_partially_extended\ndendrite_clean\n\n\n1\n864691135257669039\n258377\nexcitatory_neuron\n23P\nt\nt\naxon_partially_extended\ndendrite_clean\n\n\n2\n864691135763593014\n258403\nexcitatory_neuron\n23P\nt\nt\naxon_partially_extended\ndendrite_clean\n\n\n3\n864691135763433270\n258225\nexcitatory_neuron\n23P\nt\nt\naxon_partially_extended\ndendrite_clean\n\n\n4\n864691135645874159\n292833\nexcitatory_neuron\n23P\nt\nt\naxon_partially_extended\ndendrite_clean",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Proofread Cells"
    ]
  },
  {
    "objectID": "quickstart_notebooks/05-cloudvolume-download.html",
    "href": "quickstart_notebooks/05-cloudvolume-download.html",
    "title": "Download Imagery and Segmentation",
    "section": "",
    "text": "The electron microscopy images and their dense reconstruction into 3D objects (respectively: imagery and segmentation) are available to download through the python client cloud-volume.\nThe MICrONS data can be representing and rendered in multiple formats, at different levels of abstraction from the original imagery.\n\n\n\n\n\n\n\n\n\nSegmentation: more expensive\n\n\n\n\n\n\n\nMesh\n\n\n\n\n\n\n\nSkeleton: less expensive",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Imagery and Segmentation"
    ]
  },
  {
    "objectID": "quickstart_notebooks/05-cloudvolume-download.html#connected-morphological-representations-segmentation",
    "href": "quickstart_notebooks/05-cloudvolume-download.html#connected-morphological-representations-segmentation",
    "title": "Download Imagery and Segmentation",
    "section": "",
    "text": "The electron microscopy images and their dense reconstruction into 3D objects (respectively: imagery and segmentation) are available to download through the python client cloud-volume.\nThe MICrONS data can be representing and rendered in multiple formats, at different levels of abstraction from the original imagery.\n\n\n\n\n\n\n\n\n\nSegmentation: more expensive\n\n\n\n\n\n\n\nMesh\n\n\n\n\n\n\n\nSkeleton: less expensive",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Imagery and Segmentation"
    ]
  },
  {
    "objectID": "quickstart_notebooks/05-cloudvolume-download.html#cloud-volume-for-downloading-imagery-segmentation",
    "href": "quickstart_notebooks/05-cloudvolume-download.html#cloud-volume-for-downloading-imagery-segmentation",
    "title": "Download Imagery and Segmentation",
    "section": "Cloud-Volume for downloading imagery, segmentation",
    "text": "Cloud-Volume for downloading imagery, segmentation\nCloudVolume is a serverless Python client for random access reading and writing of Neuroglancer volumes in “Precomputed” format, a set of representations for arbitrarily large volumetric images, meshes, and skeletons.\nPrecomputed volumes are typically stored on AWS S3, Google Storage, or locally. CloudVolume can read and write to these object storage providers given a service account token with appropriate permissions. However, these volumes can be stored on any service, including an ordinary webserver or local filesystem, that supports key-value access.\ncloud-volume is the mechanism for many programmatic data queries, and integrates heavily with the CAVE ecosystem but is distinct. You can install cloud-volume with:\npip install cloud-volume\nThe Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data.\n\n\n\n\n\n\nInitial Setup\n\n\n\nBefore using any programmatic access to the data, you first need to set up your CAVEclient token. When CAVEclient saves the token to your local machine, cloudvolume will access the same token.\n\n\n\nfrom caveclient import CAVEclient\nfrom cloudvolume import CloudVolume\n\ndatastack_name = 'minnie65_public'\nclient = CAVEclient(datastack_name)\n\nThe datastack includes information about the imagery and segmentation source, and can provide that information when prompted\n\nclient.info.image_source()\n\n'precomputed://https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/em'\n\n\n\nclient.info.segmentation_source()\n\n'graphene://https://minnie.microns-daf.com/segmentation/table/minnie65_public'\n\n\n\n# this can be used to initialize a cloudvolume object\ncv = CloudVolume(client.info.segmentation_source(), progress=False, use_https=True)\n\n\nExamples from the cloudvolume README:\nimage = cv[:,:,:] # Download the entire image stack into a numpy array\nimage = cv.download(bbox, mip=2, renumber=True) # download w/ smaller dtype\nimage = cv.download(bbox, mip=2, label=777) # download binary image for label\n\nuniq = cv.unique(bbox, mip=0) # efficient extraction of unique labels\nlisting = cv.exists( np.s_[0:64, 0:128, 0:64] ) # get a report on which chunks actually exist\nexists = cv.image.has_data(mip=0) # boolean check to see if any data is there\nlisting = cv.delete( np.s_[0:64, 0:128, 0:64] ) # delete this region (bbox must be chunk aligned)\n\nimg = cv.download_point( (x,y,z), size=256, mip=3 ) # download region around (mip 0) x,y,z at mip 3\npts = cv.scattered_points([ (x1,y1,z1), (x2,y2,z2) ]) # download voxel labels located at indicated points\n\n# download image files without decompressing or rendering them. Good for caching!\nfiles = cv.download_files(bbox, mip, decompress=False)",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Imagery and Segmentation"
    ]
  },
  {
    "objectID": "quickstart_notebooks/05-cloudvolume-download.html#imageryclient-for-aligned-downloads",
    "href": "quickstart_notebooks/05-cloudvolume-download.html#imageryclient-for-aligned-downloads",
    "title": "Download Imagery and Segmentation",
    "section": "ImageryClient for aligned downloads",
    "text": "ImageryClient for aligned downloads\nWe recommend using ImageryClient for simple download imagery and segmentation data, like for generating figures. ImageryClient makes core use of cloud-volume, but adds convenience and better integration with the CAVEclient.\nYou can install ImageryClient with:\npip install imageryclient\nImageryClient is designed to download aligned blocks of imagery and segmentation data, as well as has some convenience functions for creating overlays of the two.\nImagery is downloaded as blocks of 8-bit values (0-255) that indicate grayscale intensity, while segmentation is downloaded as blocks of 64-bit integers that describe the segmentation ID of each voxel.\nAlternatively, segmentation can be kept as a dictionary of boolean masks, where each key is a root ID and each value is a boolean mask of the same shape as the imagery.\nDetailed information on the options can be found in the documentation.\nA typical example would be to use ImageryClien to download and visualize a 512x512 pixel cutout of imagery and segmentation centered on a specific location based on the coordinates in Neuroglancer:\n\nimport imageryclient as ic\n\nimg_client = ic.ImageryClient(client=client)\n\nctr = [240640, 207872, 21360]\n\nimage, segs = img_client.image_and_segmentation_cutout(ctr,\n                                                       split_segmentations=True,\n                                                       bbox_size=(512, 512),\n                                                       scale_to_bounds=True,\n)\n\nic.composite_overlay(segs, imagery=image, palette='husl').convert(\"RGB\")\n\n# Note: the final `.convert('RGB')` is needed to build this documetnation, but is not required to run locally.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Imagery and Segmentation"
    ]
  },
  {
    "objectID": "quickstart_notebooks/07-cave-download-skeleton.html",
    "href": "quickstart_notebooks/07-cave-download-skeleton.html",
    "title": "Download Skeletons",
    "section": "",
    "text": "Version update\n\n\n\nThis tutorial has recently been updated to materialization version 1412 (from 1300).\nWe have released a new public version 1412, as part of our quarterly release schedule. See details at Release Manifests: 1412.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Skeletons"
    ]
  },
  {
    "objectID": "quickstart_notebooks/07-cave-download-skeleton.html#connected-morphological-representations-skeletons",
    "href": "quickstart_notebooks/07-cave-download-skeleton.html#connected-morphological-representations-skeletons",
    "title": "Download Skeletons",
    "section": "Connected morphological representations: skeletons",
    "text": "Connected morphological representations: skeletons\nOften in thinking about neurons, you want to measure things along a linear dimension of a neuron.\nHowever, the segmentation and meshes are a full complex 3d shape that makes this non-trivial. There are methods for reducing the shape of a segmented neuron down to a linear tree like structure usually referred to as a skeleton. We have precalculated skeletons for a large number of cells in the dataset, and make the skeleton generation available on our server, on demand.\nThe meshes that one sees in Neuroglancer are available to download through the python client cloud-volume, and can be loaded for analysis and visualization in other tools.\nThe MICrONS data can be representing and rendered in multiple formats, at different levels of abstraction from the original imagery.\n\n\n\n\n\n\n\n\n\nSegmentation: more expensive\n\n\n\n\n\n\n\nMesh\n\n\n\n\n\n\n\nSkeleton: less expensive",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Skeletons"
    ]
  },
  {
    "objectID": "quickstart_notebooks/07-cave-download-skeleton.html#what-is-a-precomputed-skeleton",
    "href": "quickstart_notebooks/07-cave-download-skeleton.html#what-is-a-precomputed-skeleton",
    "title": "Download Skeletons",
    "section": "What is a Precomputed skeleton?",
    "text": "What is a Precomputed skeleton?\nThese skeletons are stored in the cloud as a bytes-IO object, which can be viewed in Neuroglancer, or downloaded with CAVEclient.skeleton module. These precomputed skeletons also contain annotations on the skeletons that have the synapses, which skeleton nodes are axon and which are dendrite, and which are likely the apical dendrite of excitatory neurons.\n\n\n\npyChunkedGraph Skeleton\n\n\nFor more on how skeletons are generated from the mesh objects, and additional tools for generating, cacheing, and downloading meshes, see the Skeleton Service documentation",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Skeletons"
    ]
  },
  {
    "objectID": "quickstart_notebooks/07-cave-download-skeleton.html#caveclient-to-download-skeletons",
    "href": "quickstart_notebooks/07-cave-download-skeleton.html#caveclient-to-download-skeletons",
    "title": "Download Skeletons",
    "section": "CAVEclient to download skeletons",
    "text": "CAVEclient to download skeletons\nRetrieve a skeleton using get_skeleton(). The available output_formats (described below) are:\n\ndict containing vertices, edges, radius, compartment, and various metadata (default if unspecified)\nswc a Pandas Dataframe in the SWC format, with ordered vertices, edges, compartment labels, and radius.\n\nNote: if the skeleton doesn’t exist in the server cache, it may take 20-60 seconds to generate the skeleton before it is returned. This function will block during that time. Any subsequent retrieval of the same skeleton should go very quickly however.\n\n\n\n\n\n\nInitial Setup\n\n\n\nBefore using any programmatic access to the data, you first need to set up your CAVEclient token.\n\n\n\nfrom caveclient import CAVEclient\nimport numpy as np\nimport pandas as pd\n\nclient = CAVEclient(\"minnie65_public\")\n\n# specify the materialization version, for consistency across time\",\nclient.version = 1412\n\n# Example: pyramidal cell in v1412\nexample_cell_id = 864691135572530981\n\n\nsk_df = client.skeleton.get_skeleton(example_cell_id, output_format='swc')\n\nsk_df.head()\n\n\n\n\n\n\n\n\nid\ntype\nx\ny\nz\nradius\nparent\n\n\n\n\n0\n0\n1\n1365.120\n763.456\n812.60\n3.643\n-1\n\n\n1\n1\n3\n1368.200\n755.760\n811.24\n0.300\n0\n\n\n2\n2\n3\n1368.200\n758.552\n811.68\n1.915\n1\n\n\n3\n3\n3\n1368.200\n758.560\n812.12\n1.915\n2\n\n\n4\n4\n3\n1368.192\n758.560\n812.44\n2.815\n3\n\n\n\n\n\n\n\nSkeletons are “tree-like”, where every vertex (except the root vertex) has a single parent that is closer to the root than it, and any number of child vertices. Because of this, for a skeleton there are well-defined directions “away from root” and “towards root” and few types of vertices have special names:\nYou can see these vertices and edges represented in the alternative dict skeleton output.\n\nsk_dict = client.skeleton.get_skeleton(example_cell_id, output_format='dict')\n\nsk_dict.keys()\n\ndict_keys(['meta', 'edges', 'mesh_to_skel_map', 'root', 'vertices', 'compartment', 'radius'])\n\n\nAlternately, you can query a set of root_ids, for example all of the proofread cells in the dataset, and bulk download the skeletons\n\nprf_root_ids = client.materialize.tables.proofreading_status_and_strategy(\n    status_axon='t').query()['pt_root_id']\nprf_root_ids.head(3)\n\n0    864691135308929350\n1    864691135163673901\n2    864691136812081779\nName: pt_root_id, dtype: int64\n\n\n\nall_sk_dict = client.skeleton.get_bulk_skeletons(\n    prf_root_ids.head(3), \n    generate_missing_skeletons=True,\n    output_format='dict',\n    skeleton_version=4)\n\nall_sk_dict.keys()\n\ndict_keys(['864691135308929350', '864691135163673901', '864691136812081779'])\n\n\n\nall_sk_dict['864691135308929350'].keys()\n\ndict_keys(['meta', 'edges', 'mesh_to_skel_map', 'root', 'vertices', 'compartment', 'radius', 'lvl2_ids'])\n\n\n\nconvert dictionary to meshwork skeleton\nWe use the python package Meshparty to convert between mesh representations of neurons, and skeleton representations.\nSee Download Meshes | MeshParty for more details.\nSkeletons are “tree-like”, where every vertex (except the root vertex) has a single parent that is closer to the root than it, and any number of child vertices. Because of this, for a skeleton there are well-defined directions “away from root” and “towards root” and few types of vertices have special names:\nConverting the pregenerated skeleton sk_dict to a meshwork object sk converts the skeleton to a graph object, for convenient representation and analysis.\n\nfrom meshparty.skeleton import Skeleton\n\nsk = Skeleton.from_dict(sk_dict)",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Skeletons"
    ]
  },
  {
    "objectID": "quickstart_notebooks/07-cave-download-skeleton.html#plot-with-skeleton_plot",
    "href": "quickstart_notebooks/07-cave-download-skeleton.html#plot-with-skeleton_plot",
    "title": "Download Skeletons",
    "section": "Plot with skeleton_plot",
    "text": "Plot with skeleton_plot\nOur convenience package skeleton_plot renders the skeleton in aligned, 2D views.\npip install skeleton_plot\nOur convenience package skeleton_plot renders the skeleton in aligned, 2D views, with the compartments labeled in different colors. Here, dendrite is red, axon is blue, and soma (the root of the connected graph) is olive\n\nfrom skeleton_plot.plot_tools import plot_skel\n\nplot_skel(sk=sk,\n          invert_y=True,\n          pull_compartment_colors=True,\n          plot_soma=True,\n         )",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Skeletons"
    ]
  },
  {
    "objectID": "quickstart_notebooks/07-cave-download-skeleton.html#download-or-generate-skeletons-with-pcg_skel",
    "href": "quickstart_notebooks/07-cave-download-skeleton.html#download-or-generate-skeletons-with-pcg_skel",
    "title": "Download Skeletons",
    "section": "Download or generate skeletons with pcg_skel",
    "text": "Download or generate skeletons with pcg_skel\npcg_skel is a package used to rapidly build neuronal skeletons from electron microscopy data in the CAVE ecosystem. It integrates structural data, connectivity data, and local features stored across many aspects of a CAVE system, and creates objects as Meshparty meshes, skeletons, and MeshWork files for subsequent analysis.\nBy harnessing the way the structural data is stored, you can build skeletons for even very large neurons quickly and with little memory use.\nTo install pcg_skel, use pip.\npip install pcg_skel\nBy loading a skeleton as a meshwork skeleton, you can easily ‘rehydrate’ the mesh features of the neuron (annotations such as synapses and compartment labels) and MeshParty functions (such as masking and pathlength)\n\nfrom pcg_skel import get_meshwork_from_client\n\nsk_mesh = get_meshwork_from_client(example_cell_id,\n                                   client=client,\n                                   synapses=True,\n                                   restore_graph=True,\n                                   restore_properties=True,\n                                   skeleton_version=4,\n)\nsk_mesh\n\n&lt;meshparty.meshwork.meshwork.Meshwork at 0x29f79b4cc50&gt;\n\n\nSkeletons are “tree-like”, where every vertex (except the root vertex) has a single parent that is closer to the root than it, and any number of child vertices. Because of this, for a skeleton there are well-defined directions “away from root” and “towards root” and few types of vertices have special names:\n\nBranch point: vertices with two or more children, where a neuronal process splits.\nEnd point: vertices with no childen, where a neuronal process ends.\nRoot point: The one vertex with no parent node. By convention, we typically set the root vertex at the cell body, so these are equivalent to “away from soma” and “towards soma”.\nSegment: A collection of vertices along an unbranched region, between one branch point and the next end point or branch point downstream.\n\nMeshWork skeletons represent the neuron as a graph, with tools and properties to utilize the skeleton csgraph.\nProperties\n\nbranch_points: a list of skeleton vertices which are branches\nroot: the skeleton vertice which is the soma\ndistance_to_root: an array the length of vertices which tells you how far away from the root each vertex is\nroot_position: the position of the root node in nanometers\nend_points: the tips of the neuron\ncover_paths: a list of arrays containing vertex indices that describe individual paths that in total cover the neuron without repeating a vertex. Each path starts at an end point and continues toward root, stopping once it gets to a vertex already listed in a previously defined path. Paths are ordered to start with the end points farthest from root first. Each skeleton vertex appears in exactly one cover path.\ncsgraph: a scipy.sparse.csr.csr_matrix containing a graph representation of the skeleton. Useful to do more advanced graph operations and algorithms. https://docs.scipy.org/doc/scipy/reference/sparse.csgraph.html\nkdtree: a scipy.spatial.ckdtree.cKDTree containing the vertices of skeleton as a kdtree. Useful for quickly finding points that are nearby. https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.cKDTree.html\n\nMethods\n\npath_length(paths=None): the path length of the whole neuron if no arguments, or pass a list of paths to get the path length of that. A path is just a list of vertices which are connected by edges.\npath_to_root(vertex_index): returns the path to the root from the passed vertex\npath_between(source_index, target_index): the shortest path between the source vertex index and the target vertex index\nchild_nodes(vertex_indices): a list of arrays listing the children of the vertex indices passed in\nparent_nodes(vertex_indices): an array listing the parent of the vertex indices passed in\n\nYou access each of these properties and methods with sk.skeleton.* Use the ? to read more details about each one\n\nAnnotation properties\nsk.anno has set of annotation tables containing some additional information for analysis. Each annotation table has both a Pandas DataFrame object storing data and additional information that allow the rows of the DataFrame to be mapped to mesh and skeleton vertices. For the neurons that have been pre-computed, there is a consisent set of annotation tables:\n\npost_syn: Postsynaptic sites (inputs) for the cell\npre_syn: Presynaptic sites (outputs) for the cell\nis_axon: List of vertices that have been labeled as part of the axon\nlvl2_ids: Gives the PCG level 2 id for each mesh vertex, largely for book-keeping reasons.\nsegment_properties: For each vertex, information about the approximate radius, surface area, volume, and length of the segment it is on.\nvol_prop: For every vertex, information about the volume and surface area of the level 2 id it is associated with.\n\nTo access one of the DataFrames, use the name of the table as an attribute of the anno object and then get the .df property. For example, to get the postsynaptic sites, we can do:\n\nsk_mesh.anno.post_syn.df.head(3)\n\n\n\n\n\n\n\n\nid\nsize\npre_pt_supervoxel_id\npost_pt_supervoxel_id\npost_pt_root_id\npre_pt_position\npost_pt_position\nctr_pt_position\npost_pt_level2_id\npost_pt_mesh_ind\npost_pt_mesh_ind_filt\n\n\n\n\n0\n456611088\n5140\n113239596703504810\n113239596703516355\n864691135572530981\n[1410648.0, 729000.0, 850240.0]\n[1410800.0, 728960.0, 850520.0]\n[1410672.0, 728960.0, 850280.0]\n185297190741082374\n11880\n11880\n\n\n1\n460786295\n1904\n114084296310348680\n114084365029814954\n864691135572530981\n[1434984.0, 737808.0, 794360.0]\n[1435488.0, 738120.0, 794120.0]\n[1435224.0, 737856.0, 794400.0]\n186141959067271764\n13325\n13325\n\n\n2\n415745273\n14532\n110214290325399938\n110214290325398722\n864691135572530981\n[1322336.0, 744768.0, 807360.0]\n[1322032.0, 744560.0, 807440.0]\n[1322120.0, 744656.0, 807360.0]\n182271884363039168\n3695\n3695\n\n\n\n\n\n\n\nHowever, in addition to these rows, you can also easily get the mesh vertex index or skeleton vertex index that a row corresponds to with nrn.anno.post_syn.mesh_index or nrn.anno.post_syn.skel_index respectively. This seems small, but it allows you to integrate skeleton-like measurements with annotations trivially.\nFor example, to get the distance from the cell body for each postsynaptic site, we can do:\n\nsk_mesh.distance_to_root(sk_mesh.anno.post_syn.mesh_index)\n\narray([104938.90991211,  90399.17230225,  52673.86270142, ...,\n        70199.43048096, 212153.28248978,  91076.815979  ])\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the sk.distance_to_root, like all basic meshwork object functions, expects a mesh vertex index rather than a skeleton vertex index.\n\n\nA common pattern is to copy a synapse dataframe and then add columns to it. For example, to add the distance to root to the postsynaptic sites, we can do:\n\nsyn_df = sk_mesh.anno.pre_syn.df.copy()\nsyn_df['dist_to_root'] = sk_mesh.distance_to_root(sk_mesh.anno.pre_syn.mesh_index)\nsyn_df.head(3)[['id','size','dist_to_root']]\n\n\n\n\n\n\n\n\nid\nsize\ndist_to_root\n\n\n\n\n0\n395248437\n9532\n550625.977657\n\n\n1\n440616430\n4432\n59338.621429\n\n\n2\n439592768\n576\n426225.854660\n\n\n\n\n\n\n\nThis gives us the path-distance of every synapse from the root in nanometers (nm).\n\n\nCalculate path_length() of axon and dendrite\nGiven the skeleton graph, it is trivial to estimate the pathlength (sum of all branch lengths) for a neuron:\n\nsk_mesh.path_length() / 1e6 # to convert from nm to mm\n\nnp.float32(14.846251)\n\n\nHowever, the axon and dendrite compartments differ dramatically in their geometric and physiological properties. You generally will want to treat these seperately.\nFortunately the meshwork skeleton includes annotations as to whether the compartment is likely axon or not:\nsk_mesh.anno.is_axon\n\nand you can mask the neuron skeleton by the type of comparment simply by using these annotations as an index:\n\n# Apply a mask to the mesh using the axon labels\nsk_mesh.apply_mask(sk_mesh.anno.is_axon.mesh_mask)\n\nprint (f'Axon path length is {sk_mesh.path_length() / 1e6} mm')\n\n# Reset the mask\nsk_mesh.reset_mask()\n\nprint (f'Total path length is {sk_mesh.path_length() / 1e6} mm')\n\nAxon path length is 7.599738121032715 mm\nTotal path length is 14.846250534057617 mm",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Skeletons"
    ]
  },
  {
    "objectID": "quickstart_notebooks/09-nglui-neuroglancer.html",
    "href": "quickstart_notebooks/09-nglui-neuroglancer.html",
    "title": "NGLui: Neuroglancer States",
    "section": "",
    "text": "Version update\n\n\n\nThis tutorial has recently been updated to materialization version 1412 (from 1300).\nWe have released a new public version 1412, as part of our quarterly release schedule. See details at Release Manifests: 1412.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "NGLui: Neuroglancer States"
    ]
  },
  {
    "objectID": "quickstart_notebooks/09-nglui-neuroglancer.html#programmatic-interaction-with-neuroglancer-states",
    "href": "quickstart_notebooks/09-nglui-neuroglancer.html#programmatic-interaction-with-neuroglancer-states",
    "title": "NGLui: Neuroglancer States",
    "section": "Programmatic Interaction with Neuroglancer States",
    "text": "Programmatic Interaction with Neuroglancer States\nVisualizing data in Neuroglancer is one of the easiest ways to explore it in its full context. The python package nglui was made to make it easy to generate Neuroglancer states from data, particularly pandas dataframes, in a progammatic manner. The package can be installed with pip install nglui.\n\nThe nglui package interacts prominantly with caveclient and annotations queried from the database. See the section on querying the database to learn more.\n\n\nParsing Neuroglancer states\nThe nglui.parser module offers a number of tools to get information about neuroglancer states out of the JSON format that neuroglancer uses. The recommended approach here is to pass a dictionary representation of the JSON object the StateParser class and build various kinds of dataframes from it.\nThe simplest way to parse the annotations in a Neuroglancer state is to first save the state using the ‘Share’ button, and then copy the state id (the last number in the URL). But you could also use the text you can download from the {} button in the viewer, and load this JSON into python.\nIn Neuroglancer, on the upper right, there is a ‘Share’ button.\nWhen you click this link, you will be prompted for a google login. Use any google profile, but it is convenient to use the same as for this notebook.\nThis will upload the text of your neuroglancer state (the JSON format file, also accessed with the {} button) to a server, and returns a lookup number to access that state. The link with this state is automatically copied to your system’s clipboard.\nWhen you paste it, you will see something like this:\n\nhttps://neuroglancer.neuvue.io/?json_url=https://global.daf-apis.com/nglstate/api/v1/5560000195854336\n\nfor which the state id is 5560000195854336\nYou can then download the json and then use the annotation_dataframe function to generate a comprehensive dataframe of all the annotations in the state.\n\nimport os\nfrom caveclient import CAVEclient\nfrom nglui import parser\n\nclient = CAVEclient('minnie65_public')\n\nstate_id = 5560000195854336\nstate_json = client.state.get_state_json(state_id)\nstate = parser.StateParser(state_json)\n\nYou can now access different aspects of the state. For example, to get a list of all layers and their core info, you can use the layer_dataframe method.\n\nstate.layer_dataframe()\n\n\n\n\n\n\n\n\nlayer\ntype\nsource\narchived\n\n\n\n\n0\nimg\nimage\nprecomputed://https://bossdb-open-data.s3.amaz...\nFalse\n\n\n1\nseg\nsegmentation_with_graph\ngraphene://https://minnie.microns-daf.com/segm...\nFalse\n\n\n2\nsyns_in\nannotation\nNone\nFalse\n\n\n3\nsyns_out\nannotation\nNone\nFalse\n\n\n\n\n\n\n\nThis will give you a table with a row for each layer and columns for layer name, type, source, and whether the layer is archived (i.e. visible) or not.\nWith parser you can get a list of all annotations with the annotation_dataframe method.\n\nstate.annotation_dataframe()\n\n\n\n\n\n\n\n\nlayer\nanno_type\npoint\npointB\nlinked_segmentation\ntags\ngroup_id\ndescription\n\n\n\n\n0\nsyns_in\npoint\n[294095, 196476, 24560]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n1\nsyns_in\npoint\n[294879, 196374, 24391]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n2\nsyns_in\npoint\n[300246, 200562, 24297]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n3\nsyns_in\npoint\n[300894, 201844, 24377]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n4\nsyns_in\npoint\n[294742, 199552, 23392]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5272\nsyns_out\npoint\n[277152, 200746, 22723]\nNaN\n[864691132294257136]\n[]\nNone\nNone\n\n\n5273\nsyns_out\npoint\n[298884, 189782, 21453]\nNaN\n[864691132135519710]\n[]\nNone\nNone\n\n\n5274\nsyns_out\npoint\n[330182, 198986, 23862]\nNaN\n[864691132100215248]\n[]\nNone\nNone\n\n\n5275\nsyns_out\npoint\n[326552, 186446, 24792]\nNaN\n[864691131892380409]\n[]\nNone\nNone\n\n\n5276\nsyns_out\npoint\n[275784, 206898, 21524]\nNaN\n[864691131593914919]\n[]\nNone\nNone\n\n\n\n\n5277 rows × 8 columns\n\n\n\nThis will give you a dataframe where each row is an annotation, and columns show layer name, point locations, annotation type, annotation id, descriptive text, linked segmentations, tags, etc.\nIf you have multiple annotation layers, each layer is specified by the layer column and all points across all layers are concatenated together.\nThe point coordinates are returned at the resolution of the neuroglancer view (default: 4x4x40 nm). You can change this resolution with argument point_resolution which will rescale the points to the requested resolution.\nThe description column populates with any text you have attached to the point.\nNote that tags in the dataframe are stored as a list of integers, with each integer corresponding to one of the tags in the list.\nTo get the mapping between the tag index and the tag name for each layer, you can use the tag_dictionary function.\n\nparser.tag_dictionary(state_json, layer_name='syns_out')\n\n{1: 'targets_spine', 2: 'targets_shaft', 3: 'targets_soma'}\n\n\nAlternately if you are using tags, the expand_tags=True argument will create a column for every tag and assign a boolean value to the row based on whether the tag is present in the annotation.\nAnother option that is sometimes useful is split_points=True, which will create a separate column for each x, y, or z coordinate in the annotation.\n\nstate.annotation_dataframe(expand_tags=True, split_points=True, point_resolution=[1,1,1])\n\n\n\n\n\n\n\n\nlayer\nanno_type\nlinked_segmentation\ntags\ngroup_id\ndescription\npoint_x\npoint_y\npoint_z\npointB_x\npointB_y\npointB_z\ntargets_spine\ntargets_shaft\ntargets_soma\n\n\n\n\n0\nsyns_in\npoint\n[864691136333760691]\n[]\nNone\nNone\n73523.75\n49119.0\n614.000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nsyns_in\npoint\n[864691136333760691]\n[]\nNone\nNone\n73719.75\n49093.5\n609.775\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nsyns_in\npoint\n[864691136333760691]\n[]\nNone\nNone\n75061.50\n50140.5\n607.425\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nsyns_in\npoint\n[864691136333760691]\n[]\nNone\nNone\n75223.50\n50461.0\n609.425\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nsyns_in\npoint\n[864691136333760691]\n[]\nNone\nNone\n73685.50\n49888.0\n584.800\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5272\nsyns_out\npoint\n[864691132294257136]\n[]\nNone\nNone\n69288.00\n50186.5\n568.075\nNaN\nNaN\nNaN\nFalse\nFalse\nFalse\n\n\n5273\nsyns_out\npoint\n[864691132135519710]\n[]\nNone\nNone\n74721.00\n47445.5\n536.325\nNaN\nNaN\nNaN\nFalse\nFalse\nFalse\n\n\n5274\nsyns_out\npoint\n[864691132100215248]\n[]\nNone\nNone\n82545.50\n49746.5\n596.550\nNaN\nNaN\nNaN\nFalse\nFalse\nFalse\n\n\n5275\nsyns_out\npoint\n[864691131892380409]\n[]\nNone\nNone\n81638.00\n46611.5\n619.800\nNaN\nNaN\nNaN\nFalse\nFalse\nFalse\n\n\n5276\nsyns_out\npoint\n[864691131593914919]\n[]\nNone\nNone\n68946.00\n51724.5\n538.100\nNaN\nNaN\nNaN\nFalse\nFalse\nFalse\n\n\n\n\n5277 rows × 15 columns\n\n\n\n\n\nGenerating Neuroglancer States from Data\nThe nglui.statebuilder package is used to build Neuroglancer states that express arbitrary data.\nThe Site Configuration options determine the default configurations for your StateBuilder objects. We will set this to spelunker, and set the materialization version to 661 for reproducibility.\n\nfrom nglui import statebuilder\nstatebuilder.site_utils.set_default_config(target_site='spelunker')\n\nclient.version=1412\n\nThe general pattern is that one makes a “StateBuilder” object that has rules for how to build a Neuroglancer state layer by layer, including selecting certain neurons, and populate layers of annotations.\nYou then pass a DataFrame to the StateBuilder, and the rules tell it how to render the DataFrame into a Neuroglancer link. The same set of rules can be used on similar dataframes but with different data, such as synapses from different neurons.\nTo understand the detailed use of the package, please see the tutorial.\nHowever, a number of basic helper functions allow nglui to be used for common functions in just a few lines.\nFor example, to generate a Neuroglancer state that shows a neuron and its synaptic inputs and outputs, we can use the make_neuron_neuroglancer_link helper function.\n\nfrom nglui.statebuilder import helpers\n\nstatebuilder.helpers.make_neuron_neuroglancer_link(\n    client,\n    864691135441799752,\n    show_inputs=True,\n    show_outputs=True,\n    return_as='html'\n)\n\nNeuroglancer Link\n\n\nThe main helper functions are:\n\nmake_neuron_neuroglancer_link - Shows one or more neurons and, optionally, synaptic inputs and/or outputs.\nmake_synapse_neuroglancer_link - Using a pre-downloaded synapse table, make a link that shows the synapse and the listed synaptic partners.\nmake_point_statebuilder - Generate a statebuilder to map a dataframe containing points (by default, formatted like a cell types table) to a Neuroglancer link.\n\nIn all cases, please look at the docstrings for more information on how to use the functions.\n\n\nUploading local annotations to neuroglancer\nNGLUI also has many functions for turning data into neuroglancer states. As a toy example, lets reupload the points from the parser example.\n\n\n\n\n\n\nTip\n\n\n\nHere we generate the state for seunglab version. See the NGLUI documentation on site configuration for more options.\n\n\n\ndata_df = state.annotation_dataframe()[['point','description','tags','linked_segmentation']]\ndata_df.head(3)\n\n\n\n\n\n\n\n\npoint\ndescription\ntags\nlinked_segmentation\n\n\n\n\n0\n[294095, 196476, 24560]\nNone\n[]\n[864691136333760691]\n\n\n1\n[294879, 196374, 24391]\nNone\n[]\n[864691136333760691]\n\n\n2\n[300246, 200562, 24297]\nNone\n[]\n[864691136333760691]\n\n\n\n\n\n\n\n\nfrom nglui.statebuilder import *\n\n# Set sensible defaults for your link generation\nsite_utils.set_default_config(target_site='seunglab')\n\nimg, seg = helpers.from_client(client)\n\n# Make a basic imagery source layer\nimg_layer = ImageLayerConfig(img.source)\n\n# Make a basic segmentation source layer\nseg_layer = SegmentationLayerConfig(seg.source)\n\n# # Alternately: set v1300 flat as source\n# seg_layer = SegmentationLayerConfig('precomputed://gs://iarpa_microns/minnie/minnie65/seg_m1300')\n\n# Create the mapping between your dataframe and the annotation layer\npoints = PointMapper(point_column='point',\n                     description_column='description',\n                     linked_segmentation_column='linked_segmentation',\n                     tag_column='tags',\n                     )\nanno_layer = AnnotationLayerConfig(name='new-annotations',\n                                   linked_segmentation_layer='seg',\n                                   mapping_rules=points,\n                                   tags = ['targets_spine', 'targets_shaft', 'targets_soma']\n                                  )\n\n\n# Stack the layers together, and render\nsb = StateBuilder([img_layer, seg_layer, anno_layer], client=client)\nsb.render_state(data_df.head(10), return_as='short')\n\n'https://neuromancer-seung-import.appspot.com/?json_url=https://global.daf-apis.com/nglstate/api/v1/5069482274848768'\n\n\nWe built up the StateBuilder with different layers, including imagery, segmentation, and annotations.\nPointMapper determines how data is interpretted in the annotation layer, including which columns in a dataframe to use\nAnnotationLayerConfig controls how the points will appear, including the name of the layer as it appears in neuroglancer, which segmentation layer is activated under the points, and the list of active ‘tags’.\nThe StateBuilder is rendered in the final line, taking only the first 10 elements of the dataframe and returned a ‘shortened’ link. Other options include: return_as='html' for a hyperlink, return_as='url' for a long link, return_as='dict' for an editable text version of the JSON state.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "NGLui: Neuroglancer States"
    ]
  },
  {
    "objectID": "release_manifests/version-117.html",
    "href": "release_manifests/version-117.html",
    "title": "117 (Jun 2021)",
    "section": "",
    "text": "Data Release v117 (June 11, 2021) is the first public release of the dataset.\nThis version introduces the following CAVE tables:\nThis data release coincides with the release of many of the Static Data Repositories, including:\nInformation about the version 117 release can be queried with the following:\n# report timestamp metadata on the version of interest\nclient.materialize.get_version_metadata(117)['time_stamp']\n\ndatetime.datetime(2021, 6, 11, 8, 10, 0, 215114, tzinfo=datetime.timezone.utc)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "117 (Jun 2021)"
    ]
  },
  {
    "objectID": "release_manifests/version-117.html#synapse-table",
    "href": "release_manifests/version-117.html#synapse-table",
    "title": "117 (Jun 2021)",
    "section": "Synapse Table",
    "text": "Synapse Table\nTable name: synapses_pni_2\nThe only synapse table is synapses_pni_2. This is by far the largest table in the dataset with 337 million entries, one for each synapse. It contains the following columns:\n\nSynapse table column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npre_pt_position / pre_pt_supervoxel_id / pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse.\n\n\npost_pt_position / post_pt_supervoxel_id / post_pt_root_id\nIThe bound spatial point data for the postsynaptic side of the synapse.\n\n\nsize\nThe size of the synapse in voxels. This correlates well, but not perfectly, with the surface area of synapse.\n\n\nctr_pt_position\nA position in the center of the detected synaptic junction. Of all points in the synapse table, this is usually the closest point to the surface (and thus mesh) of both neurons. Because it is at the edge of cells, it is not associated with a root id.\n\n\n\n# Synapse query: outputs\nclient.materialize.synapse_query(pre_ids=example_root_id)\n\n# Synapse query: inputs\nclient.materialize.synapse_query(post_ids=example_root_id)\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "117 (Jun 2021)"
    ]
  },
  {
    "objectID": "release_manifests/version-117.html#nucleus-table",
    "href": "release_manifests/version-117.html#nucleus-table",
    "title": "117 (Jun 2021)",
    "section": "Nucleus Table",
    "text": "Nucleus Table\nTable name: nucleus_detection_v0\nNucleus detection has been used to define unique cells in the dataset. Distinct from the neuronal segmentation, a convolutional neural network was trained to segment nuclei. Each nucleus detection was given a unique ID, and the centroid of the nucleus was recorded as well as its volume. Many other tables in the dataset are reference tables on nucleus_detection_v0, meaning they are linked by the same annotation id. The id of the segmented nucelus, a 6-digit integer, is static across data versions and for this reason is the preferred method to identify the same ‘cell’ across time.\nThe key columns of nucleus_detection_v0 are:\n\nNucleus table column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\n6-digit number of the segmentation for that nucleus; ‘nucleus ID’\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\n\nNote that the id column is the nucleus ID, also called the ‘soma ID’ or the ‘cell ID’.\n# Standard query\nclient.materialize.query_table('nucleus_detection_v0')\n\n# Content-aware query\nclient.materialize.tables.nucleus_detection_v0(id=example_nucleus_id).query()\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "117 (Jun 2021)"
    ]
  },
  {
    "objectID": "release_manifests/version-117.html#neuron-nucleus-table",
    "href": "release_manifests/version-117.html#neuron-nucleus-table",
    "title": "117 (Jun 2021)",
    "section": "Neuron Nucleus Table",
    "text": "Neuron Nucleus Table\nTable name: nucleus_neuron_svm\nNucleus detection classified as likely neuron.\n\n\n\n\n\n\nDeprecated table\n\n\n\nThis table remains available from materialization versions: 117, 343, 795, 943, 1078\nUse nucleus_ref_neuron_svm instead, which is a reference on the nucleus table nucleus_detection_v0.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "117 (Jun 2021)"
    ]
  },
  {
    "objectID": "release_manifests/version-117.html#automated-cell-type-classifier-coarse-version-1",
    "href": "release_manifests/version-117.html#automated-cell-type-classifier-coarse-version-1",
    "title": "117 (Jun 2021)",
    "section": "Automated cell-type classifier (coarse, version 1)",
    "text": "Automated cell-type classifier (coarse, version 1)\nTable name: allen_soma_coarse_cell_class_model_v1\nThis is a model developed by Leila Elabbady and Forrest Collman, it uses features extracted from the somatic region and nucleus segmentation (developed in collaboration with Shang Mu and Gayathri Mahalingam). Those features included the number of soma synapses, the somatic area, the somatic area to volume ratio, the density of somatic synapses, teh volume of the soma, the depth in cortex of the cell (based upon the y coordinate after a 5 degree rotation), the nucleus area, the ratio of the nucleus area to nucleus volume, the average diameter of the proximal processes of the cell, the area of the nucleus with a fold, the fraction of the nucleus area within a fold, the volume of the nucleus, and the ratio of the volume of the nucleus to the volume of the soma. The model was trained using labels from the allen_v1_column_types_v2 table supplemented with NP labels from allen_minnie_extra_types as of version 91 . The model is a SVM classifier, using an rbf kernel, with class balance. On 20% of the data held out the model has a 77% accuracy, with principal confusion between layer 6 IT and CT, and layer 5 IT with layer 4 and many types.\n\n\n\n\n\n\nDeprecated table\n\n\n\nThis table remains available from materialization versions: 117\nUse the current version aibs_metamodel_celltypes_v661 instead.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "117 (Jun 2021)"
    ]
  },
  {
    "objectID": "release_manifests/version-117.html#flat-segmentation",
    "href": "release_manifests/version-117.html#flat-segmentation",
    "title": "117 (Jun 2021)",
    "section": "Flat Segmentation",
    "text": "Flat Segmentation\n\nminnie65 flat segmentation v117\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nProofread Segmentation (v117)\nminnie65\nhttps://storage.googleapis.com/iarpa_microns/minnie/minnie65/seg\nMulit-resolution flat / static cellular segmentation voxels and meshes from 8,8,40 nm and above\nPrecomputed Shareded Compressed Segmentation (12 TB)\n\n\n\nThis contains the fixed state of the cellular segmentation at each version, where each voxel has been assigned an ID which is unique to each cellular object at 8,8,40, along with downsampled versions. Not all objects have been proofread, but a summary of the most focused efforts on cells can be found in the proofreading status metadata. In addition the mesh folder contains meshes of each object available at 3 different levels of downsampling. Folder contains many files, for download use cloud-volume, tensor-store, for bulk download use igneous, AWS CLI or gsutil CLI.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "117 (Jun 2021)"
    ]
  },
  {
    "objectID": "release_manifests/version-1300.html",
    "href": "release_manifests/version-1300.html",
    "title": "1300 (Jan 2025)",
    "section": "",
    "text": "Data Release v1300 (January 13, 2025) is the first quarterly release of 2025.\nThis version ADDS the following tables\nThe following tables have updated status or entries, from VORTEX proofreading and annotations\nThe following tables from previous versions are included:\nThe following tables have been REMOVED (removed tables still available in version 1181):\nInformation about the version 1300 release can be queried with the following:\n# report timestamp metadata on the version of interest\nclient.materialize.get_version_metadata(1300)['time_stamp']\n\ndatetime.datetime(2025, 1, 13, 10, 10, 1, 286229, tzinfo=datetime.timezone.utc)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1300 (Jan 2025)"
    ]
  },
  {
    "objectID": "release_manifests/version-1300.html#synapse-target-classifier",
    "href": "release_manifests/version-1300.html#synapse-target-classifier",
    "title": "1300 (Jan 2025)",
    "section": "Synapse-target classifier",
    "text": "Synapse-target classifier\nTable name: synapse_target_predictions_ssa\nThis table contains synapse target structure predictions (‘soma’, ‘shaft’, ‘spine’) for synapses in synapses_pni_2. Predictions are currently available for the majority of cells predicted to be neurons by aibs_metamodel_mtypes_v661_v2. The predictions come from a model trained on synapse target annotations from vortex_compartment_targets as well as some additional manual annotations where the model was failing.\nThe model is a random forest trained on spectral shape analysis features from the mesh. Current failure modes include some stubby spines being predicted as shafts, and some very thin dendrites being classified as spines.\nIt contains the following columns:\n\nSynapse target predictions column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npre_pt_position / pre_pt_supervoxel_id / pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse.\n\n\npost_pt_position / post_pt_supervoxel_id / post_pt_root_id\nThe bound spatial point data for the postsynaptic side of the synapse.\n\n\nsize\nThe size of the synapse in voxels. This correlates well, but not perfectly, with the surface area of synapse.\n\n\nctr_pt_position\nA position in the center of the detected synaptic junction. Of all points in the synapse table, this is usually the closest point to the surface (and thus mesh) of both neurons. Because it is at the edge of cells, it is not associated with a root id.\n\n\ntag\nlabel of the post-synaptic structure, one of: spine, shaft, soma\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on synapses_pni_2, and can be indexed by the same synapse id.\n\n\n# Standard query\nclient.materialize.query_table('synapse_target_predictions_ssa', limit=10)\n\n# Content-aware query\nclient.materialize.tables.synapse_target_predictions_ssa(post_pt_root_id=example_root_id).query()\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1300 (Jan 2025)"
    ]
  },
  {
    "objectID": "release_manifests/version-1300.html#automated-coregistration",
    "href": "release_manifests/version-1300.html#automated-coregistration",
    "title": "1300 (Jan 2025)",
    "section": "Automated Coregistration",
    "text": "Automated Coregistration\nTable name: coregistration_auto_phase3_fwd_apl_vess_combined_v2\nA table of EM nucleus centroids automatically matched to Baylor functional units. This table reconciles the following two tables that both make a best match of the of registration using different techniques: coregistration_auto_phase3_fwd_v2 and apl_functional_coreg_vess_fwd. A unique functional unit is identified by its session, scan_idx and unit_id. An EM nucleus centroid may have matched to more than one functional unit if it was scanned on more than one imaging field.\nThe key columns are:\n\nCoregistration table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nsession\nThe session index from functional imaging\n\n\nscan_idx\nThe scan index from functional imaging\n\n\nunit_id\nThe functional unit index from imaging. Only unique within scan and session\n\n\nfield\nThe field index from functional imaging\n\n\nresidual\nThe residual distance between the functional and the assigned structural points after transformation, in microns\n\n\nscore\nA separation score, measuring the difference between the residual distance to the assigned neuron and the distance to the nearest non-assigned neuron, in microns. This can be negative if the non-assigned neuron is closer than the assigned neuron. Larger values indicate fewer nearby neurons that could be confused with the assigned neuron.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis table includes duplicate entries for the same pt_root_id and nucleus id if the coregistered cell has multiple unit recordings\n\n\n# Standard query\nclient.materialize.query_table('coregistration_auto_phase3_fwd_apl_vess_combined_v2')\n\n# Content-aware query\nclient.materialize.tables.coregistration_auto_phase3_fwd_apl_vess_combined_v2(id=example_nucleus_id).query()\nThis table coregistration_auto_phase3_fwd_apl_vess_combined_v2 supercedes previous iterations of this table:\n\ncoregistration_auto_phase3_fwd_apl_vess_combined_v2\napl_functional_coreg_forward_v5\n\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1300 (Jan 2025)"
    ]
  },
  {
    "objectID": "release_manifests/version-1300.html#publication-gamlin-et-al.",
    "href": "release_manifests/version-1300.html#publication-gamlin-et-al.",
    "title": "1300 (Jan 2025)",
    "section": "Publication: Gamlin et al.",
    "text": "Publication: Gamlin et al.\n\nMartinotti cells\nTable name: gamlin_2023_mcs\nA table containing the manually selected Martinotti cells (MC) included in (Gamlin et al. 2025).\nThe key columns are:\n\nGamlin et al. Martinotti cells\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nallen_cortex_inhibitory.\n\n\ncell_type\nMartinotti\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('gamlin_2023_mcs')\n\n# Content-aware query\nclient.materialize.tables.gamlin_2023_mcs(id=example_nucleus_id).query()\n\n\nMartinotti cell synaptic targets\nTable name: cg_cell_type_calls\nManual cell type calls on the targets of L5 Martinotti cells (MC) in gamlin_2023_mcs, annotated for (Gamlin et al. 2025).\nIt contains the following columns:\n\nGamlin et al. Martinotti targets\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npre_pt_position / pre_pt_supervoxel_id / pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse.\n\n\npost_pt_position / post_pt_supervoxel_id / post_pt_root_id\nIThe bound spatial point data for the postsynaptic side of the synapse.\n\n\nsize\nThe size of the synapse in voxels. This correlates well, but not perfectly, with the surface area of synapse.\n\n\nctr_pt_position\nA position in the center of the detected synaptic junction. Of all points in the synapse table, this is usually the closest point to the surface (and thus mesh) of both neurons. Because it is at the edge of cells, it is not associated with a root id.\n\n\nclassification_system\ncg_calls\n\n\ncell_type\nThe excitatory subtype or inhibitory type of the target. One of: ‘1P’, ‘23P’, ‘4P’, ‘5P_IT’, ‘5P_PT’, ‘5P_NP’, ‘6P’, ‘INH’\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on synapses_pni_2, and can be indexed by the same synapse id.\n\n\n# Standard query\nclient.materialize.query_table('cg_cell_type_calls')\n\n# Content-aware query\nclient.materialize.tables.cg_cell_type_calls(pre_pt_root_id=example_root_id).query()\n\n\nMartinotti cell MET-types\nTable name: gamlin_2023_mcs_met_types\nA table matching Martinotti cells (MC) included in (Gamlin et al. 2025) to link the EM and MET-types (morphology, electrophysiology, transcriptomic cell types).\nThe key columns are:\n\nGamlin et al. Martinotti MET types\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\ntag\npredicted MET-type label\n\n\nvalue\nprediction reliability from the MET-type classifier, specifically the fraction of times that the cell was classified under that MET-type label in all the folds of the cross-validation\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('gamlin_2023_mcs_met_types')\n\n# Content-aware query\nclient.materialize.tables.gamlin_2023_mcs_met_types(id=example_nucleus_id).query()\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1300 (Jan 2025)"
    ]
  },
  {
    "objectID": "release_manifests/version-1300.html#vortex-efforts",
    "href": "release_manifests/version-1300.html#vortex-efforts",
    "title": "1300 (Jan 2025)",
    "section": "VORTEX Efforts",
    "text": "VORTEX Efforts\n\nProofreading Status and Strategy\nTable name: proofreading_status_and_strategy\nThe table proofreading_status_and_strategy describes the status of cells that have undergone manual proofreading.\nBecause of the inherent difference in the difficulty and time required for different kinds of proofreading, we describe the status of axons and dendrites separately.\nEach compartment status may be either:\n\nFALSE: indicates no comprehensive proofreading has been performed, or is not applicable.\n\nTRUE: indicates that false merges have been comprehensively removed, and the compartment is at least ‘clean’. Consult the strategy column if completeness of the compartment is relevant to your research.\n\nAn axon or dendrite labeled as status=TRUE can be trusted to be correct, but may not be complete. The degree of completion can be read from the strategy column. For more information, please see Proofreading and Data Quality; or also the microns-explorer page on proofreading strategies.\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made. NOTE: if this does not match the pt_root_id then the cell has undergone further changes. This is usually and improvement in proofreading, but proceed with caution.\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. May be TRUE or FALSE\n\n\nstatus_axon\nThe status of the axon proofreading. May be TRUE or FALSE\n\n\nstrategy_dendrite\nThe strategy employed to proofread the dendrite. See strategy table below for details\n\n\nstrategy_axon\nThe strategy employed to proofread the axon. See strategy table below for details\n\n\n\nThe specific strategies are as follows (and will update over time):\n\nProofreading Strategies\n\nProofreading Strategy Table\n\n\n\n\n\n\nStrategy\nDescription\n\n\n\n\nnone\nNo cleaning, and no extension. Indicates an entry in proofreading_status that is FALSE for that compartment\n\n\ndendrite_clean\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. The dendrite may be incorrectly truncated by segmentation error. Not all dendrite tips have been checked for extension. No comprehensive attempt was made to re-attach spine heads.\n\n\ndendrite_extended\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. Every tip was identified, manually inspected, and extended if possible. No comprehensive attempt was made to re-attach spine heads.\n\n\naxon_column_truncated\nAThe axon was extended within the V1 cortical column, with a preference for local connections. In some cases the axon was cut at the column boundary and/or the layer boundary, especially the boundary between layers 2/3 and layer 4. Output synapses represent a sampling of potential partners\n\n\naxon_interareal\nThe axon was extended with a preference for branches that projected to other brain areas. Some axon branches were fully extended, but local connections may be incomplete. Output synapses represent a sampling of potential partners.\n\n\naxon_partially_extended\nThe axon was extended outward from the soma, following each branch to its termination. Output synapses represent a sampling of potential partners.\n\n\naxon_fully_extended\nAxon was extended outward from the soma, following each branch to its termination. After initial extension, every endpoint was identified, manually inspected, and extended again if possible. Output synapses represent a largely complete sampling of partners.\n\n\n\nThis table, proofreading_status_and_strategy, supercedes proofreading_status_public_release.\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()\n\n\n\nVORTEX synapse compartments\nTable name: vortex_compartment_targets\nManual labeling of synaptic targets of putative parvalbumin basket cells, and inputs to pyramidal cells as part of the VORTEX project. This is a reference table on the synapses table synapses_pni_2, and provided ground truth for the automatic classifier behind synapse_target_predictions_ssa.\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nreference id of the synapse in synapses_pni_2\n\n\ntag\nManual label of synapse type. One of: ‘spine’, ‘shaft’, ‘soma’, ‘soma_spine’, ‘orphan’, ‘other’. Orphan refers to orphan neurites; other is a catch-all for synapses that do not match other labels\n\n\npre_pt_position  pre_pt_supervoxel_id  pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse\n\n\npost_pt_position  post_pt_supervoxel_id  post_pt_root_id\nThe bound spatial point data for the postsynaptic side of the synapse\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on synapses_pni_2, and can be indexed by the same synapse id.\n\n\n# Standard query\nclient.materialize.query_table('vortex_compartment_targets', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_compartment_targets(pre_pt_root_id=example_root_id).query()\n\n\nVORTEX manual myelin\nTable name: vortex_manual_myelination_v0\nManual labeling of myelin at 1 um resolution along axons, as part of the VORTEX project. This table was created in parallel to the node of Ranvier labeling, vortex_manual_nodes_of_ranvier, and used for ground truth of the myelin_auto_tags myelination classifier.\n\nManual Myelination\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npt_position\nThe position along the axon that was evaluated for myelin. Note that this position is not always on the segmentation of the axon\n\n\nvalid_id\nThe segmentation root_id associated with this assessment is the valid_id, NOT necessarily the pt_root_id associated with the point\n\n\ntag\nMyelination status, either True or False\n\n\npt_supervoxel_id  pt_root_id\nThe bound spatial point data for the evaluated point. This may not match the valid_id due to proofreading, or spatial downsampling when evaluating the points.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe myelination was evaluated for the valid_id and not necessarily the pt_root_id.\n\n\n# Standard query\nclient.materialize.query_table('vortex_manual_myelination_v0', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_manual_myelination_v0(valid_id=example_root_id).query()\nFor more on how to interpret the table, see Annotation Tables",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1300 (Jan 2025)"
    ]
  },
  {
    "objectID": "release_manifests/version-1300.html#flat-segmentation",
    "href": "release_manifests/version-1300.html#flat-segmentation",
    "title": "1300 (Jan 2025)",
    "section": "Flat Segmentation",
    "text": "Flat Segmentation\n\nminnie65 flat segmentation v1300\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nProofread Segmentation (v1300)\nminnie65\nhttps://storage.googleapis.com/iarpa_microns/minnie/minnie65/seg_m1300\nMulit-resolution flat / static cellular segmentation voxels and meshes from 8,8,40 nm and above\nPrecomputed Shareded Compressed Segmentation (12 TB)\n\n\n\nThis contains the fixed state of the cellular segmentation at each version, where each voxel has been assigned an ID which is unique to each cellular object at 8,8,40, along with downsampled versions. Not all objects have been proofread, but a summary of the most focused efforts on cells can be found in the proofreading status metadata. In addition the mesh folder contains meshes of each object available at 3 different levels of downsampling. Folder contains many files, for download use cloud-volume, tensor-store, for bulk download use igneous, AWS CLI or gsutil CLI.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1300 (Jan 2025)"
    ]
  },
  {
    "objectID": "release_manifests/version-343.html",
    "href": "release_manifests/version-343.html",
    "title": "343 (Feb 2022)",
    "section": "",
    "text": "Data Release v343 (February 24, 2022) is the first update to the public datastack.\nThis version ADDS the following tables\nThe following tables from previous versions are included:\nThe following tables have been REMOVED (removed tables still available in version 117):\nInformation about the version 343 release can be queried with the following:\n# report timestamp metadata on the version of interest\nclient.materialize.get_version_metadata(343)['time_stamp']\n\ndatetime.datetime(2022, 2, 24, 8, 10, 0, 184668, tzinfo=datetime.timezone.utc)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "343 (Feb 2022)"
    ]
  },
  {
    "objectID": "release_manifests/version-343.html#manual-cell-types-v1-column",
    "href": "release_manifests/version-343.html#manual-cell-types-v1-column",
    "title": "343 (Feb 2022)",
    "section": "Manual Cell Types (V1 Column)",
    "text": "Manual Cell Types (V1 Column)\n\nNeurons\nTable name: allen_v1_column_types_slanted\nA subset of nucleus detections in a 100 um column (n=2204) in VISp were manually classified by anatomists at the Allen Institute into categories of cell subclasses, first distinguishing cells into classes of non-neuronal, excitatory and inhibitory; then into subclasses.\nFor the non-neuronal subclasses, see aibs_column_nonneuronal_ref\n\n\n\n\n\n\nDeprecated table\n\n\n\nThis table remains available from materialization versions: 343\nIt is recommended to use allen_v1_column_types_slanted_ref instead, which indexes on a consistent nucleus_id\n\n\n\n\nNon-neurons\nTable name: aibs_column_nonneuronal\nA subset of nucleus detections in a 100 um column (n=2204) in VISp were manually classified by anatomists at the Allen Institute into categories of cell subclasses, first distinguishing cells into classes of non-neuronal, excitatory and inhibitory; then into subclasses.\nFor the neuronal subclasses, see allen_v1_column_types_slanted_ref\n\n\n\n\n\n\nDeprecated table\n\n\n\nThis table remains available from materialization versions: 343\nIt is recommended to use aibs_column_nonneuronal_ref instead, which indexes on a consistent nucleus_id",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "343 (Feb 2022)"
    ]
  },
  {
    "objectID": "release_manifests/version-343.html#automated-cell-type-classifier",
    "href": "release_manifests/version-343.html#automated-cell-type-classifier",
    "title": "343 (Feb 2022)",
    "section": "Automated cell-type classifier",
    "text": "Automated cell-type classifier\nTable name: aibs_soma_nuc_metamodel_preds_v117\nThis table contains the results of a hierarchical classifier trained on features of the cell body and nucleus of cells. This was applied to most cells in the dataset that had complete cell bodies (e.g. not cut off by the edge of the data), detected by nucleus_detection_v0, with small-objects and multi-soma errors removed. The model was run with cell-based features as of version 117 of the dataset. For more details, see (Elabbady et al. 2025).\nThe key columns are:\n\nAIBS Soma Nuc Metamodel Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nOne of excitatory_neuron or inhibitory_neuron for detected neurons, or nonneuron for non-neurons (glia/pericytes).\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\n\n\n\n\n\n\nDeprecated table\n\n\n\nThis table remains available from materialization versions: 343, 661, 795\nThis table is deprecated, use aibs_metamodel_celltypes_v661 instead.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "343 (Feb 2022)"
    ]
  },
  {
    "objectID": "release_manifests/version-343.html#coregistration",
    "href": "release_manifests/version-343.html#coregistration",
    "title": "343 (Feb 2022)",
    "section": "Coregistration",
    "text": "Coregistration\nTable name: func_unit_em_match_release\nA table of 200 EM centroids manually matched to Baylor unit_ids in the functional scans.\n\n\n\n\n\n\nDeprecated table\n\n\n\nThis table remains available from materialization versions: 343\nThis table is deprecated, use coregistration_manual_v4 instead.\n\n\nTable name: functional_coreg\nA table of 9608 EM centroids manually matched to Baylor unit_ids in the functional scans.\n\n\n\n\n\n\nDeprecated table\n\n\n\nThis table remains available from materialization versions: 343\nThis table is deprecated, use coregistration_manual_v4 instead.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "343 (Feb 2022)"
    ]
  },
  {
    "objectID": "release_manifests/version-343.html#proofreading-status",
    "href": "release_manifests/version-343.html#proofreading-status",
    "title": "343 (Feb 2022)",
    "section": "Proofreading Status",
    "text": "Proofreading Status\nTable name: proofreading_status_public_release\nThe table proofreading_status_public_release describes the status of cells selected for manual proofreading.\nBecause of the inherent difference in the challenge and time required for different kinds of proofreading, we describe the status of axons and dendrites separately. Further, we distinguish three different categories of proofreading:\n\nnon: No proofreading has been comprehensively performed.\nclean: Proofreading has comprehensively removed false merges, but not necessarily added missing parts.\nextended: Proofreading has comprehensively removed false merges and attempted to add all or most missing parts.\n\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. One of the three categories described above\n\n\nstatus_axon\nThe status of the axon proofreading. One of the three categories described above\n\n\n\nThis table is deprecated and superseded by proofreading_status_and_strategy.\n# Set version to 1078 or before\nclient.version = 1078\n\n# Standard query\nclient.materialize.query_table('proofreading_status_public_release')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_public_release(status_axon='extended').query()\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "343 (Feb 2022)"
    ]
  },
  {
    "objectID": "release_manifests/version-343.html#flat-segmentation",
    "href": "release_manifests/version-343.html#flat-segmentation",
    "title": "343 (Feb 2022)",
    "section": "Flat Segmentation",
    "text": "Flat Segmentation\n\nminnie65 flat segmentation v343\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nProofread Segmentation (v343)\nminnie65\nhttps://storage.googleapis.com/iarpa_microns/minnie/minnie65/seg_m343\nMulit-resolution flat / static cellular segmentation voxels and meshes from 8,8,40 nm and above\nPrecomputed Shareded Compressed Segmentation (12 TB)\n\n\n\nThis contains the fixed state of the cellular segmentation at each version, where each voxel has been assigned an ID which is unique to each cellular object at 8,8,40, along with downsampled versions. Not all objects have been proofread, but a summary of the most focused efforts on cells can be found in the proofreading status metadata. In addition the mesh folder contains meshes of each object available at 3 different levels of downsampling. Folder contains many files, for download use cloud-volume, tensor-store, for bulk download use igneous, AWS CLI or gsutil CLI.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "343 (Feb 2022)"
    ]
  },
  {
    "objectID": "release_manifests/version-795.html",
    "href": "release_manifests/version-795.html",
    "title": "795 (Aug 2023)",
    "section": "",
    "text": "Data Release v795 (August 23, 2023) is the first quarterly release of 2024.\nThis version ADDS the following tables\nThe following tables from previous versions are included:\nThe following tables have been REMOVED (removed tables still available in version 661):\n# report timestamp metadata on the version of interest\nclient.materialize.get_version_metadata(795)['time_stamp']\n\ndatetime.datetime(2023, 8, 23, 8, 10, 1, 404268, tzinfo=datetime.timezone.utc)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "795 (Aug 2023)"
    ]
  },
  {
    "objectID": "release_manifests/version-795.html#automated-cell-types",
    "href": "release_manifests/version-795.html#automated-cell-types",
    "title": "795 (Aug 2023)",
    "section": "Automated Cell types",
    "text": "Automated Cell types\n\nSoma and nucleus model\nTable name: aibs_metamodel_celltypes_v661\nThis table contains the results of a hierarchical classifier trained on features of the cell body and nucleus of cells. This was applied to most cells in the dataset that had complete cell bodies (e.g. not cut off by the edge of the data), detected by nucleus_detection_v0, with small-objects and multi-soma errors removed. The model was run with cell-based features as of version 661 of the dataset. For more details, see (Elabbady et al. 2025). In general, this does a good job, but sometimes confuses layer 5 inhibitory neurons as being excitatory:\nThe key columns are:\n\nAIBS Soma Nuc Metamodel Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nOne of excitatory_neuron or inhibitory_neuron for detected neurons, or nonneuron for non-neurons (glia/pericytes).\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\nThe cell types in the table are:\n\n\n\n\n\n\nSoma Nuc Metamodel Cell types\n\n\n\n\n\n\nAIBS Soma Nuc Metamodel: Cell Type definitions\n\n\n\n\n\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\n23P\nExcitatory\nLayer 2/3 cells\n\n\n4P\nExcitatory\nLayer 4 cells\n\n\n5P-IT\nExcitatory\nLayer 5 intratelencephalic cells\n\n\n5P-ET\nExcitatory\nLayer 5 extratelencephalic cells\n\n\n5P-NP\nExcitatory\nLayer 5 near-projecting cells\n\n\n6P-IT\nExcitatory\nLayer 6 intratelencephalic cells\n\n\n6P-CT\nExcitatory\nLayer 6 corticothalamic cells\n\n\nBC\nInhibitory\nBasket cell\n\n\nBPC\nInhibitory\nBipolar cell. In practice, this was used for all cells thought to be VIP cell, not only those with a bipolar dendrite\n\n\nMC\nInhibitory\nMartinotti cell. In practice, this label was used for all inhibitory neurons that appeared to be Somatostatin cell, not only those with a Martinotti cell morphology\n\n\nNGC\nInhibitory\nNeurogliaform cell. In practice, this label also is used for all inhibitory neurons in layer 1, many of which may not be neurogliaform cells although they might be in the same molecular family\n\n\nOPC\nNon-neuronal\nOligodendrocyte precursor cell\n\n\nastrocyte\nNon-neuronal\nAstrocyte\n\n\nmicroglia\nNon-neuronal\nMicroglia\n\n\npericyte\nNon-neuronal\nPericyte\n\n\noligo\nNon-neuronal\nOligodendrocyte\n\n\n\n\n\n\n# Standard query\nclient.materialize.query_table('aibs_metamodel_celltypes_v661')\n\n# Content-aware query\nclient.materialize.tables.aibs_metamodel_celltypes_v661(id=example_nucleus_id).query()\nFor more on how to interpret the table, see Annotation Tables.\n\n\nDendritic features and connectivity motifs\nTable name: aibs_metamodel_mtypes_v661_v2\nExcitatory neurons and inhibitory neurons were distinguished with the soma-nucleus model (Elabbady et al. 2025), and subclasses were assigned based on a data-driven clustering of the neuronal features. Inhibitory neurons were classified based on how they distributed they synaptic outputs onto target cells, while exictatory neurons were classified based on a collection of dendritic features.\nThis was applied to most detected neurons in the dataset that had complete cell bodies (e.g. not cut off by the edge of the data), detected by nucleus_detection_v0, with small-objects and multi-soma errors removed. The model was run with cell-based features as of version 661 of the dataset.For more details, see (Schneider-Mizell et al. 2025).\nNote that all cell-type labels in this table come from a clustering specific to this paper, and while they are intended to align with the broader literature they are not a direct mapping or a well-established convention.\nFor a more conventional set of labels on the same set of cells, look at the manual table allen_v1_column_types_slanted_ref. Cell types in that table align with those in the aibs_metamodel_celltypes_v661.\nThe key columns are:\n\nAllen Motif-type (mtype) Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nexcitatory or inhibitory\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\nThe cell types in the table are:\n\n\n\n\n\n\nM-type Cell Type definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\nL2a\nExcitatory\nA cluster of layer 2 (upper layer 2/3) excitatory neurons\n\n\nL2b\nExcitatory\nA cluster of layer 2 (upper layer 2/3) excitatory neurons\n\n\nL3a\nExcitatory\nA cluster of excitatory neurons transitioning between upper and lower layer 2/3\n\n\nL3b\nExcitatory\nA cluster of layer 3 (upper layer 2/3) excitatory neurons\n\n\nL3c\nExcitatory\nA cluster of layer 3 (upper layer 2/3) excitatory neurons\n\n\nL4a\nExcitatory\nThe largest cluster of layer 4 excitatory neurons\n\n\nL4b\nExcitatory\nAnother cluster of layer 4 excitatory neurons\n\n\nL4c\nExcitatory\nA cluster of layer 4 excitatory neurons along the border with layer 5\n\n\nL5a\nExcitatory\nA cluster of layer 5 IT neurons at the top of layer 5\n\n\nL5b\nExcitatory\nA cluster of layer 5 IT neurons throughout layer 5\n\n\nL5ET\nExcitatory\nThe cluster of layer 5 ET neurons\n\n\nL5NP\nExcitatory\nThe cluster of layer 5 NP neurons\n\n\nL6a\nExcitatory\nA cluster of layer 6 IT neurons at the top of layer 6\n\n\nL6b\nExcitatory\nA cluster of layer 6 IT neurons throughout layer 6. Note that this is different than the label “Layer 6b” which refers to a narrow band at the border between layer 6 and white matter\n\n\nL6c\nExcitatory\nA cluster of tall layer 6 cells (unsure if IT or CT)\n\n\nL6CT\nExcitatory\nA cluster of tall layer 6 cells matching manual CT labels\n\n\nL6wm\nExcitatory\nA cluster of layer 6 cells along the border with white matter\n\n\nPTC\nInhibitory\nPerisomatic targeting cells, a cluster of inhibitory neurons that target the soma and proximal dendrites of excitatory neurons. Approximately corresponds to basket cell\n\n\nDTC\nInhibitory\nDendrite targeting cells, a cluster of inhibitory neurons that target the distal dendrites of excitatory neurons. Most SST cells would be DTC\n\n\nSTC\nInhibitory\nSparsely targeting cells, a cluster of inhibitory neurons that don’t concentrate multiple synapses onto the same target neurons. Many neurogliaform cells and layer 1 interneurons fall into this category\n\n\nITC\nInhibitory\nInhibitory targeting cells, a cluster of inhibitory neurons that preferntially target other inhibitory neurons. Most VIP cells would be ITCs\n\n\n\n\n\n\n# Standard query\nclient.materialize.query_table('aibs_metamodel_mtypes_v661_v2')\n\n# Content-aware query\nclient.materialize.tables.aibs_metamodel_mtypes_v661_v2(id=example_nucleus_id).query()\n\n\nDendritic features and connectivity motifs of V1 column\nTable name: allen_column_mtypes_v2\nThis table contains the anatomical cell type clusters (m-types) for the Minnie columnar sample described in (Schneider-Mizell et al. 2025).\nThe key columns are:\n\nAllen Motif-type (mtype) Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nexcitatory or inhibitory\n\n\ncell_type\nOne of several cell types, detailed in the reference\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nConsider using aibs_metamodel_mtypes_v661_v2 instead, which is the same model run across the entire dataset.\n\n\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "795 (Aug 2023)"
    ]
  },
  {
    "objectID": "release_manifests/version-795.html#manual-cell-types",
    "href": "release_manifests/version-795.html#manual-cell-types",
    "title": "795 (Aug 2023)",
    "section": "Manual Cell types",
    "text": "Manual Cell types\n\nmanual cell typing outside V1 column\nTable name: allen_minnie_extra_types\nThis is a table to store extra manual cell typing that the Allen Insitute has done outside the column, in relation to several papers including: (Elabbady et al. 2025), (Schneider-Mizell et al. 2025), (Gamlin et al. 2025), and (Bodor, Agnes L. et al. 2023).\n\nAllen Extra Manual Cell Types Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\naibs_coarse_neuronal,aibs_coarse_excitatory, 'aibs_coarse_excitatory,aibs_coarse_inhibitory|  |cell_type| One of several cell types, depending on theclassification-system`\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis working table is only available from materialization version 795",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "795 (Aug 2023)"
    ]
  },
  {
    "objectID": "release_manifests/version-795.html#publication-bodor-et-al.",
    "href": "release_manifests/version-795.html#publication-bodor-et-al.",
    "title": "795 (Aug 2023)",
    "section": "Publication: Bodor et al.",
    "text": "Publication: Bodor et al.\n\nL5 extratelencephalic cells\nTable name: l5et_column\nAll the Layer 5 extratelencephalic cells in the V1 column used for (Bodor, Agnes L. et al. 2023)\nThe key columns are:\n\nBodor et al. L5ET\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n\n\nL5 pyramidal tract cells\nTable name: bodor_pt_cells\nPyramidal tract Layer 5 cells used for (Bodor, Agnes L. et al. 2023)\nThe key columns are:\n\nBodor et al. L5PT\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nis layer5_pt for all entries in the table\n\n\ncell_type\nbrain area of the cell body, one of: primary visual cortex v1 or higher visual areas hva\n\n\n\n\n\nTargets of L5 PT cells\nTable name: pt_synapse_targets\nTarget classification of a subset of downstream synaptic partners of PT cells in the bodor_pt_cells table, used for (Bodor, Agnes L. et al. 2023).\nThe key columns are:\n\nBodor et al. L5PT Targets\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nOne of excitatory, inhibitory, error.\n\n\ncell_type\nSubclass label or none\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on synapses_pni_2, and can be indexed by the same synapse id.\n\n\n\n\nProofread targets of L5 PT cells\nTable name: bodor_pt_target_proofread\nA proofread collection of numerically strong BC and MC targets downstream of the L5-ET cells in the bodor_pt_cells table, used for (Bodor, Agnes L. et al. 2023).\nThe key columns are:\n\nBodor et al. L5PT Targets, proofread\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nproofread_inhib_target for all entries.\n\n\ncell_type\nInhibitory subclass, one of: BC or MC\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_neuron_svm, and can be indexed by the same nucleus id.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "795 (Aug 2023)"
    ]
  },
  {
    "objectID": "release_manifests/version-795.html#count-of-cell-edits",
    "href": "release_manifests/version-795.html#count-of-cell-edits",
    "title": "795 (Aug 2023)",
    "section": "Count of cell edits",
    "text": "Count of cell edits\nTable name: cell_edits_v661\nA count of how many proofreading edits were made to each cell, one entry for every nucleus that had a valid id, calculated at v661.\nThe key columns of are:\n\nCell edit at v661\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\ngroup_id\nThe number of edits at v661\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\nThis working table is only available from materialization version 795",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "795 (Aug 2023)"
    ]
  },
  {
    "objectID": "static-repositories.html",
    "href": "static-repositories.html",
    "title": "Static Repositories",
    "section": "",
    "text": "minnie65 and minnie35 imagery\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nFine-aligned Image (EM)\nminnie65\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/em\nMulti-resolution electron microscopy (EM) imagery from 8,8,40 nm and above\nPrecomputed Image Data (117 TB)\n\n\nFine-aligned Image (EM)\nminnie65\nhttps://storage.googleapis.com/iarpa_microns/minnie/minnie65/em\nMulti-resolution electron microscopy (EM) imagery from 8,8,40 nm and above\nPrecomputed Image Data (117 TB)\n\n\nFine-aligned Image (EM)\nminnie35\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie35/em\nMulti-resolution electron microscopy (EM) imagery from 8,8,40 nm and above\nPrecomputed Image Data (55 TB)\n\n\nFine-aligned Image (EM)\nminnie35\nhttps://storage.googleapis.com/iarpa_microns/minnie/minnie35/em\nMulti-resolution electron microscopy (EM) imagery from 8,8,40 nm and above\nPrecomputed Image Data (55 TB)\n\n\n\nThis contains the fine aligned Electron Microscopy (EM) image data downsampled to 8,8,40 nm resolution stored in precomputed image format. Lower resolution downsampling is available in this bucket as well, including [16, 16, 40], [32, 32, 40], [64, 64, 40], [128, 128, 80], [256, 256, 160], [512, 512, 320], [1024, 1024, 640], [2048, 2048, 1280]. Folder contains many files, for download use cloud-volume, tensor-store, for bulk download use igneous, AWS CLI or gsutil CLI.\n\n\n\n\nminnie65 and minnie35 MicroCT\n\n\n\n\n\n\n\n\nName\nCloudpath\nShort Description\nType (size)\n\n\n\n\nMicroCT\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/microCT\nMicroCT used for inspection and alignment\nZIP (13 GB)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Static Repositories"
    ]
  },
  {
    "objectID": "static-repositories.html#imagery",
    "href": "static-repositories.html#imagery",
    "title": "Static Repositories",
    "section": "",
    "text": "minnie65 and minnie35 imagery\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nFine-aligned Image (EM)\nminnie65\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/em\nMulti-resolution electron microscopy (EM) imagery from 8,8,40 nm and above\nPrecomputed Image Data (117 TB)\n\n\nFine-aligned Image (EM)\nminnie65\nhttps://storage.googleapis.com/iarpa_microns/minnie/minnie65/em\nMulti-resolution electron microscopy (EM) imagery from 8,8,40 nm and above\nPrecomputed Image Data (117 TB)\n\n\nFine-aligned Image (EM)\nminnie35\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie35/em\nMulti-resolution electron microscopy (EM) imagery from 8,8,40 nm and above\nPrecomputed Image Data (55 TB)\n\n\nFine-aligned Image (EM)\nminnie35\nhttps://storage.googleapis.com/iarpa_microns/minnie/minnie35/em\nMulti-resolution electron microscopy (EM) imagery from 8,8,40 nm and above\nPrecomputed Image Data (55 TB)\n\n\n\nThis contains the fine aligned Electron Microscopy (EM) image data downsampled to 8,8,40 nm resolution stored in precomputed image format. Lower resolution downsampling is available in this bucket as well, including [16, 16, 40], [32, 32, 40], [64, 64, 40], [128, 128, 80], [256, 256, 160], [512, 512, 320], [1024, 1024, 640], [2048, 2048, 1280]. Folder contains many files, for download use cloud-volume, tensor-store, for bulk download use igneous, AWS CLI or gsutil CLI.\n\n\n\n\nminnie65 and minnie35 MicroCT\n\n\n\n\n\n\n\n\nName\nCloudpath\nShort Description\nType (size)\n\n\n\n\nMicroCT\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/microCT\nMicroCT used for inspection and alignment\nZIP (13 GB)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Static Repositories"
    ]
  },
  {
    "objectID": "static-repositories.html#cellular-segmentation",
    "href": "static-repositories.html#cellular-segmentation",
    "title": "Static Repositories",
    "section": "Cellular Segmentation",
    "text": "Cellular Segmentation\n\nFlat Segmentation\n\nminnie65 and minnie35 flat segmentation\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nAutomated Segmentation (v0)\nminnie65\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/autoseg\nMulit-resolution flat / static cellular segmentation voxels and meshes from 8,8,40 nm and above\nPrecomputed Shareded Compressed Segmentation (12 TB)\n\n\nProofread Segmentation (v117)\nminnie65\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/seg\nMulit-resolution flat / static cellular segmentation voxels and meshes from 8,8,40 nm and above\nPrecomputed Shareded Compressed Segmentation (12 TB)\n\n\nProofread Segmentation (v117)\nminnie65\nhttps://storage.googleapis.com/iarpa_microns/minnie/minnie65/seg\nMulit-resolution flat / static cellular segmentation voxels and meshes from 8,8,40 nm and above\nPrecomputed Shareded Compressed Segmentation (12 TB)\n\n\nProofread Segmentation (v343)\nminnie65\nhttps://storage.googleapis.com/iarpa_microns/minnie/minnie65/seg_m343\nMulit-resolution flat / static cellular segmentation voxels and meshes from 8,8,40 nm and above\nPrecomputed Shareded Compressed Segmentation (12 TB)\n\n\nProofread Segmentation (v943)\nminnie65\nhttps://storage.googleapis.com/iarpa_microns/minnie/minnie65/seg_m943\nMulit-resolution flat / static cellular segmentation voxels and meshes from 8,8,40 nm and above\nPrecomputed Shareded Compressed Segmentation (12 TB)\n\n\nProofread Segmentation (v1300)\nminnie65\nhttps://storage.googleapis.com/iarpa_microns/minnie/minnie65/seg_m1300\nMulit-resolution flat / static cellular segmentation voxels and meshes from 8,8,40 nm and above\nPrecomputed Shareded Compressed Segmentation (12 TB)\n\n\nAutomated Segmentation (v0)\nminnie35\ns3://bossdb-open-data/iarpa_microns/minnie/minnie35/autoseg\nMulit-resolution flat / static cellular segmentation voxels and meshes from 8,8,40 nm and above\nPrecomputed Shareded Compressed Segmentation (10 TB)\n\n\nProofread Segmentation (v0)\nminnie35\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie35/seg\nMulit-resolution flat / static cellular segmentation voxels and meshes from 8,8,40 nm and above\nPrecomputed Shareded Compressed Segmentation (10 TB)\n\n\nProofread Segmentation (v0)\nminnie35\nhttps://storage.googleapis.com/microns_phase3/minnie/minnie35/seg\nMulit-resolution flat / static cellular segmentation voxels and meshes from 8,8,40 nm and above\nPrecomputed Shareded Compressed Segmentation (10 TB)\n\n\n\nThis contains the fixed state of the cellular segmentation at each version, where each voxel has been assigned an ID which is unique to each cellular object at 8,8,40, along with downsampled versions. Not all objects have been proofread, but a summary of the most focused efforts on cells can be found in the proofreading status metadata. In addition the mesh folder contains meshes of each object available at 3 different levels of downsampling. Folder contains many files, for download use cloud-volume, tensor-store, for bulk download use igneous, AWS CLI or gsutil CLI.\n\n\nWatershed segmentation and affinities\n\nminnie65 and minnie35 Watershed Segmentation\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nWatershed Segmentation\nminnie65\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/ws\nThe supervoxel segmentation\nPrecomputed Shareded Compressed Segmentation (42 TB)\n\n\nWatershed Segmentation\nminnie35\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie35/ws\nThe supervoxel segmentation\nCompressed Sharded Precomputed Segmentation Data (22 TB)\n\n\n\nThe individual supervoxels predicted by the affinity network before they were agglomerated by the automated segmentation and then modified through proofreading. Folder contains many files, for download use cloud-volume, tensor-store, for bulk download use igneous, AWS CLI or gsutil CLI.\n\n\nDynamic segmentation\n\nminnie65 and minnie35 Dynamic Segmentation\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nDynamic segmentation (public)\nminnie65\ngraphene://https://minnie.microns-daf.com/segmentation/table/minnie65_public\nDynamic cellular segmentation on the chunked graph\nGraphene Segmentation\n\n\nDynamic segmentation (production)\nminnie65\ngraphene://https://minnie.microns-daf.com/segmentation/table/minnie3_v1\nDynamic cellular segmentation on the chunked graph\nGraphene Segmentation\n\n\nDynamic segmentation (production)\nminnie35\ngraphene://middleauth+https://minnie.microns-daf.com/segmentation/table/minnie35_public_v0\nDynamic cellular segmentation on the chunked graph\nGraphene Segmentation\n\n\n\nThe cellular segmentation graphene layers, segmentation on the chunkedgraph. Access through cloud-volume or CAVEclient.\n\n\nGround truth segmentation\n\nminnie65 and minnie35 Dynamic Segmentation\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nGround Truth Segmentation\nminnie65\nhttps://zenodo.org/record/5760218/files/minnie.tar.gz\nManual ground truth\nHDF5 file\n\n\n\nManual ground truth labels provided for training the automated segmentation pipeline. For details and use, see the Zenodo record associated with the manual lables: https://zenodo.org/records/5760218",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Static Repositories"
    ]
  },
  {
    "objectID": "static-repositories.html#synapse-segmentation-and-graph",
    "href": "static-repositories.html#synapse-segmentation-and-graph",
    "title": "Static Repositories",
    "section": "Synapse segmentation and graph",
    "text": "Synapse segmentation and graph\n\nSynaptic cleft segmentation\n\nminnie65 synaptic cleft segmentation\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nPSD Segmentation\nminie65\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/clefts\nVoxel segmentation of each synapse (post-synaptic density - PSD) detected\nPrecomputed Compressed Segmentation Data (127 GB)\n\n\nPSD Segmentation\nminnie35\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie35/clefts\nVoxel segmentation of each synapse (post-synaptic density - PSD) detected\nPrecomputed Compressed Segmentation Data (94 GB)\n\n\n\nThis contains a flattened segmentation of the synaptic clefts where each voxel has been assigned an ID which is unique to each synapse at 8,8,40. Folder contains many files, for download use cloud-volume, for bulk download use igneous or AWS or gsutill CLI.\n\n\nSynapse graph (versioned)\n\nminnie65 and minnie35 Synapse Graph\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nSynapse Graph (v117)\nminnie65\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/synapse_graph/synapses_pni_2.csv\nMetadata about each synapse detection, including which cellular segmentation(s) are pre/post synaptic\nCSV (47.5 GB)\n\n\nSynapse Graph\nminnie35\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie35/synapse_graph/assigned.csv.gz\nMetadata about each synapse detection, including which cellular segmentation(s) are pre/post synaptic\nCSV (14.5 GB)\n\n\n\nThis CSV contains columns of metadata about the synapse detections. Note that this is as of the segmentation at version 117; pre_pt_root_id and post_pt_root_id are pinned to that version.\nColumn Descriptions:\n\nid: corresponds to the ID from the PSD segmentation volume\nvalid: internal check, uniformly ‘t’\npre_pt_position_{x,y,z}: the location in 4,4,40 nm voxels of the pre-synaptic point\npost_pt_position_{x,y,z}: the location in 4,4,40 nm voxels of the post-synaptic point\nctr_pt_position_{x,y,z}: the location of the center of mass of the PSD segmentation in 4,4,40 nm voxels\npre_pt_supervoxel_id: the ID of the supervoxel from the watershed segmentation that is under the pre_pt_position\npost_pt_supervoxel_id: the ID of the supervoxel from the watershed segmentation that is under the post_pt_position\npre_pt_root_id: the ID of the segment/root_id under the pre_pt_position from the Proofread Segmentation (v117)\npost_pt_root_id: the ID of the segment/root_id under the post_pt_position from the Proofread Segmentation (v117)\nsize: the count of voxels in 4,4,40 nm resolution of the PSD segmentation object",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Static Repositories"
    ]
  },
  {
    "objectID": "static-repositories.html#nucleus-segmentation-and-classification",
    "href": "static-repositories.html#nucleus-segmentation-and-classification",
    "title": "Static Repositories",
    "section": "Nucleus segmentation and classification",
    "text": "Nucleus segmentation and classification\n\nNucleus segmentation\n\nminnie65 nucleus segmentation\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nNucleus Segmentation\nminnie65\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/nuclei\nVoxel segmentation and meshes of each cell nucleus detected in image volume\nPrecomputed Compressed Segmentation Data (26.8 GB)\n\n\nNucleus Segmentation\nminnie65\nhttps://storage.googleapis.com/iarpa_microns/minnie/minnie65/nuclei\nVoxel segmentation and meshes of each cell nucleus detected in image volume\nPrecomputed Compressed Segmentation Data (26.8 GB)\n\n\n\nThis contains a flattened segmentation of the nucleus segmentation where each voxel has been assigned an ID which is unique to each nucleus at 8,8,40. Folder contains many files, for download use cloud-volume, for bulk download use igneous or AWS CLI.\n\n\nNucleus detection and centroids\n\nminnie65 nucleus detection\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nNucleus Detection\nminnie65\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/nucleus_detection/nucleus_detection_v0.csv\nMetadata about each nucleus detection, including the cellular segment that it overlaps with\nCSV (10.5 MB)\n\n\n\nA table of nuclei detections from a nucleus detection model developed by Shang Mu, Leila Elabbady, Gayathri Mahalingam and Forrest Collman. Only included nucleus detections of volume&gt;25 um^3, below which detections are false positives, though some false positives above that threshold remain.\nColumn Descriptions:\n\nid: corresponds to the ID from the nucleus detection and segmentation\nvalid: internal check, uniformly ‘t’\npt_position_{x,y,z}: the location in 4,4,40 nm voxels of the nucleus location\npt_supervoxel_id: the ID of the supervoxel from the watershed segmentation that is under the pt_position\npt_root_id: the ID of the segment/root_id under the pt_position from the Proofread Segmentation (v117).\nvolume: the volume of the nucleus detection in um^3\n\n\n\nNucleus-based cell typing\n\nminnie65 neuron nucleus classification\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nNucleus Neuron Classification\nminnie65\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/nucleus_neuron_classification/nucleus_neuron_svm.csv\nAn automated annotation of which nuclei are neurons\nCSV (12.8 MB)\n\n\n\nThis table contains a prediction about what nucleus detections are neurons and which are likely not neurons. This is based upon a model trained by Leila Elabbady for (Elabbady et al. 2025) on nucleus segmentations in Basil (Phase 2), processed for features such as volume, foldedness, location in cortex, etc, and applied to Minnie65. In Basil the model had a cross validated f1 score of .97 and a recall of .97 for neurons. Manual validation performed on a column of 1316 nuclei in Minnie65 measured a recall of .996 and a precision of .969.\nColumn Descriptions:\n\nid: corresponds to the ID from the nucleus detection and segmentation\nvalid: internal check, uniformly ‘t’\npt_position_{x,y,z}: the location in 4,4,40 nm voxels of the nucleus location\npt_supervoxel_id: the ID of the supervoxel from the watershed segmentation that is under the pt_position\npt_root_id: the ID of the segment/root_id under the pt_position from the Proofread Segmentation (v117).\nclassification_system: uniformly “is_neuron” for all entries.\ncell_type: ‘neuron’ if the classifier called this a neuron, ‘not-neuron’ if it was not classified as a neuron, this contains both non-neuronal cells as well as false positive detections.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Static Repositories"
    ]
  },
  {
    "objectID": "static-repositories.html#functional-data",
    "href": "static-repositories.html#functional-data",
    "title": "Static Repositories",
    "section": "Functional Data",
    "text": "Functional Data\nDownloads available from the following links at BossDB: IARPA MICrONS Minnie, and also as Neuro-data without borders (.NWB) format from DANDI: MICrONS Two Photon Functional Imaging\n\nStimulus presentation (movies)\n\nFunctional Data: Stimulus Presentation\n\n\n\n\n\n\n\n\nName\nCloudpath\nShort Description\nType (size)\n\n\n\n\nStimulus Presentation\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/functional_data/stimulus_movies\nVisual stimulus presented during functional imaging scans\nAVI (multiple, 9.8 GB each, 186.2 GB total)\n\n\n\nThe visual stimulus shown to the animal in each scan for 19 scans was recreated by aligning, concatenating, and temporally filtering individual stimulus clips into a single movie, which was sampled by interpolation at scan depth frame times and saved as an avi file. Please see technical documentation for details.\n\n\nFunctional Imaging Scans (tif)\n\nFunctional Data: Functional Imaging Scans\n\n\n\n\n\n\n\n\nName\nCloudpath\nShort Description\nType (size)\n\n\n\n\nFunctional Imaging Scans\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/functional_data/two_photon_functional_scans/\nTwo-photon functional imaging scans\nTIF (multiple, 66-95 GB each, 1.3TB total)\n\n\n\nThe two-photon imaging collected during 19 scans was raster- and motion-corrected, then saved as TIF files. Please see technical documentation for details.\n\n\nStructural Imaging (z-stack)\n\nFunctional Data: Structural Imaging Stack\n\n\n\n\n\n\n\n\nName\nCloudpath\nShort Description\nType (size)\n\n\n\n\nStructural Imaging Stack\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/functional_data/two_photon_structural_stacks\nTwo-photon structural volume enclosing imaged area\nTIFF (multiple, 1.2-9.4 GB each, 10.6 GB total)\n\n\n\nTwo-photon volume imaging including vasculature label of the tissue enclosing the two-photon imaged area, saved at original and upsampled resolutions as TIF files. Please see technical documentation for details.\n\n\nDataJoint database\n\nFunctional Data: DataJoint\n\n\n\n\n\n\n\n\nName\nCloudpath\nShort Description\nType (size)\n\n\n\n\nDataJoint Database\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/functional_data/two_photon_processed_data_and_metadata\nFunctional Data, Meta Data, Experimental Data\nSQL, Containers (225 GB total)\n\n\n\nScan metadata and processed data including scan and stack metadata, synchronized stimulus movies, synchronized behavioral traces, cell segmentation masks, calcium traces, and inferred spikes, pre-ingested into a containerized MYSQL v5.7 database, schematized using DataJoint. Please see technical documentation for details.\nInstructions for setting up the containers available at https://github.com/cajal/microns-nda-access.\n\n\nDigital twin and derived data\n\nFunctional Data: Digital Twin\n\n\n\n\n\n\n\n\nName\nCloudpath\nShort Description\nType (size)\n\n\n\n\nDigital Twin Properties (netCDF)\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/functional_data/digital_twin_properties\nxarray dataset containing the digital twin neural responses and derived characteristics\nnetCDF (30.6 GB)\n\n\nDigital Twin Properties (csv)\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/functional_data/units_visual_properties\ntable containing the digital twin derived characteristics\nCSV (22 MB)\n\n\n\nA deep neural network model trained to predict visual responses has been applied to the functional recordings in the MICrONS dataset. This model predicts neural activity in response to the visual stimuli, and also estimates important visual properties of these neurons. This includes fits of their orientation and direction selectivity, as well as an estimate of their receptive field locations.\nFor details and use, see (Wang et al. 2025) and (Ding et al. 2025).\nVariables included in both the .netCDF and .csv data sources are:\n\nsession: session ID for the recording. The combination of session and scan_idx uniquely identifies the recording\nscan_idx: scan ID for the recording. The combination of session and scan_idx uniquely identifies the recording scan\nunit_id: functional unit ID, unique per recording scan\npref_ori: preferred orientation in radians (0 - pi), horizontal bar moving upward is 0 and orientation increases clockwise, extracted from model responses to oriented noise stimuli\npref_dir: preferred direction in radians (0 - 2pi), horizontal bar moving upward is 0 and orientation increases clockwise, extracted from model responses to oriented noise stimuli\ngOSI: global orientation selectivity index\ngDSI: global direction selectivity index\nreadout_weight: readout weight vector\nreadout_location_x: x coordinate of the readout location per unit, in a 128 * 72 downsampled stimulus space, this is an approximation of a neuron’s receptive field center\nreadout_location_y: y coordinate of the readout location per unit, in a 128 * 72 downsampled stimulus space, this is an approximation of a neuron’s receptive field center\ncc_abs: prediction performance of the model, higher is better\ncc_max: tuning reliability of functional units, higher is better\n\nThe following data structures are available ONLY in the netCDF files:\n\nnatural_movie: a concatenated collection of 10-sec naturalistic movie clips (netCDF file only)\nnatural_response: model responses to natural_movie, each 10-sec movie clip is fed independently to the model, and the responses are concatenated\n\nThe following variables are available ONLY in the .csv file.\n\noracle_scan_set_hash: metadata to lookup the matching the visual stimulus set\nori_scan_set_hash: metadata to lookup the matching the visual stimulus set\ndynamic_model_scan_set_hash: metadata to lookup the matching the digital twin model responses",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Static Repositories"
    ]
  },
  {
    "objectID": "static-repositories.html#proofreading-status",
    "href": "static-repositories.html#proofreading-status",
    "title": "Static Repositories",
    "section": "Proofreading Status",
    "text": "Proofreading Status\n\n\n\n\n\n\nImportant\n\n\n\nThis table is available for posterity, but it is highly recommended you use the up-to-date proofreading status available through CAVE. See CAVE Query: Proofread Cells for the preferred workflow\n\n\n\nminnie65 Proofreading Status\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nProofreading Status\nminnie65\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/proofreading_status/proofreading_status_public_release.csv\nMetadata about which cells have undergone what level of proofreading\nCSV (56 KB)\n\n\n\nThe proofreading status of neurons that have been comprehensively proofread as of v117. Axon and dendrite compartment status are marked separately under ‘axon_status’ and ‘dendrite_status’, as proofreading effort was applied differently to the different compartments in some cells. There are three possible status values for each compartment: ‘non’ indicates no comprehensive proofreading. ‘clean’ indicates that all false merges have been removed, but all tips have not necessarily been followed. ‘extended’ indicates that the cell is both clean and all tips have been followed as far as a proofreader was able to. Very small false axon merges (axon fragments approximately 5 microns or less in length) were considered acceptable for clean neurites. Note that this table does not list all edited cells, but only those with comprehensive effort toward the status mentioned here. It is meant to serve as a resource for analysis as to a list of objects that have undergone different levels of quality control by humans.\nColumn Descriptions:\n\nid: a unique identifier for this row\nvalid: internal check, uniformly ‘t’\npt_position_{x,y,z}: the location in 4,4,40 nm voxels at a cell body or similar core position for the cell\npt_supervoxel_id: the ID of the supervoxel from the watershed segmentation that is under the pt_position\npt_root_id: the ID of the segment/root_id under the pt_position from the Proofread Segmentation (v117).\nvalid_id: the root id when the proofreading was last checked. If the current root id in ‘pt_root_id’ is not the same as ‘valid_id’, there is no guarantee that the proofreading status is correct. Should not happen be true for all rows in this release.\nstatus_dendrite: the status of proofreading for this cell’s dendrites. (clean, extended, non)\nstatus_axon: the status of proofreading for this cell’s axon. (clean, extended, non)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Static Repositories"
    ]
  },
  {
    "objectID": "static-repositories.html#functional-co-registration",
    "href": "static-repositories.html#functional-co-registration",
    "title": "Static Repositories",
    "section": "Functional Co-registration",
    "text": "Functional Co-registration\n\n\n\n\n\n\nImportant\n\n\n\nThis table is available for posterity, but it is highly recommended you use the up-to-date coregistration table available through CAVE. See Annotation Tables: Functional Coregistration Tables for current best practices.\n\n\n\nminnie65 Functional Co-registration\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nFunctional Co-registration\nminnie65\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/functional_coregistration/func_unit_em_match_release.csv\nMetadata about which cellular segmentations correspond to which functional ROIs\nCSV (14KB)\n\n\n\nA table of EM nuclear centroids manually matched to corresponding units from the functional scans. Functional imaging performed by Paul Fahey and Jake Reimer of BCM. A functional unit is uniquely identified by its session, scan_idx and unit_id. An EM centroid may been present in more than one imaging field and therefore be associated with more than one functional unit. Coregistration of Two-Photon imaging data and EM data performed by AIBS. Coregistration: Nuno da Costa and Mark Takeno of AIBS generated correspondence points between the datasets and Dan Kapner of AIBS fit the transform. (Github: https://github.com/AllenInstitute/em_coregistration/tree/phase3). Matching: Functional unit to EM cell matching protocol developed by Stelios Papadopoulos of BCM and performed by trained personnel. Briefly, a summary image of the functional imaging field was compared to its corresponding plane from the coregistered EM volume. Both nearby neuronal somas and vessel features were used as fiducials to confirm the accuracy of coregistration locally and to determine the functional unit to EM cell match.\nColumn Descriptions:\n\nid: a unique identifier for this row\nvalid: internal check, uniformly ‘t’\npt_position_{x,y,z}: the location in 4,4,40 nm voxels at a cell body or similar core position for the cell\npt_supervoxel_id: the ID of the supervoxel from the watershed segmentation that is under the pt_position\npt_root_id: the ID of the segment/root_id under the pt_position from the Proofread Segmentation (v117).\nsession: the ID indicating the imaging period for the mouse (combination of session and scan_idx indicate a distinct recording)\nscan_idx: the index of the scan within the imaging session (combination of session and scan_idx indicate a distinct recording)\nunit_id: the ID of the functional ROI (unique per scan)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Static Repositories"
    ]
  },
  {
    "objectID": "static-repositories.html#neuron-skeletons-v661",
    "href": "static-repositories.html#neuron-skeletons-v661",
    "title": "Static Repositories",
    "section": "Neuron Skeletons (v661)",
    "text": "Neuron Skeletons (v661)\n\n\n\n\n\n\nImportant\n\n\n\nThese skeletons are available for posterity, but it is highly recommended you use the up-to-date skeletons and meshes available through CAVE. See Download Skeletons for the preferred workflow, or the v1300 section below for a more recent static repository\n\n\n\nminni65 SWC Skeletons v661\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nSWC Skeleton\nminnie65\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie/minnie65/skeletons/v661/skeletons\nGraph neuron with nodes, the radius, and compartment\n.csv (many files)\n\n\n\nThe swc files contain a simple graphical representation of neurons that has nodes with their 3d points in space, the estimated radius of those points and the compartment of the node (axon, (basal) dendrite, apical dendrite, soma).\n\nminnie65 Meshwork Skeletons v661\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nMeshwork Skeleton\nminnie65\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie/minnie65/skeletons/v661/meshworks\nGraphical representation of neurons as networkx object)\n.h5 (many files)\n\n\n\nEach h5 file here can be ingested into a meshwork object in python with the package MeshParty. This meshwork integrates anatomical data and other user defined annotations such as synapses. The meshworks here contain annotation tables including pre and post synaptic chemical synapses on the given neuron and the ids of the post or presynaptic neurons respectively. Unless the neuron has had a high level of proofreading, the axon has been masked and removed.\n\nminnie65 Skeleton metadata v661\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nMetadata Json\nminnie65\nhttps://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie/minnie65/skeletons/v661/metadata\nFeatures of the skeleton and processing pipeline\n.json (many files)\n\n\n\nThe json files contain metadata and various features of the neuron skeleton including the proofreading version of the microns dataset, the various ids of the given body, and information on masked out indices.\nThe variables included in the Json file are:\n\nsoma_location: location of the center of the soma for the given neuron in voxels\nsoma_id: identification number of the soma nucleus of the given neuron\nroot_id: identification number (segmentation id) of this specific version of this neuron. If the given neuron undergoes proofreading and some neuron segment is added or removed, it will be given a new root it. The root id recorded here was the root for this neuron in the current version (see ‘version’ value in this meta file)\nsupervoxel_id: identification number of a supervoxel (smallest unit of segmentation) at the center of the soma of the given neuron version MICrONS dataset materialization version from which the meshwork and skeleton was generated from. Different versions have different level of proofreading\napical_nodes: indices of excitatory skeleton nodes in this neuron that have been predicted to be apical nodes. Per the system we used, there could be no apicals, one apical, or multiple apicals. One apical is the most common\nbasal_nodes: indices of excitatory skeleton nodes predicted to be basal dendrites\ntrue_axon_nodes: Only measured in the highly proofread neurons for which we retained the axons on the skeletons. Indicates the indices of skeleton nodes that represent the axon. Compare to classified_axon_nodes below, and additional metadata on the proofreading status\ntotal_skeleton_path_length: total cable length of skeleton (in nm)\napical_length: total cable length of apical nodes along skeleton (in nm)\nbasal_length: total cable length of basal nodes along skeleton (in nm)\naxon_length: total cable length of axon nodes along skeleton (in nm)\npercent_apical_pathlength: (apical_length/total_skeleton_path_length)*100\npercent_basal_pathlength: (basal_length/total_skeleton_path_length)*100\npercent_axon_pathlength: (axon_length/total_skeleton_path_length)*100\nclassified_axon_nodes: indices of skeleton nodes that were predicted to be axons by algorithm, but were not removed during peel-back due to having a downstream segment that was identified as a dendrite. (A measure of potential error in the compartment labeling process)\nlength_remaining_classified_axon: total cable length of skeleton nodes that were predicted to be axons by algorithm, but were not removed due to having a downstream segment that was identified as a dendrite. (A measure of potential error in the compartment labeling process)\npercent_remaining_classified_axon_pathlength: (total_classified_axon_pathlength/total_skeleton_path_length)*100\n\nThe Json metadata files also include the proofreading status indicators for the cells, which determined whether the axon was excluded from skeletonization and compartment labeling. These metrics are as follows:\n\ntracing_status_dendrite: indicates the amount and mode of proofreading that has been conducted on the dendrite of the neuron. May be one of: non, clean, extended. See Proofreading Strategies for more information.\ntracing_status_axon: indicates the amount and mode of proofreading that has been conducted on axon of the neuron. May be one of: non, clean, extended. See Proofreading Strategies for more information.\n`tracing_pipeline indicates if axon is included or has been masked out/removed from the meshwork and skeletons\nclean_axon_included: indicates that the neuron has been sufficiently proofread and that the axon has not been removed and has been included in the skeleton and not masked over in the meshwork. There are 373 such neurons in this collection.\nall_axon_removed: indicates that the neuron has not been sufficiently proofread, meaning the axon is not biologically accurate and has therefor been masked and removed. Asterisk indicates that this process may not have removed or masked out all the axon. See classified_axon_nodes above.\n\nA Use Example for the v661 skeletons follows, and requires the convenience python package skeleton_plot\npip install skeleton_plot\n\nSet paths to the public repository\nThe skeletons of the meshes are calculated at specific timepoints. This collection of neurons in the dataset was at materialization version 661.\nimport skeleton_plot as skelplot\nimport skeleton_plot.skel_io as skel_io\nimport matplotlib.pyplot as plt\n\n# path to the skeleton .swc files\nskel_path = \"s3://bossdb-open-data/iarpa_microns/minnie/minnie65/skeletons/v661/skeletons/\"\n\n# path to the skeleton and meshwork .h5 files\nmesh_path = \"s3://bossdb-open-data/iarpa_microns/minnie/minnie65/skeletons/v661/meshworks/\"\n\n\nExample: load cell with known nucleus id and segment id\n# Skeleton\nnucleus_id = 292685\nsegment_id = 864691135122603047\nskel_filename = f\"{segment_id}_{nucleus_id}.swc\"\n\n# load the .swc skeleton\nsk = skel_io.read_skeleton(skel_path, skel_filename)\n\n# load the meshwork (may take minutes to locate, if using public credentials)\nmesh_filename = f\"{segment_id}_{nucleus_id}.h5\"\nmw = skel_io.load_mw(mesh_path, mesh_filename)\n\nPlot skeleton\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_skel(\n    sk,\n    title=nucleus_id,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\nPlot meshwork)\nThe meshwork object (h5 file) includes additional information about the input and output synapse positions along the skeleton\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_mw_skel(\n    mw, \n    title=nucleus_id,\n    pull_radius = True,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n    plot_presyn = True,\n    plot_postsyn = True,)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Static Repositories"
    ]
  },
  {
    "objectID": "static-repositories.html#neuron-skeletons-v1300",
    "href": "static-repositories.html#neuron-skeletons-v1300",
    "title": "Static Repositories",
    "section": "Neuron Skeletons (v1300)",
    "text": "Neuron Skeletons (v1300)\n\nminnie65 SWC Skeletons v1300\n\n\n\n\n\n\n\n\n\nName\nVolume\nCloudpath\nShort Description\nType (size)\n\n\n\n\nSWC Skeleton (proofread)\nminnie65\nhttps://storage.googleapis.com/microns-static-links/skel/swc/proofread\nGraph neuron with nodes, the radius, and compartment (axon included)\n.swc (many files)\n\n\nSWC Skeleton\nminnie65\nhttps://storage.googleapis.com/microns-static-links/skel/swc/dendrite\nGraph neuron with nodes, the radius, and compartment (axon excluded)\n.swc (many files)\n\n\n\nThe swc files contain a simple graphical representation of neurons that has nodes with their 3d points in space, the estimated radius of those points and the compartment of the node (axon, (basal) dendrite, apical dendrite, soma). The proofread cells include the axons, while all unproofread cells have had their axons removed from the skeleton reconstruction for clarity.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Static Repositories"
    ]
  }
]