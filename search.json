[
  {
    "objectID": "vortex-overview.html",
    "href": "vortex-overview.html",
    "title": "Virtual Observatory of the Cortex (VORTEX)",
    "section": "",
    "text": "A BRAIN initiative funded project to bring the MICrONS dataset to the scientific research community.\nIs there an observation you could make from this dataset that would help you answer a scientific question, but you aren’t sure how to make it? Or maybe you know how to make it, but don’t have the time or resources to repair or annotate enough examples to answer your question.\nThe Virtual Observatory of the Cortex is a grant funded mechanism designed to assist you in precisely this situation. We provide the labor and experience to make corrections and annotations in the data. The results of these efforts will be integrated with an evolving version of the dataset, and shared with you and the larger scientific community.\nWe think of it as a metaphor for a large scale astronomy survey: we are providing the infrastructure and focusing mechanisms, the community is steering the observations to the spots of maximum interest, and everyone can see what comes out!",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Virtual Observatory of the Cortex (VORTEX)"
    ]
  },
  {
    "objectID": "vortex-overview.html#how-it-works",
    "href": "vortex-overview.html#how-it-works",
    "title": "Virtual Observatory of the Cortex (VORTEX)",
    "section": "How it works:",
    "text": "How it works:\n1. Submit a Request\nMembers of the scientific community submits a short request for information they believe might be contained within this dataset, but might require some work to extract. For example:\n\nproofreading axons and dendrites for a specific group of neurons\nannotating the locations of dense core vesicles in these particular cells\nannotating locations where microglia have engulfed synapses\n\n2. Develop a Plan\nA scientist at the Allen Institute will work with you on developing a plan that helps address the scientific question you are interested in, including any manual proofreading and annotation work that needs to be done.\nPlans will be designed to maximize impact across all requests when possible.\n3. Plan Execution\nA Scientific Steering Group will evaluate all the plans and prioritize the manual work based upon the resources available to the virtual observatory.\nPlans will then be carried out by staff of the virtual observatory, with no costs to the requestor.\n4. Data Sharing\nThe data produced from the efforts of the manual proofreaders is then shared with the entire scientific community for everyone to benefit from.\nData releases will occur at least quarterly.\n\nData Sharing Policy\nData created by the Vortex project, such a proofreading and annotations, will be integrated into broader publicly available dataset for everyone to use. Proofreading updates the segmentation, which will be released along with any other changes that are part of the centralized dataset, no matter their source. Annotations will be made on the dataset and released as tables via the CAVEclient, and periodically as static file dumps available via google cloud. See the specific dataset pages for details on data access.\n\n\nSubmit the Form\nAccess the submission system at:\nmicrons-explorer.org/requests | Scientific Request Form\nWe are also conducting a community survey. If you are using, have used, or plan to use the MICrONS data, give us some feedback!\nMICrONS Community Survey",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Virtual Observatory of the Cortex (VORTEX)"
    ]
  },
  {
    "objectID": "vortex-overview.html#scientific-proposals-recieved",
    "href": "vortex-overview.html#scientific-proposals-recieved",
    "title": "Virtual Observatory of the Cortex (VORTEX)",
    "section": "Scientific Proposals Recieved",
    "text": "Scientific Proposals Recieved\n\nVORTEX Requests Recieved\n\n\nRecieved\nTopic\nTask\nStatus\n\n\n\n\n\nY1\nHow does the orientation tuning of visual cortical neurons arise from the functional organization of inputs onto their dendritic tree?\nAxon extension of functionally recorded cells\nComplete (111 cells): Y1: 1078; (547 cells): Y2: 1300\n\n\n\nY1\nHow does neural computation arise from the combination of inputs along the dendritic tree?\nAxon extension of functionally recorded cells\nComplete (111 cells): Y1: 1078; (547 cells): Y2: 1300\n\n\n\nY1\nTo what extent do interareal cortical connections exhibit precise recurrent interactions, and are these connection probabilities governed by shared retinotopy?\nAxon extension of functionally recorded cells\nComplete (111 cells): Y1: 1078; (547 cells): Y2: 1300\n\n\n\nY1\nWhat are the detailed connectivity properties of cortical PV basket cells?\nAnnotation of parvalbumin interneurons\nComplete (20 cells): Y1: 1078; (5 cells): Y2: 1300\n\n\n\nY1\nHow does the connectivity pattern of SST/Chodl cells give rise to their physiological feature: synchronization of cortical states?\nAxon extension of putative Chodl cells\nIncomplete: pending putative Chodl cells\n\n\n\nY1\nHow do astrocytes interact with neurons, at multiple scales of organization and operation?\nProofread ‘cleaning’ of astrocytes\nComplete (12 cells): Y1: 1078\n\n\n\nY1\nHow does the presence of vasculature affect the distribution of electric charge due to an applied electric field (brain stimulation)?\nAnalysis support\nComplete: Y1: 1078\n\n\n\nY1\nWhat is the mechanism of electrical neuro-vasculature coupling, at the microscopic level?\nAnalysis support\nComplete: Y1: 1078\n\n\n\nY1\nCensus of spinehead apparatus\nAnnotation of pyramidal spine inputs\nComplete (8 cells): Y2: 1300\n\n\n\nY1\nOPC spacing and cilia orientation\nOPC proofreading and annotation\nIncomplete: pending resources\n\n\n\nY1\nMyelin density in cortex\nAnnotation of myelin\nComplete: Y2: 1300\n\n\n\nY1\nPerivascular fibroblast\nAnalysis support\nComplete\n\n\n\nY1\nLocal environment of spine synapses\nAnnotation of pyramidal spine inputs\nComplete (8 cells): Y2: 1300\n\n\n\nY1\nInhibitory-Excitatory connectivity motifs\nAxon extenstion of functionally coregistered cells\nComplete (547 cells: Y2:1300\n\n\n\nY2\nProximity metrics for pyramidal cell dendrites\nAnalysis support\nOngoing\n\n\n\nY2\nSubcellular distribution of large secretory vesicles\nAnnotation of dense core vesicles\nOngoing\n\n\n\nY2\nNeuromodulation at the blood-brain barrier\nAxon extenstion of putative neuromodulatory axons\nOngoing\n\n\n\nY2\nNeuron structure modeling, using skeletonized neurons\nAxon extension: pyramidal cells ; Axon extension: inhibitory cells\nOngoing\n\n\n\nY2\nMicroglia interaction at synapses\nAnnotation of microglia lysosomes\nOngoing\n\n\n\nY2\nUncovering local and long-range circuit organization\nAxon extension: pyramidal cells ; Axon extension: inhibitory cells\nOngoing\n\n\n\nY2\nConnectomics of response heterogeneity in mouse visual cortex\nAxon extension of functionally recorded cells\nOngoing\n\n\n\nY2\nDesign principles underlying astrocyte morphology\nProofread ‘cleaning’ of astrocyte\nOngoing\n\n\n\nY2\nConnection between SST subtypes and pyramidal neurons\nAxon extension: inhibitory cells\nOngoing\n\n\n\nY2\nDopaminergic projections to cortex\nAxon extenstion of putative neuromodulatory axons\nOngoing",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Virtual Observatory of the Cortex (VORTEX)"
    ]
  },
  {
    "objectID": "vortex-overview.html#project-outcomes",
    "href": "vortex-overview.html#project-outcomes",
    "title": "Virtual Observatory of the Cortex (VORTEX)",
    "section": "Project Outcomes",
    "text": "Project Outcomes\n\nYear 1: Period 1\n\n\n\nVORTEX Proofreading Plans | Year 1 Period 1\n\n\nSSG Rank\nTask\n\n\n\n\n1.6\nAxon extension of functionally recorded cells\n\n\n2.0\nProofread ‘cleaning’ of astrocytes\n\n\n2.6\nVasculature Analysis support\n\n\n3.4\nAnnotation of parvalbumin interneurons\n\n\n4.6\nAxon extension of putative Chodl cells\n\n\n\n\n\n\n\n\nProofreading edits by type\n\n\n\n\n\nAxon extension of functionally recorded cells\n\nProofreading and extending functionally coregistered pyramidal cells\n\n\n\nSynaptic connectivity added: version 943 to 1078\n\n\n\n\n\nProofreading status upgraded: version 943 to 1078\n\n\nQueried from CAVE table: proofreading_status_and_strategy:\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()\n\nProofread ‘cleaning’ of astrocytes\n\nSelecting quality astrocytes within the V1 Column, and clean to remove false-merges: 12 astrocytes, 7 cleaned\n\n\n\nSix physically apposed ‘cleaned’ astrocytes\n\n\nCAVE table: vortex_astrocyte_proofreading_status\n# Standard query\nclient.materialize.query_table('vortex_astrocyte_proofreading_status')\n\n# Content-aware query\nclient.materialize.tables.vortex_astrocyte_proofreading_status(status='clean').query()\n\nAnnotation of parvalbumin interneurons\n\nManual annotation of spine and shaft synaptic targets of basket cells: 53517 annotations\nQueried from CAVE table: vortex_compartment_targets\n# Standard query\nclient.materialize.query_table('vortex_compartment_targets', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_compartment_targets(pre_pt_root_id=example_root_id).query()\nManual annotation of myelination and nodes of Ranvier along basket cell axons: 73237 annotations\n\n\n\nMyelination annotated along basket cell (putative PV) axon\n\n\nCAVE table: vortex_manual_myelination_v0\n# Standard query\nclient.materialize.query_table('vortex_manual_myelination_v0', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_manual_myelination_v0(valid_id=example_root_id).query()\nCAVE table: vortex_manual_nodes_of_ranvier\n# Standard query\nclient.materialize.query_table('vortex_manual_nodes_of_ranvier')\n\n# Content-aware query\nclient.materialize.tables.vortex_manual_nodes_of_ranvier(valid_id=example_root_id).query()\n\n\nYear 1: Period 2\n\n\n\nVORTEX Proofreading Plans | Year 1 Period 1{.light .hover}\n\n\nSSG Rank\nTask\n\n\n\n\n1.3\nForward-tracing of pyramidal cell axons\n\n\n1.7\nAnnotation of basket cell spine targets\n\n\n2\nAnnotation of pyramidal spine input\n\n\n3.7\nAnnotation of basket cell myelin\n\n\n5\nAnnotation of spine input (any cell type)\n\n\n5\nOPC proofreading and annotation\n\n\n5.7\nAnnotation of myelin (any cell type)\n\n\n\n\n\n\n\n\nProofreading edits by type\n\n\n\n\n\nAxon extension of functionally recorded cells\n\nProofreading and extending functionally coregistered pyramidal cells\n\n\n\nSynaptic connectivity added: version 1078 to 1300\n\n\n\n\n\nProofreading status upgraded: version 1078 to 1300\n\n\nWith the combined effort across Year 1 and Year 2, VORTEX proofreading has doubled the number of synapses between excitatory cells, and tripled the number of coregistered-coregistered connections\n\n\n\nImprovement in connectivity between functionally-coregistered pyramidal cells\n\n\nQueried from CAVE table: proofreading_status_and_strategy:\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()\n\nAnnotation of spine synapses\n\nManual annotation of spine and shaft synaptic targets of 1) basket cells, and 2) inputs onto pyramidal cells: 67440 annotations\n\n\n\nDense manual annotations on the inputs to a pyramidal cell\n\n\nQueried from CAVE table: vortex_compartment_targets\n# Standard query\nclient.materialize.query_table('vortex_compartment_targets', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_compartment_targets(pre_pt_root_id=example_root_id).query()\nProvided ground truth for a new automatic classifier:\n# Standard query\nclient.materialize.query_table('synapse_target_predictions_ssa', limit=10)\n\n# Content-aware query\nclient.materialize.tables.synapse_target_predictions_ssa(post_pt_root_id=example_root_id).query()\n\nAnnotation of myelin\n\nManual annotation of myelination and nodes of Ranvier 1) along basket cell axons, and 2) dense annotations within a subvolume: 26393 annotations\nCAVE table: vortex_manual_myelination_v0\n# Standard query\nclient.materialize.query_table('vortex_manual_myelination_v0', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_manual_myelination_v0(valid_id=example_root_id).query()\nCAVE table: vortex_manual_nodes_of_ranvier\n# Standard query\nclient.materialize.query_table('vortex_manual_nodes_of_ranvier')\n\n# Content-aware query\nclient.materialize.tables.vortex_manual_nodes_of_ranvier(valid_id=example_root_id).query()",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Virtual Observatory of the Cortex (VORTEX)"
    ]
  },
  {
    "objectID": "vortex-overview.html#contributions",
    "href": "vortex-overview.html#contributions",
    "title": "Virtual Observatory of the Cortex (VORTEX)",
    "section": "Contributions",
    "text": "Contributions\nVORTEX Community Manager: Bethanny Danskin\nProofreading Management: Szi-chieh Yu, Celia David\nProofreading: Erika Neace, Rachael Swanstrom, Agnes Bodor, Elanur Acmad, Al-Mishrie Sahijuan, Joselito Astrologo, Arvin Caballes, Rey Cabasag, Jovie Cano, Jessi Deocampo, Emely Deresas, Mark Encallado, Mary Krestine Guzarem, Hannie Grace Hordillo, Logic Domme Ibanez, Angeline Libre, Geronimo Mon, Geomar Pagalan, Joren Paulines, Joshua Angel Remullo, Rezi Selgas,\nPrincipal Investigators: Forrest Collman, Nuno Da Costa, R. Clay Reid, Casey Schneider-Mizell, H. Sebastian Seung\nNIH BRAIN Initiative: U24 NS120053",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Virtual Observatory of the Cortex (VORTEX)"
    ]
  },
  {
    "objectID": "release_manifests/version-943.html",
    "href": "release_manifests/version-943.html",
    "title": "943 (Jan 2024)",
    "section": "",
    "text": "Data Release v943 (January 22, 2024) is the first quarterly release of 2024.\nThis version ADDS the following tables\nIt includes all other tables as of v795, with the following exceptions (removed tables still available in version 795):\nA flat segmentation of the meshes at version 943 is available at: precomputed://gs://iarpa_microns/minnie/minnie65/seg_m943",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "943 (Jan 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-943.html#functional-properties-of-coregistered-cells",
    "href": "release_manifests/version-943.html#functional-properties-of-coregistered-cells",
    "title": "943 (Jan 2024)",
    "section": "Functional properties of coregistered cells",
    "text": "Functional properties of coregistered cells\nTable name: functional_properties_v3_bcm\nA summary of the functional properties for each of the coregistered neurons (as of coregistration_manual_v3). For details, see [@ding_functional_2025]\nThe key columns are:\n\nFuctional properties of coregistered neurons\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\nsession\nThe session index from functional imaging\n\n\nscan_idx\nThe scan index from functional imaging\n\n\nunit_id\nThe functional unit index from imaging. Only unique within scan and session\n\n\npref_ori\npreferred orientation in radians (0 - pi), horizontal bar moving upward is 0 and orientation increases clockwise, extracted from model responses to oriented noise stimuli\n\n\npref_dir\npreferred direction in radians (0 - 2pi), horizontal bar moving upward is 0 and orientation increases clockwise, extracted from model responses to oriented noise stimuli\n\n\ngOSI\nglobal orientation selectivity index\n\n\ngDSI\nglobal direction selectivity index\n\n\ncc_abs\nprediction performance of the model, higher is better\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis table includes duplicate entries for the same ‘pt_root_id’ and nucleus id if the coregistered cell has multiple unit recordings\n\n\n# Standard query\nclient.materialize.query_table('functional_properties_v3_bcm')\n\n# Content-aware query\nclient.materialize.tables.functional_properties_v3_bcm(id=example_nucleus_id).query()\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "943 (Jan 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-943.html#manual-coregistration",
    "href": "release_manifests/version-943.html#manual-coregistration",
    "title": "943 (Jan 2024)",
    "section": "Manual Coregistration",
    "text": "Manual Coregistration\nTable name: coregistration_manual_v4\nA table of EM nucleus centroids manually matched to Baylor functional units. A unique functional unit is identified by its session, scan_idx and unit_id. An EM nucleus centroid may have matched to more than one functional unit if it was scanned on more than one imaging field.\nThe key columns are:\n\nCoregistration table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nsession\nThe session index from functional imaging\n\n\nscan_idx\nThe scan index from functional imaging\n\n\nunit_id\nThe functional unit index from imaging. Only unique within scan and session\n\n\nfield\nThe field index from functional imaging\n\n\nresidual\nThe residual distance between the functional and the assigned structural points after transformation, in microns\n\n\nscore\nA separation score, measuring the difference between the residual distance to the assigned neuron and the distance to the nearest non-assigned neuron, in microns. This can be negative if the non-assigned neuron is closer than the assigned neuron. Larger values indicate fewer nearby neurons that could be confused with the assigned neuron.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis table includes duplicate entries for the same ‘pt_root_id’ and nucleus id if the coregistered cell has multiple unit recordings\n\n\n# Standard query\nclient.materialize.query_table('coregistration_manual_v4')\n\n# Content-aware query\nclient.materialize.tables.coregistration_manual_v4(id=example_nucleus_id).query()\nThis table coregistration_manual_v4 supercedes previous iterations of this table:\n\ncoregistration_manual_v3\ncoregistration_manual\n\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "943 (Jan 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-1181.html",
    "href": "release_manifests/version-1181.html",
    "title": "1181 (Sep 2024)",
    "section": "",
    "text": "Data Release v1181 (September 16, 2024) is the third quarterly release of 2024.\nThe following tables have updated status or entries, from VORTEX proofreading and annotations\nIt includes all other tables as of v1078, with the following exceptions (removed tables still available in version 1078):",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1181 (Sep 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-1181.html#vortex-efforts",
    "href": "release_manifests/version-1181.html#vortex-efforts",
    "title": "1181 (Sep 2024)",
    "section": "VORTEX Efforts",
    "text": "VORTEX Efforts\n\nProofreading Status and Strategy\nTable name: proofreading_status_and_strategy\nThe table proofreading_status_and_strategy describes the status of cells that have undergone manual proofreading.\nBecause of the inherent difference in the difficulty and time required for different kinds of proofreading, we describe the status of axons and dendrites separately.\nEach compartment status may be either:\n\nFALSE: indicates no comprehensive proofreading has been performed, or is not applicable.\n\nTRUE: indicates that false merges have been comprehensively removed, and the compartment is at least ‘clean’. Consult the strategy column if completeness of the compartment is relevant to your research.\n\nAn axon or dendrite labeled as status=TRUE can be trusted to be correct, but may not be complete. The degree of completion can be read from the strategy column. For more information, please see Proofreading and Data Quality; or also the microns-explorer page on proofreading strategies.\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made. NOTE: if this does not match the pt_root_id then the cell has undergone further changes. This is usually and improvement in proofreading, but proceed with caution.\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. May be TRUE or FALSE\n\n\nstatus_axon\nThe status of the axon proofreading. May be TRUE or FALSE\n\n\nstrategy_dendrite\nThe strategy employed to proofread the dendrite. See strategy table below for details\n\n\nstrategy_axon\nThe strategy employed to proofread the axon. See strategy table below for details\n\n\n\nThe specific strategies are as follows (and will update over time):\n\nProofreading Strategies\n\nProofreading Strategy Table\n\n\n\n\n\n\nStrategy\nDescription\n\n\n\n\nnone\nNo cleaning, and no extension. Indicates an entry in proofreading_status that is FALSE for that compartment\n\n\ndendrite_clean\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. The dendrite may be incorrectly truncated by segmentation error. Not all dendrite tips have been checked for extension. No comprehensive attempt was made to re-attach spine heads.\n\n\ndendrite_extended\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. Every tip was identified, manually inspected, and extended if possible. No comprehensive attempt was made to re-attach spine heads.\n\n\naxon_column_truncated\nAThe axon was extended within the V1 cortical column, with a preference for local connections. In some cases the axon was cut at the column boundary and/or the layer boundary, especially the boundary between layers 2/3 and layer 4. Output synapses represent a sampling of potential partners\n\n\naxon_interareal\nThe axon was extended with a preference for branches that projected to other brain areas. Some axon branches were fully extended, but local connections may be incomplete. Output synapses represent a sampling of potential partners.\n\n\naxon_partially_extended\nThe axon was extended outward from the soma, following each branch to its termination. Output synapses represent a sampling of potential partners.\n\n\naxon_fully_extended\nAxon was extended outward from the soma, following each branch to its termination. After initial extension, every endpoint was identified, manually inspected, and extended again if possible. Output synapses represent a largely complete sampling of partners.\n\n\n\nThis table, proofreading_status_and_strategy, supercedes proofreading_status_public_release.\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()\n\n\n\nVORTEX synapse compartments\nTable name: vortex_compartment_targets\nManual labeling of synaptic targets of putative parvalbumin basket cells, and inputs to pyramidal cells as part of the VORTEX project. This is a reference table on the synapses table synapses_pni_2, and provided ground truth for the automatic classifier behind synapse_target_predictions_ssa.\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nreference id of the synapse in synapses_pni_2\n\n\ntag\nManual label of synapse type. One of: ‘spine’, ‘shaft’, ‘soma’, ‘soma_spine’, ‘orphan’, ‘other’. Orphan refers to orphan neurites; other is a catch-all for synapses that do not match other labels\n\n\npre_pt_position  pre_pt_supervoxel_id  pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse\n\n\npost_pt_position  post_pt_supervoxel_id  post_pt_root_id\nThe bound spatial point data for the postsynaptic side of the synapse\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on synapses_pni_2, and can be indexed by the same synapse id.\n\n\n# Standard query\nclient.materialize.query_table('vortex_compartment_targets', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_compartment_targets(pre_pt_root_id=example_root_id).query()\n\n\nVORTEX proofread astrocytes\nTable name: vortex_astrocyte_proofreading_status\nManual screening and proofreading of astrocytes as part of the as part of the VORTEX project.\nThe key columns are:\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nstatus\nOne of ‘non’ indicating no cleaning or extension; ‘clean’ indicating neurite and non-astrocytic elements have been manually removed; ‘extended’ meaning astrocytic processes within the volume of the astrocyte have been comprehensively reviewed and assigned to the most likely cell\n\n\nvalid_id\nThe root id of the astrocyte when it the proofreading assessment was made\n\n\n\n# Standard query\nclient.materialize.query_table('vortex_astrocyte_proofreading_status')\n\n# Content-aware query\nclient.materialize.tables.vortex_astrocyte_proofreading_status(status='clean').query()\nFor more on how to interpret the table, see Annotation Tables",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1181 (Sep 2024)"
    ]
  },
  {
    "objectID": "quickstart_notebooks/09-nglui-neuroglancer.html",
    "href": "quickstart_notebooks/09-nglui-neuroglancer.html",
    "title": "NGLui: Neuroglancer States",
    "section": "",
    "text": "Visualizing data in Neuroglancer is one of the easiest ways to explore it in its full context. The python package nglui was made to make it easy to generate Neuroglancer states from data, particularly pandas dataframes, in a progammatic manner. The package can be installed with pip install nglui.\n\nThe nglui package interacts prominantly with caveclient and annotations queried from the database. See the section on querying the database to learn more.\n\n\n\nThe nglui.parser package can be used to parse Neuroglancer states.\nThe simplest way to parse the annotations in a Neuroglancer state is to first save the state using the Share button, and then copy the state id (the last number in the URL).\nFor example, for the share URL https://neuroglancer.neuvue.io/?json_url=https://global.daf-apis.com/nglstate/api/v1/5560000195854336, the state id is 5560000195854336\nYou can then download the json and then use the annotation_dataframe function to generate a comprehensive dataframe of all the annotations in the state.\n\nimport os\nfrom caveclient import CAVEclient\nfrom nglui import parser\n\nclient = CAVEclient('minnie65_public')\n\nstate_id = 5560000195854336\nstate = client.state.get_state_json(state_id)\nparser.annotation_dataframe(state).head()\n\n\n\n\n\n\n\n\nlayer\nanno_type\npoint\npointB\nlinked_segmentation\ntags\ngroup_id\ndescription\n\n\n\n\n0\nsyns_in\npoint\n[294095, 196476, 24560]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n1\nsyns_in\npoint\n[294879, 196374, 24391]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n2\nsyns_in\npoint\n[300246, 200562, 24297]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n3\nsyns_in\npoint\n[300894, 201844, 24377]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n4\nsyns_in\npoint\n[294742, 199552, 23392]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n\n\n\n\n\nNote that tags in the dataframe are stored as a list of integers, with each integer corresponding to one of the tags in the list.\nTo get the mapping between the tag index and the tag name for each layer, you can use the tag_dictionary function.\n\nparser.tag_dictionary(state, layer_name='syns_out')\n\n{1: 'targets_spine', 2: 'targets_shaft', 3: 'targets_soma'}\n\n\n\n\n\nThe nglui.statebuilder package is used to build Neuroglancer states that express arbitrary data.\nThe Site Configuration options determine the default configurations for your StateBuilder objects. We will set this to spelunker, and set the materialization version to 661 for reproducibility.\n\nfrom nglui import statebuilder\nstatebuilder.site_utils.set_default_config(target_site='spelunker')\n\nclient.version=1300\n\nThe general pattern is that one makes a “StateBuilder” object that has rules for how to build a Neuroglancer state layer by layer, including selecting certain neurons, and populate layers of annotations.\nYou then pass a DataFrame to the StateBuiler, and the rules tell it how to render the DataFrame into a Neuroglancer link.\nThe same set of rules can be used on similar dataframes but with different data, such as synapses from different neurons.\nTo understand the detailed use of the package, please see the tutorial.\nHowever, a number of basic helper functions allow nglui to be used for common functions in just a few lines.\nFor example, to generate a Neuroglancer state that shows a neuron and its synaptic inputs and outputs, we can use the make_neuron_neuroglancer_link helper function.\n\nfrom nglui.statebuilder import helpers\n\nstatebuilder.helpers.make_neuron_neuroglancer_link(\n    client,\n    864691135441799752,\n    show_inputs=True,\n    show_outputs=True,\n    return_as='html'\n)\n\nNeuroglancer Link\n\n\nThe main helper functions are:\n\nmake_neuron_neuroglancer_link - Shows one or more neurons and, optionally, synaptic inputs and/or outputs.\nmake_synapse_neuroglancer_link - Using a pre-downloaded synapse table, make a link that shows the synapse and the listed synaptic partners.\nmake_point_statebuilder - Generate a statebuilder to map a dataframe containing points (by default, formatted like a cell types table) to a Neuroglancer link.\n\nIn all cases, please look at the docstrings for more information on how to use the functions.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "NGLui: Neuroglancer States"
    ]
  },
  {
    "objectID": "quickstart_notebooks/09-nglui-neuroglancer.html#programmatic-interaction-with-neuroglancer-states",
    "href": "quickstart_notebooks/09-nglui-neuroglancer.html#programmatic-interaction-with-neuroglancer-states",
    "title": "NGLui: Neuroglancer States",
    "section": "",
    "text": "Visualizing data in Neuroglancer is one of the easiest ways to explore it in its full context. The python package nglui was made to make it easy to generate Neuroglancer states from data, particularly pandas dataframes, in a progammatic manner. The package can be installed with pip install nglui.\n\nThe nglui package interacts prominantly with caveclient and annotations queried from the database. See the section on querying the database to learn more.\n\n\n\nThe nglui.parser package can be used to parse Neuroglancer states.\nThe simplest way to parse the annotations in a Neuroglancer state is to first save the state using the Share button, and then copy the state id (the last number in the URL).\nFor example, for the share URL https://neuroglancer.neuvue.io/?json_url=https://global.daf-apis.com/nglstate/api/v1/5560000195854336, the state id is 5560000195854336\nYou can then download the json and then use the annotation_dataframe function to generate a comprehensive dataframe of all the annotations in the state.\n\nimport os\nfrom caveclient import CAVEclient\nfrom nglui import parser\n\nclient = CAVEclient('minnie65_public')\n\nstate_id = 5560000195854336\nstate = client.state.get_state_json(state_id)\nparser.annotation_dataframe(state).head()\n\n\n\n\n\n\n\n\nlayer\nanno_type\npoint\npointB\nlinked_segmentation\ntags\ngroup_id\ndescription\n\n\n\n\n0\nsyns_in\npoint\n[294095, 196476, 24560]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n1\nsyns_in\npoint\n[294879, 196374, 24391]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n2\nsyns_in\npoint\n[300246, 200562, 24297]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n3\nsyns_in\npoint\n[300894, 201844, 24377]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n4\nsyns_in\npoint\n[294742, 199552, 23392]\nNaN\n[864691136333760691]\n[]\nNone\nNone\n\n\n\n\n\n\n\nNote that tags in the dataframe are stored as a list of integers, with each integer corresponding to one of the tags in the list.\nTo get the mapping between the tag index and the tag name for each layer, you can use the tag_dictionary function.\n\nparser.tag_dictionary(state, layer_name='syns_out')\n\n{1: 'targets_spine', 2: 'targets_shaft', 3: 'targets_soma'}\n\n\n\n\n\nThe nglui.statebuilder package is used to build Neuroglancer states that express arbitrary data.\nThe Site Configuration options determine the default configurations for your StateBuilder objects. We will set this to spelunker, and set the materialization version to 661 for reproducibility.\n\nfrom nglui import statebuilder\nstatebuilder.site_utils.set_default_config(target_site='spelunker')\n\nclient.version=1300\n\nThe general pattern is that one makes a “StateBuilder” object that has rules for how to build a Neuroglancer state layer by layer, including selecting certain neurons, and populate layers of annotations.\nYou then pass a DataFrame to the StateBuiler, and the rules tell it how to render the DataFrame into a Neuroglancer link.\nThe same set of rules can be used on similar dataframes but with different data, such as synapses from different neurons.\nTo understand the detailed use of the package, please see the tutorial.\nHowever, a number of basic helper functions allow nglui to be used for common functions in just a few lines.\nFor example, to generate a Neuroglancer state that shows a neuron and its synaptic inputs and outputs, we can use the make_neuron_neuroglancer_link helper function.\n\nfrom nglui.statebuilder import helpers\n\nstatebuilder.helpers.make_neuron_neuroglancer_link(\n    client,\n    864691135441799752,\n    show_inputs=True,\n    show_outputs=True,\n    return_as='html'\n)\n\nNeuroglancer Link\n\n\nThe main helper functions are:\n\nmake_neuron_neuroglancer_link - Shows one or more neurons and, optionally, synaptic inputs and/or outputs.\nmake_synapse_neuroglancer_link - Using a pre-downloaded synapse table, make a link that shows the synapse and the listed synaptic partners.\nmake_point_statebuilder - Generate a statebuilder to map a dataframe containing points (by default, formatted like a cell types table) to a Neuroglancer link.\n\nIn all cases, please look at the docstrings for more information on how to use the functions.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "NGLui: Neuroglancer States"
    ]
  },
  {
    "objectID": "quickstart_notebooks/07-cave-download-skeleton.html",
    "href": "quickstart_notebooks/07-cave-download-skeleton.html",
    "title": "Download Skeletons",
    "section": "",
    "text": "Often in thinking about neurons, you want to measure things along a linear dimension of a neuron.\nHowever, the segmentation and meshes are a full complex 3d shape that makes this non-trivial. There are methods for reducing the shape of a segmented neuron down to a linear tree like structure usually referred to as a skeleton. We have precalculated skeletons for a large number of cells in the dataset, and make the skeleton generation available on our server, on demand.\nThe meshes that one sees in Neuroglancer are available to download through the python client cloud-volume, and can be loaded for analysis and visualization in other tools.\nThe MICrONS data can be representing and rendered in multiple formats, at different levels of abstraction from the original imagery.\n\n\n\n\n\n\n\n\n\nSegmentation: more expensive\n\n\n\n\n\n\n\nMesh\n\n\n\n\n\n\n\nSkeleton: less expensive",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Skeletons"
    ]
  },
  {
    "objectID": "quickstart_notebooks/07-cave-download-skeleton.html#connected-morphological-representations-skeletons",
    "href": "quickstart_notebooks/07-cave-download-skeleton.html#connected-morphological-representations-skeletons",
    "title": "Download Skeletons",
    "section": "",
    "text": "Often in thinking about neurons, you want to measure things along a linear dimension of a neuron.\nHowever, the segmentation and meshes are a full complex 3d shape that makes this non-trivial. There are methods for reducing the shape of a segmented neuron down to a linear tree like structure usually referred to as a skeleton. We have precalculated skeletons for a large number of cells in the dataset, and make the skeleton generation available on our server, on demand.\nThe meshes that one sees in Neuroglancer are available to download through the python client cloud-volume, and can be loaded for analysis and visualization in other tools.\nThe MICrONS data can be representing and rendered in multiple formats, at different levels of abstraction from the original imagery.\n\n\n\n\n\n\n\n\n\nSegmentation: more expensive\n\n\n\n\n\n\n\nMesh\n\n\n\n\n\n\n\nSkeleton: less expensive",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Skeletons"
    ]
  },
  {
    "objectID": "quickstart_notebooks/07-cave-download-skeleton.html#what-is-a-precomputed-skeleton",
    "href": "quickstart_notebooks/07-cave-download-skeleton.html#what-is-a-precomputed-skeleton",
    "title": "Download Skeletons",
    "section": "What is a Precomputed skeleton?",
    "text": "What is a Precomputed skeleton?\nThese skeletons are stored in the cloud as a bytes-IO object, which can be viewed in Neuroglancer, or downloaded with CAVEclient.skeleton module. These precomputed skeletons also contain annotations on the skeletons that have the synapses, which skeleton nodes are axon and which are dendrite, and which are likely the apical dendrite of excitatory neurons.\n\n\n\npyChunkedGraph Skeleton\n\n\nFor more on how skeletons are generated from the mesh objects, and additional tools for generating, cacheing, and downloading meshes, see the Skeleton Service documentation",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Skeletons"
    ]
  },
  {
    "objectID": "quickstart_notebooks/07-cave-download-skeleton.html#caveclient-to-download-skeletons",
    "href": "quickstart_notebooks/07-cave-download-skeleton.html#caveclient-to-download-skeletons",
    "title": "Download Skeletons",
    "section": "CAVEclient to download skeletons",
    "text": "CAVEclient to download skeletons\nRetrieve a skeleton using get_skeleton(). The available output_formats (described below) are:\n\ndict containing vertices, edges, radius, compartment, and various metadata (default if unspecified)\nswc a Pandas Dataframe in the SWC format, with ordered vertices, edges, compartment labels, and radius.\n\nNote: if the skeleton doesn’t exist in the server cache, it may take 20-60 seconds to generate the skeleton before it is returned. This function will block during that time. Any subsequent retrieval of the same skeleton should go very quickly however.\n\n\n\n\n\n\nInitial Setup\n\n\n\nBefore using any programmatic access to the data, you first need to set up your CAVEclient token.\n\n\n\nfrom caveclient import CAVEclient\nimport numpy as np\nimport pandas as pd\n\nclient = CAVEclient(\"minnie65_public\")\n\n# specify the materialization version, for consistency across time\",\nclient.version = 1300\n\n# Example: pyramidal cell in v1300\nexample_cell_id = 864691135572530981\n\n\nsk_df = client.skeleton.get_skeleton(example_cell_id, output_format='swc')\n\nsk_df.head()\n\n\n\n\n\n\n\n\nid\ntype\nx\ny\nz\nradius\nparent\n\n\n\n\n0\n0\n1\n1365.120\n763.456\n812.60\n3.643\n-1\n\n\n1\n1\n3\n1368.200\n755.760\n811.24\n0.300\n0\n\n\n2\n2\n3\n1368.200\n758.552\n811.68\n1.915\n1\n\n\n3\n3\n3\n1368.200\n758.560\n812.12\n1.915\n2\n\n\n4\n4\n3\n1368.192\n758.560\n812.44\n2.815\n3\n\n\n\n\n\n\n\nSkeletons are “tree-like”, where every vertex (except the root vertex) has a single parent that is closer to the root than it, and any number of child vertices. Because of this, for a skeleton there are well-defined directions “away from root” and “towards root” and few types of vertices have special names:\nYou can see these vertices and edges represented in the alternative dict skeleton output.\n\nsk_dict = client.skeleton.get_skeleton(example_cell_id, output_format='dict')\n\nsk_dict.keys()\n\ndict_keys(['meta', 'edges', 'mesh_to_skel_map', 'root', 'vertices', 'compartment', 'radius'])\n\n\nAlternately, you can query a set of root_ids, for example all of the proofread cells in the dataset, and bulk download the skeletons\n\nprf_root_ids = client.materialize.tables.proofreading_status_and_strategy(\n    status_axon='t').query()['pt_root_id']\nprf_root_ids.head(3)\n\n0    864691135441799752\n1    864691135855890478\n2    864691136521831825\nName: pt_root_id, dtype: int64\n\n\n\nall_sk_dict = client.skeleton.get_bulk_skeletons(\n    prf_root_ids.head(3), \n    generate_missing_skeletons=False,\n    output_format='dict',\n    skeleton_version=4)\n\nall_sk_dict.keys()\n\ndict_keys(['864691135441799752', '864691135855890478', '864691136521831825'])\n\n\n\nall_sk_dict['864691135441799752'].keys()\n\ndict_keys(['meta', 'edges', 'mesh_to_skel_map', 'root', 'vertices', 'compartment', 'radius', 'lvl2_ids'])\n\n\n\nconvert dictionary to meshwork skeleton\nWe use the python package Meshparty to convert between mesh representations of neurons, and skeleton representations.\nSee Download Meshes | MeshParty for more details.\nSkeletons are “tree-like”, where every vertex (except the root vertex) has a single parent that is closer to the root than it, and any number of child vertices. Because of this, for a skeleton there are well-defined directions “away from root” and “towards root” and few types of vertices have special names:\nConverting the pregenerated skeleton sk_dict to a meshwork object sk converts the skeleton to a graph object, for convenient representation and analysis.\n\nfrom meshparty.skeleton import Skeleton\n\nsk = Skeleton.from_dict(sk_dict)",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Skeletons"
    ]
  },
  {
    "objectID": "quickstart_notebooks/07-cave-download-skeleton.html#plot-with-skeleton_plot",
    "href": "quickstart_notebooks/07-cave-download-skeleton.html#plot-with-skeleton_plot",
    "title": "Download Skeletons",
    "section": "Plot with skeleton_plot",
    "text": "Plot with skeleton_plot\nOur convenience package skeleton_plot renders the skeleton in aligned, 2D views.\npip install skeleton_plot\nOur convenience package skeleton_plot renders the skeleton in aligned, 2D views, with the compartments labeled in different colors. Here, dendrite is red, axon is blue, and soma (the root of the connected graph) is olive\n\nfrom skeleton_plot.plot_tools import plot_skel\n\nplot_skel(sk=sk,\n          invert_y=True,\n          pull_compartment_colors=True,\n          plot_soma=True,\n         )",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Skeletons"
    ]
  },
  {
    "objectID": "quickstart_notebooks/07-cave-download-skeleton.html#download-or-generate-skeletons-with-pcg_skel",
    "href": "quickstart_notebooks/07-cave-download-skeleton.html#download-or-generate-skeletons-with-pcg_skel",
    "title": "Download Skeletons",
    "section": "Download or generate skeletons with pcg_skel",
    "text": "Download or generate skeletons with pcg_skel\npcg_skel is a package used to rapidly build neuronal skeletons from electron microscopy data in the CAVE ecosystem. It integrates structural data, connectivity data, and local features stored across many aspects of a CAVE system, and creates objects as Meshparty meshes, skeletons, and MeshWork files for subsequent analysis.\nBy harnessing the way the structural data is stored, you can build skeletons for even very large neurons quickly and with little memory use.\nTo install pcg_skel, use pip.\npip install pcg_skel\nBy loading a skeleton as a meshwork skeleton, you can easily ‘rehydrate’ the mesh features of the neuron (such as synapses) and MeshParty functions (such as masking)\n\nfrom pcg_skel import get_meshwork_from_client\n\nsk_mesh = get_meshwork_from_client(example_cell_id,\n                                   client=client,\n                                   synapses=True,\n                                   restore_graph=True,\n                                   restore_properties=True,\n                                   skeleton_version=4,\n)\nsk_mesh\n\n&lt;meshparty.meshwork.meshwork.Meshwork at 0x1fe3734e110&gt;",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Skeletons"
    ]
  },
  {
    "objectID": "quickstart_notebooks/05-cloudvolume-download.html",
    "href": "quickstart_notebooks/05-cloudvolume-download.html",
    "title": "Download Imagery and Segmentation",
    "section": "",
    "text": "The electron microscopy images and their dense reconstruction into 3D objects (respectively: imagery and segmentation) are available to download through the python client cloud-volume.\nThe MICrONS data can be representing and rendered in multiple formats, at different levels of abstraction from the original imagery.\n\n\n\n\n\n\n\n\n\nSegmentation: more expensive\n\n\n\n\n\n\n\nMesh\n\n\n\n\n\n\n\nSkeleton: less expensive",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Imagery and Segmentation"
    ]
  },
  {
    "objectID": "quickstart_notebooks/05-cloudvolume-download.html#connected-morphological-representations-segmentation",
    "href": "quickstart_notebooks/05-cloudvolume-download.html#connected-morphological-representations-segmentation",
    "title": "Download Imagery and Segmentation",
    "section": "",
    "text": "The electron microscopy images and their dense reconstruction into 3D objects (respectively: imagery and segmentation) are available to download through the python client cloud-volume.\nThe MICrONS data can be representing and rendered in multiple formats, at different levels of abstraction from the original imagery.\n\n\n\n\n\n\n\n\n\nSegmentation: more expensive\n\n\n\n\n\n\n\nMesh\n\n\n\n\n\n\n\nSkeleton: less expensive",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Imagery and Segmentation"
    ]
  },
  {
    "objectID": "quickstart_notebooks/05-cloudvolume-download.html#cloud-volume-for-downloading-imagery-segmentation",
    "href": "quickstart_notebooks/05-cloudvolume-download.html#cloud-volume-for-downloading-imagery-segmentation",
    "title": "Download Imagery and Segmentation",
    "section": "Cloud-Volume for downloading imagery, segmentation",
    "text": "Cloud-Volume for downloading imagery, segmentation\nCloudVolume is a serverless Python client for random access reading and writing of Neuroglancer volumes in “Precomputed” format, a set of representations for arbitrarily large volumetric images, meshes, and skeletons.\nPrecomputed volumes are typically stored on AWS S3, Google Storage, or locally. CloudVolume can read and write to these object storage providers given a service account token with appropriate permissions. However, these volumes can be stored on any service, including an ordinary webserver or local filesystem, that supports key-value access.\ncloud-volume is the mechanism for many programmatic data queries, and integrates heavily with the CAVE ecosystem but is distinct. You can install cloud-volume with:\npip install cloud-volume\nThe Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data.\n\n\n\n\n\n\nInitial Setup\n\n\n\nBefore using any programmatic access to the data, you first need to set up your CAVEclient token. When CAVEclient saves the token to your local machine, cloudvolume will access the same token.\n\n\n\nfrom caveclient import CAVEclient\nfrom cloudvolume import CloudVolume\n\ndatastack_name = 'minnie65_public'\nclient = CAVEclient(datastack_name)\n\nThe datastack includes information about the imagery and segmentation source, and can provide that information when prompted\n\nclient.info.image_source()\n\n'precomputed://https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/em'\n\n\n\nclient.info.segmentation_source()\n\n'graphene://https://minnie.microns-daf.com/segmentation/table/minnie65_public'\n\n\n\n# this can be used to initialize a cloudvolume object\ncv = CloudVolume(client.info.segmentation_source(), progress=False, use_https=True)\n\n\nExamples from the cloudvolume README:\nimage = cv[:,:,:] # Download the entire image stack into a numpy array\nimage = cv.download(bbox, mip=2, renumber=True) # download w/ smaller dtype\nimage = cv.download(bbox, mip=2, label=777) # download binary image for label\n\nuniq = cv.unique(bbox, mip=0) # efficient extraction of unique labels\nlisting = cv.exists( np.s_[0:64, 0:128, 0:64] ) # get a report on which chunks actually exist\nexists = cv.image.has_data(mip=0) # boolean check to see if any data is there\nlisting = cv.delete( np.s_[0:64, 0:128, 0:64] ) # delete this region (bbox must be chunk aligned)\n\nimg = cv.download_point( (x,y,z), size=256, mip=3 ) # download region around (mip 0) x,y,z at mip 3\npts = cv.scattered_points([ (x1,y1,z1), (x2,y2,z2) ]) # download voxel labels located at indicated points\n\n# download image files without decompressing or rendering them. Good for caching!\nfiles = cv.download_files(bbox, mip, decompress=False)",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Imagery and Segmentation"
    ]
  },
  {
    "objectID": "quickstart_notebooks/05-cloudvolume-download.html#imageryclient-for-aligned-downloads",
    "href": "quickstart_notebooks/05-cloudvolume-download.html#imageryclient-for-aligned-downloads",
    "title": "Download Imagery and Segmentation",
    "section": "ImageryClient for aligned downloads",
    "text": "ImageryClient for aligned downloads\nWe recommend using ImageryClient for simple download imagery and segmentation data, like for generating figures. ImageryClient makes core use of cloud-volume, but adds convenience and better integration with the CAVEclient.\nYou can install ImageryClient with:\npip install imageryclient\nImageryClient is designed to download aligned blocks of imagery and segmentation data, as well as has some convenience functions for creating overlays of the two.\nImagery is downloaded as blocks of 8-bit values (0-255) that indicate grayscale intensity, while segmentation is downloaded as blocks of 64-bit integers that describe the segmentation ID of each voxel.\nAlternatively, segmentation can be kept as a dictionary of boolean masks, where each key is a root ID and each value is a boolean mask of the same shape as the imagery.\nDetailed information on the options can be found in the documentation.\nA typical example would be to use ImageryClien to download and visualize a 512x512 pixel cutout of imagery and segmentation centered on a specific location based on the coordinates in Neuroglancer:\n\nimport imageryclient as ic\n\nimg_client = ic.ImageryClient(client=client)\n\nctr = [240640, 207872, 21360]\n\nimage, segs = img_client.image_and_segmentation_cutout(ctr,\n                                                       split_segmentations=True,\n                                                       bbox_size=(512, 512),\n                                                       scale_to_bounds=True,\n)\n\nic.composite_overlay(segs, imagery=image, palette='husl').convert(\"RGB\")\n\n# Note: the final `.convert('RGB')` is needed to build this documetnation, but is not required to run locally.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Imagery and Segmentation"
    ]
  },
  {
    "objectID": "quickstart_notebooks/03-cave-query-proofread-cells.html",
    "href": "quickstart_notebooks/03-cave-query-proofread-cells.html",
    "title": "CAVE Query: Proofread Cells",
    "section": "",
    "text": "The Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Proofread Cells"
    ]
  },
  {
    "objectID": "quickstart_notebooks/03-cave-query-proofread-cells.html#caveclient",
    "href": "quickstart_notebooks/03-cave-query-proofread-cells.html#caveclient",
    "title": "CAVE Query: Proofread Cells",
    "section": "CAVEclient",
    "text": "CAVEclient\nMost programmatic access to the CAVE services occurs through CAVEclient, a Python client to access various types of data from the online services.\nFull documentation for CAVEclient is available here.\nTo initialize a caveclient, we give it a datastack, which is a name that defines a particular combination of imagery, segmentation, and annotation database. For the MICrONs public data, we use the datastack name minnie65_public.\n\nfrom caveclient import CAVEclient\ndatastack_name = 'minnie65_public'\nclient = CAVEclient(datastack_name)\n\n# Show the description of the datastack\nclient.info.get_datastack_info()['description']\n\n'This is the publicly released version of the minnie65 volume and segmentation. '\n\n\n\nMaterialization versions\nData in CAVE is timestamped and periodically versioned - each (materialization) version corresponds to a specific timestamp. Individual versions are made publicly available. The materialization service provides annotation queries to the dataset. It is available under client.materialize.\nPeriodic updates are made to the public datastack, which will include updates to the available tables. Some cells will have different pt_root_id because they have undergone proofreading.\nIt is worth checking the version of the data you are using, and specifying the version for analysis consistency.\n\n# see the available materialization versions\nclient.materialize.get_versions()\n\n[1300, 1078, 117, 661, 343, 1181, 795, 943]\n\n\nAnd these are their associated timestamps (all timestamps are in UTC):\n\nfor version in client.materialize.get_versions():\n    print(f\"Version {version}: {client.materialize.get_timestamp(version)}\")\n\nVersion 1300: 2025-01-13 10:10:01.286229+00:00\nVersion 1078: 2024-06-05 10:10:01.203215+00:00\nVersion 117: 2021-06-11 08:10:00.215114+00:00\nVersion 661: 2023-04-06 20:17:09.199182+00:00\nVersion 343: 2022-02-24 08:10:00.184668+00:00\nVersion 1181: 2024-09-16 10:10:01.121167+00:00\nVersion 795: 2023-08-23 08:10:01.404268+00:00\nVersion 943: 2024-01-22 08:10:01.497934+00:00\n\n\n\n# set materialization version, for consistency\nmaterialization = 1300 # current public as of 1/13/2025\nclient.version = materialization",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Proofread Cells"
    ]
  },
  {
    "objectID": "quickstart_notebooks/03-cave-query-proofread-cells.html#querying-proofread-neurons",
    "href": "quickstart_notebooks/03-cave-query-proofread-cells.html#querying-proofread-neurons",
    "title": "CAVE Query: Proofread Cells",
    "section": "Querying Proofread neurons",
    "text": "Querying Proofread neurons\n\nProofread neurons\nProofreading is necessary to obtain accurate reconstructions of a cell. In the MICrONS dataset, the general rule is that dendrites onto cells with a single cell body are sufficiently proofread to trust synaptic connections onto a cell. Axons on the other hand require so much proofread that only ~1,000 cells have axons that were proofread to various degrees such that their outputs can be used for analysis.\nThe table proofreading_status_and_strategy contains proofreading information about ~1,300 neurons. This website provides the most detailed overview. In brief, axons annotated with any strategy_axon were cleaned of false mergers but not all were fully extended. The most important distinction is axons annotated with axon_column_truncated were only proofread within a certain volume wheras others were proofread without such bias.\n\nproof_all_df = client.materialize.query_table(\"proofreading_status_and_strategy\", desired_resolution=[1, 1, 1], split_positions=True)\n\n\nproof_all_df[\"strategy_axon\"].value_counts()\n\nstrategy_axon\naxon_partially_extended    1459\nnone                        149\naxon_interareal             130\naxon_fully_extended         127\nName: count, dtype: int64\n\n\nWe can filter our query to only return rows that match a condition by adding a filter to our query:\n\nproof_df = client.materialize.query_table(\"proofreading_status_and_strategy\", filter_in_dict={\"strategy_axon\": [\"axon_partially_extended\", \"axon_fully_extended\", \"axon_interareal\", \"axon_column_truncated\"]}, desired_resolution=[1, 1, 1], split_positions=True)\n\n\nproof_df[\"strategy_axon\"].value_counts()\n\nstrategy_axon\naxon_partially_extended    1459\naxon_interareal             130\naxon_fully_extended         127\nName: count, dtype: int64",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Proofread Cells"
    ]
  },
  {
    "objectID": "quickstart_notebooks/01-caveclient-setup.html",
    "href": "quickstart_notebooks/01-caveclient-setup.html",
    "title": "CAVEclient Setup",
    "section": "",
    "text": "The Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data.\nThe CAVEclient is the main way to programmatically access the MICrONS data using Python.\nIn particular, the CAVEclient provides an interface to query the CAVE database for annotations such as synapses, as well as to get a variety of other kinds of information.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVEclient Setup"
    ]
  },
  {
    "objectID": "quickstart_notebooks/01-caveclient-setup.html#installation",
    "href": "quickstart_notebooks/01-caveclient-setup.html#installation",
    "title": "CAVEclient Setup",
    "section": "Installation",
    "text": "Installation\nTo install caveclient, use pip: pip install caveclient.\nOnce you have installed caveclient, to use it you need to set up your user token in one of two ways:",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVEclient Setup"
    ]
  },
  {
    "objectID": "quickstart_notebooks/01-caveclient-setup.html#setting-up-credentials",
    "href": "quickstart_notebooks/01-caveclient-setup.html#setting-up-credentials",
    "title": "CAVEclient Setup",
    "section": "Setting up credentials",
    "text": "Setting up credentials\nTo access the data programmatically, you need to set up a user token. This token is assigned by the server and functions as a both a username and password to access any aspect of the data. You will need to save this token to your computer using the tools.\n\nScenario 1: New User, No Previous Account\nIf you have never interacted with CAVE before, you will need to both create an account and get a token. Note that you can only have one token at a time, and thus if you create a new token any computer running a previous one will no longer have access.\nStep 1: Log into a CAVE site to set up a new account with a GMail-associated email address. To do this, click this link and acknowledge the terms of service associated with the MICrONS dataset. Once you have done this, your account is automatically created.\nStep 2: Generate a token. To generate a token, run the following code in a Jupyter notebook:\n\nfrom caveclient import CAVEclient\nclient = CAVEclient()\nclient.auth.setup_token(make_new=True)\n\nThis will open a new browser window and ask you to log in.\nYou will show you a web page with an alphanumeric string that is your token.\nCopy your token, and save it to your computer using the following:\n\nclient.auth.save_token(token=YOUR_TOKEN)\n\nNote that the token must be formatted as a string.\nTo check if your setup works, run the following:\n\nclient = CAVEclient('minnie65_public')\n\nIf you don’t get any errors, your setup has worked and you can move on!\n\n\nScenario 2: Existing user, New computer\nIf you have already created an account and token but not set up your computer yet, you can use the same token on a new computer.\nStep 1) Find your token by running the following code in a Jupyter notebook:\n\nfrom caveclient import CAVEclient\nclient = CAVEclient()\nclient.auth.setup_token(make_new=False)\n\nThis will open a new browser window and ask you to log in.\nAfter lgging in, the page will display your current token (the value after the token:).\nCopy this token, and save it to your computer using the following:\n\nclient.auth.save_token(token=YOUR_TOKEN, overwrite=True)\n\nNote that the token must be formatted as a string.\nTo check if your setup works, run the following:\n\nclient = CAVEclient('minnie65_public')\n\nIf you don’t get any errors, your setup has worked and you can move on!\n\n\nSomething went wrong?\nIf you are having trouble with authentication or permissions, it is probably because the token you are trying to use is not the one CAVE expects.\nTo find your current token, run the following code in a Jupyter notebook:\n\nclient = CAVEclient()\nclient.auth.token\n\nThe notebook will print the token your computer is sending to the server.\nIf this token is not the one you find from running the code in Scenario 2, follow the steps there to set up your computer with the correct token.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVEclient Setup"
    ]
  },
  {
    "objectID": "quickstart_notebooks/00_cave_quickstart.html",
    "href": "quickstart_notebooks/00_cave_quickstart.html",
    "title": "CAVE Quickstart",
    "section": "",
    "text": "The Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data."
  },
  {
    "objectID": "quickstart_notebooks/00_cave_quickstart.html#caveclient",
    "href": "quickstart_notebooks/00_cave_quickstart.html#caveclient",
    "title": "CAVE Quickstart",
    "section": "CAVEclient",
    "text": "CAVEclient\nMost programmatic access to the CAVE services occurs through CAVEclient, a Python client to access various types of data from the online services.\nFull documentation for CAVEclient is available here.\nTo initialize a caveclient, we give it a datastack, which is a name that defines a particular combination of imagery, segmentation, and annotation database. For the MICrONs public data, we use the datastack name minnie65_public.\n\nfrom caveclient import CAVEclient\ndatastack_name = 'minnie65_public'\nclient = CAVEclient(datastack_name)\n\n# Show the description of the datastack\nclient.info.get_datastack_info()['description']\n\n'This is the publicly released version of the minnie65 volume and segmentation. '\n\n\n\nMaterialization versions\nData in CAVE is timestamped and periodically versioned - each (materialization) version corresponds to a specific timestamp. Individual versions are made publicly available. The materialization service provides annotation queries to the dataset. It is available under client.materialize.\nPeriodic updates are made to the public datastack, which will include updates to the available tables. Some cells will have different pt_root_id because they have undergone proofreading.\nIt is worth checking the version of the data you are using, and specifying the version for analysis consistency.\n\n# see the available materialization versions\nclient.materialize.get_versions()\n\n[1078, 117, 661, 343, 1181, 795, 943]\n\n\nAnd these are their associated timestamps (all timestamps are in UTC):\n\nfor version in client.materialize.get_versions():\n    print(f\"Version {version}: {client.materialize.get_timestamp(version)}\")\n\nVersion 1078: 2024-06-05 10:10:01.203215+00:00\nVersion 117: 2021-06-11 08:10:00.215114+00:00\nVersion 661: 2023-04-06 20:17:09.199182+00:00\nVersion 343: 2022-02-24 08:10:00.184668+00:00\nVersion 1181: 2024-09-16 10:10:01.121167+00:00\nVersion 795: 2023-08-23 08:10:01.404268+00:00\nVersion 943: 2024-01-22 08:10:01.497934+00:00\n\n\n\n# set materialization version, for consistency\nmaterialization = 1181 # current public as of 9/16/2024\nclient.version = materialization"
  },
  {
    "objectID": "quickstart_notebooks/00_cave_quickstart.html#caveclient-basics",
    "href": "quickstart_notebooks/00_cave_quickstart.html#caveclient-basics",
    "title": "CAVE Quickstart",
    "section": "CAVEclient Basics",
    "text": "CAVEclient Basics\nThe most frequent use of the CAVEclient is to query the database for annotations like synapses. All database functions are under the client.materialize property. To see what tables are available, use the get_tables function:\n\nclient.materialize.get_tables()\n\n['nucleus_alternative_points',\n 'allen_column_mtypes_v2',\n 'bodor_pt_cells',\n 'aibs_metamodel_mtypes_v661_v2',\n 'allen_v1_column_types_slanted_ref',\n 'aibs_column_nonneuronal_ref',\n 'nucleus_ref_neuron_svm',\n 'apl_functional_coreg_vess_fwd',\n 'vortex_compartment_targets',\n 'baylor_log_reg_cell_type_coarse_v1',\n 'functional_properties_v3_bcm',\n 'l5et_column',\n 'pt_synapse_targets',\n 'coregistration_auto_phase3_fwd_apl_vess_combined',\n 'coregistration_manual_v4',\n 'vortex_manual_myelination_v0',\n 'synapses_pni_2',\n 'nucleus_detection_v0',\n 'vortex_manual_nodes_of_ranvier',\n 'vortex_astrocyte_proofreading_status',\n 'bodor_pt_target_proofread',\n 'nucleus_functional_area_assignment',\n 'coregistration_auto_phase3_fwd',\n 'synapse_target_structure',\n 'proofreading_status_and_strategy',\n 'aibs_metamodel_celltypes_v661']\n\n\nFor each table, you can see the metadata describing that table. For example, let’s look at the nucleus_detection_v0 table:\n\nclient.materialize.get_table_metadata('nucleus_detection_v0')\n\n{'aligned_volume': 'minnie65_phase3',\n 'created': '2020-11-02T18:56:35.530100',\n 'id': 45664,\n 'schema': 'nucleus_detection',\n 'table_name': 'nucleus_detection_v0',\n 'valid': True,\n 'schema_type': 'nucleus_detection',\n 'user_id': '121',\n 'description': 'A table of nuclei detections from a nucleus detection model developed by Shang Mu, Leila Elabbady, Gayathri Mahalingam and Forrest Collman. Pt is the centroid of the nucleus detection. id corresponds to the flat_segmentation_source segmentID. Only included nucleus detections of volume&gt;25 um^3, below which detections are false positives, though some false positives above that threshold remain. ',\n 'notice_text': None,\n 'reference_table': None,\n 'flat_segmentation_source': 'precomputed://https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/nuclei',\n 'write_permission': 'PRIVATE',\n 'read_permission': 'PUBLIC',\n 'last_modified': '2022-10-25T19:24:28.559914',\n 'segmentation_source': '',\n 'pcg_table_name': 'minnie3_v1',\n 'last_updated': '2024-10-24T22:00:00.145632',\n 'voxel_resolution': [4.0, 4.0, 40.0]}\n\n\nYou get a dictionary of values. Two fields are particularly important: the description, which offers a text description of the contents of the table and voxel_resolution which defines how the coordinates in the table are defined, in nm/voxel.\n\n\n\n\n\n\nAnnotation tables\n\n\n\nYou can also find a semantic description of the most commonly used tables at the Annotation Tables page."
  },
  {
    "objectID": "quickstart_notebooks/00_cave_quickstart.html#querying-tables",
    "href": "quickstart_notebooks/00_cave_quickstart.html#querying-tables",
    "title": "CAVE Quickstart",
    "section": "Querying Tables",
    "text": "Querying Tables\nTo get the contents of a table, use the query_table function. This will return the whole contents of a table without any filtering, up to for a maximum limit of 200,000 rows. The table is returned as a Pandas DataFrame and you can immediately use standard Pandas function on it.\n\ncell_type_df = client.materialize.query_table('nucleus_detection_v0')\ncell_type_df.head()\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n730537\n2020-09-28 22:40:41.780734+00:00\nNaN\nt\n32.307937\n0\n0\n[381312, 273984, 19993]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n373879\n2020-09-28 22:40:41.781788+00:00\nNaN\nt\n229.045043\n96218056992431305\n864691136090135607\n[228816, 239776, 19593]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n601340\n2020-09-28 22:40:41.782714+00:00\nNaN\nt\n426.138010\n0\n0\n[340000, 279152, 20946]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n201858\n2020-09-28 22:40:41.783784+00:00\nNaN\nt\n93.753836\n84955554103121097\n864691135373893678\n[146848, 213600, 26267]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n600774\n2020-09-28 22:40:41.785273+00:00\nNaN\nt\n135.189791\n0\n0\n[339120, 276112, 19442]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nWhile most tables are small enough to be returned in full, the synapse table has hundreds of millions of rows and is too large to download this way\n\n\nTables have a collection of columns, some of which specify point in space (columns ending in _position), some a root id (ending in _root_id), and others that contain other information about the object at that point. Before describing some of the most important tables in the database, it’s useful to know about a few advanced options that apply when querying any table.\n\ndesired_resolution : This parameter allows you to convert the columns specifying spatial points to different resolutions. Many tables are stored at a resolution of 4x4x40 nm/voxel, for example, but you can convert to nanometers by setting desired_resolution=[1,1,1].\nsplit_positions : This parameter allows you to split the columns specifying spatial points into separate columns for each dimension. The new column names will be the original column name with _x, _y, and _z appended.\nselect_columns : This parameter allows you to get only a subset of columns from the table. Once you know exactly what you want, this can save you some cleanup.\nlimit : This parameter allows you to limit the number of rows returned. If you are just testing out a query or trying to inspect the kind of data within a table, you can set this to a small number to make sure it works before downloading the whole table. Note that this will show a warning so that you don’t accidentally limit your query when you don’t mean to.\n\nFor example, using all of these together:\n\ncell_type_df = client.materialize.query_table('nucleus_detection_v0', split_positions=True, desired_resolution=[1,1,1], select_columns=['pt_position', 'pt_root_id'], limit=10)\ncell_type_df\n\n\n\n\n\n\n\n\npt_position_x\npt_position_y\npt_position_z\npt_root_id\n\n\n\n\n0\n241856.0\n374464.0\n838720.0\n0\n\n\n1\n227200.0\n389120.0\n797160.0\n0\n\n\n2\n230144.0\n422336.0\n795320.0\n0\n\n\n3\n239488.0\n386432.0\n794120.0\n0\n\n\n4\n239744.0\n423488.0\n803120.0\n864691136050815731\n\n\n5\n245888.0\n384512.0\n800120.0\n0\n\n\n6\n249792.0\n391680.0\n807080.0\n0\n\n\n7\n243328.0\n403008.0\n794280.0\n0\n\n\n8\n247872.0\n386816.0\n805320.0\n0\n\n\n9\n260352.0\n416640.0\n802360.0\n864691135013273238"
  },
  {
    "objectID": "quickstart_notebooks/00_cave_quickstart.html#filtering-queries",
    "href": "quickstart_notebooks/00_cave_quickstart.html#filtering-queries",
    "title": "CAVE Quickstart",
    "section": "Filtering Queries",
    "text": "Filtering Queries\nFiltering tables so that you only get data about certain rows back is a very common operation. While there are filtering options in the query_table function (see documentation for more details), a more unified filter interface is available through a “table manager” interface.\nRather than passing a table name to the query_table function, client.materialize.tables has a subproperty for each table in the database that can be used to filter that table.\nThe general pattern for usage is\nclient.materialize.tables.{table_name}({filter options}).query({format and timestamp options})\nwhere {table_name} is the name of the table you want to filter, {filter options} is a collection of arguments for filtering the query, and {format and timestamp options} are those parameters controlling the format and timestamp of the query.\nFor example, let’s look at the table aibs_metamodel_celltypes_v661, which has cell type predictions across the dataset. We can get the whole table as a DataFrame:\n\ncell_type_df = client.materialize.tables.aibs_metamodel_celltypes_v661().query()\ncell_type_df.head()\n\n\n\n\n\n\n\n\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\nid_ref\ncreated_ref\nvalid_ref\ntarget_id\nclassification_system\ncell_type\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n336365\n2020-09-28 22:42:48.966292+00:00\nt\n272.488202\n93606511657924288\n864691136274724621\n36916\n2023-12-19 22:47:18.659864+00:00\nt\n336365\nexcitatory_neuron\n5P-IT\n[209760, 180832, 27076]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n110648\n2020-09-28 22:45:09.650639+00:00\nt\n328.533443\n79385153184885329\n864691135489403194\n1070\n2023-12-19 22:38:00.472115+00:00\nt\n110648\nexcitatory_neuron\n23P\n[106448, 129632, 25410]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n112071\n2020-09-28 22:43:34.088785+00:00\nt\n272.929423\n79035988248401958\n864691136147292311\n1099\n2023-12-19 22:38:00.898837+00:00\nt\n112071\nexcitatory_neuron\n23P\n[103696, 149472, 15583]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n197927\n2020-09-28 22:43:10.652649+00:00\nt\n91.308851\n84529699506051734\n864691136050858227\n13259\n2023-12-19 22:41:14.417986+00:00\nt\n197927\nnonneuron\noligo\n[143600, 186192, 26471]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n198087\n2020-09-28 22:41:36.677186+00:00\nt\n161.744978\n83756261929388963\n864691135809440972\n13271\n2023-12-19 22:41:14.685474+00:00\nt\n198087\nnonneuron\nastrocyte\n[137952, 190944, 27361]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\nand we can add similar formatting options as in the last section to the query function:\n\ncell_type_df = client.materialize.tables.aibs_metamodel_celltypes_v661().query(split_positions=True, desired_resolution=[1,1,1], select_columns=['pt_position', 'pt_root_id', 'cell_type'], limit=10)\ncell_type_df\n\n\n\n\n\n\n\n\ncell_type\npt_position_x\npt_position_y\npt_position_z\npt_root_id\n\n\n\n\n0\n23P\n257600.0\n487936.0\n802760.0\n864691135724233643\n\n\n1\n23P\n260992.0\n493568.0\n801560.0\n864691136436395166\n\n\n2\nNGC\n256256.0\n466432.0\n831040.0\n864691135462260637\n\n\n3\n23P\n255744.0\n480640.0\n833200.0\n864691136723556861\n\n\n4\n23P\n262144.0\n505856.0\n824880.0\n864691135776658528\n\n\n5\n23P\n257536.0\n521728.0\n804440.0\n864691135941166708\n\n\n6\n23P\n251840.0\n552896.0\n832320.0\n864691135545065768\n\n\n7\n23P\n251136.0\n546048.0\n821320.0\n864691135479369926\n\n\n8\n23P\n256000.0\n626368.0\n814000.0\n864691135697633557\n\n\n9\nastrocyte\n324096.0\n417920.0\n658880.0\n864691135937358133\n\n\n\n\n\n\n\nHowever, now we can also filter the table to get only cells that are predicted to have cell type \"BC\" (for “basket cell”).\n\nmy_cell_type = \"BC\"\nclient.materialize.tables.aibs_metamodel_celltypes_v661(cell_type=my_cell_type).query()\n\n\n\n\n\n\n\n\nid_ref\ncreated_ref\nvalid_ref\ntarget_id\nclassification_system\ncell_type\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n43009\n2023-12-19 22:48:53.577191+00:00\nt\n369908\ninhibitory_neuron\nBC\n369908\n2020-09-28 22:40:41.814964+00:00\nt\n332.862751\n96002690286851358\n864691136522768017\n[227104, 207840, 20841]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n12051\n2023-12-19 22:40:57.133228+00:00\nt\n193846\ninhibitory_neuron\nBC\n193846\n2020-09-28 22:40:41.897904+00:00\nt\n306.148966\n82838443188669165\n864691135684976823\n[131568, 168496, 16452]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n83044\n2023-12-19 22:58:50.269173+00:00\nt\n615735\ninhibitory_neuron\nBC\n615735\n2020-09-28 22:40:41.957345+00:00\nt\n314.539540\n112181247505371364\n864691136311774525\n[344880, 161104, 17084]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n48718\n2023-12-19 22:50:21.192138+00:00\nt\n401681\ninhibitory_neuron\nBC\n401681\n2020-09-28 22:40:42.066718+00:00\nt\n497.801462\n98465046644219429\n864691136052141043\n[245232, 203952, 21268]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n82324\n2023-12-19 22:58:39.896999+00:00\nt\n613047\ninhibitory_neuron\nBC\n613047\n2020-09-28 22:40:41.982376+00:00\nt\n242.159780\n113234168401651200\n864691136065413528\n[352688, 141616, 25312]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3360\n8968\n2023-12-19 22:40:09.246333+00:00\nt\n170777\ninhibitory_neuron\nBC\n170777\n2020-09-28 22:45:25.310708+00:00\nt\n499.103662\n81230957054577082\n864691135065994564\n[119600, 250560, 15373]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3361\n15548\n2023-12-19 22:41:48.382554+00:00\nt\n208056\ninhibitory_neuron\nBC\n208056\n2020-09-28 22:45:25.401800+00:00\nt\n521.621668\n84540007091735344\n864691135801456226\n[143472, 262944, 23693]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3362\n79472\n2023-12-19 22:57:53.993099+00:00\nt\n591219\ninhibitory_neuron\nBC\n591219\n2020-09-28 22:45:25.526753+00:00\nt\n567.517839\n110216764830845707\n864691135279126177\n[330320, 204752, 25060]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3363\n55791\n2023-12-19 22:52:02.582669+00:00\nt\n438586\ninhibitory_neuron\nBC\n438586\n2020-09-28 22:45:25.430745+00:00\nt\n529.501389\n99807894274485381\n864691135395662581\n[254912, 247440, 23680]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3364\n50504\n2023-12-19 22:50:48.576826+00:00\nt\n419363\ninhibitory_neuron\nBC\n419363\n2020-09-28 22:45:25.436862+00:00\nt\n530.642698\n99716496901116512\n864691136691390838\n[254416, 90336, 20469]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n3365 rows × 15 columns\n\n\n\nor maybe we just want the cell types for a particular collection of root ids:\n\nmy_root_ids = [864691135771677771, 864691135560505569, 864691136723556861]\nclient.materialize.tables.aibs_metamodel_celltypes_v661(pt_root_id=my_root_ids).query()\n\n\n\n\n\n\n\n\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\nid_ref\ncreated_ref\nvalid_ref\ntarget_id\nclassification_system\ncell_type\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n19116\n2020-09-28 22:41:51.767906+00:00\nt\n301.426115\n74737997899501359\n864691135771677771\n11282\n2023-12-19 22:40:43.249642+00:00\nt\n19116\nexcitatory_neuron\n23P\n[72576, 108656, 20291]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n21783\n2020-09-28 22:41:59.966574+00:00\nt\n263.637074\n75795590176519004\n864691135560505569\n15681\n2023-12-19 22:41:50.365399+00:00\nt\n21783\nexcitatory_neuron\n23P\n[80128, 124000, 16563]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n4074\n2020-09-28 22:42:41.341179+00:00\nt\n313.678234\n73543309863605007\n864691136723556861\n50080\n2023-12-19 22:50:42.474168+00:00\nt\n4074\nexcitatory_neuron\n23P\n[63936, 120160, 20830]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\nYou can get a list of all parameters than be used for querying with the standard IPython/Jupyter docstring functionality, e.g. client.materialize.tables.aibs_metamodel_celltypes_v661.\n\n\n\n\n\n\nCaution\n\n\n\nUse of this functionality will show a brief warning that the interface is experimental. This is because the interface is still being developed and may change in the near future in response to user feedback."
  },
  {
    "objectID": "quickstart_notebooks/00_cave_quickstart.html#querying-proofread-neurons",
    "href": "quickstart_notebooks/00_cave_quickstart.html#querying-proofread-neurons",
    "title": "CAVE Quickstart",
    "section": "Querying Proofread neurons",
    "text": "Querying Proofread neurons\n\nProofread neurons\nProofreading is necessary to obtain accurate reconstructions of a cell. In the MICrONS dataset, the general rule is that dendrites onto cells with a single cell body are sufficiently proofread to trust synaptic connections onto a cell. Axons on the other hand require so much proofread that only ~1,000 cells have axons that were proofread to various degrees such that their outputs can be used for analysis.\nThe table proofreading_status_and_strategy contains proofreading information about ~1,300 neurons. This website provides the most detailed overview. In brief, axons annotated with any strategy_axon were cleaned of false mergers but not all were fully extended. The most important distinction is axons annotated with axon_column_truncated were only proofread within a certain volume wheras others were proofread without such bias.\n\nproof_all_df = client.materialize.query_table(\"proofreading_status_and_strategy\", desired_resolution=[1, 1, 1], split_positions=True)\n\n\nproof_all_df[\"strategy_axon\"].value_counts()\n\nstrategy_axon\naxon_partially_extended    979\naxon_column_truncated      233\nnone                       185\naxon_interareal            144\naxon_fully_extended         80\nName: count, dtype: int64\n\n\nWe can filter our query to only return rows that match a condition by adding a filter to our query:\n\nproof_df = client.materialize.query_table(\"proofreading_status_and_strategy\", filter_in_dict={\"strategy_axon\": [\"axon_partially_extended\", \"axon_fully_extended\", \"axon_interareal\", \"axon_column_truncated\"]}, desired_resolution=[1, 1, 1], split_positions=True)\n\n\nproof_df[\"strategy_axon\"].value_counts()\n\nstrategy_axon\naxon_column_truncated      598\naxon_partially_extended    341\naxon_interareal            146\naxon_fully_extended         77\nName: count, dtype: int64"
  },
  {
    "objectID": "proofreading.html",
    "href": "proofreading.html",
    "title": "Proofreading and Data Quality",
    "section": "",
    "text": "Automated segmentation of neuronal processes in dense EM imaging is challenging at the size of entire neurons, which can have millimeters of axons and dendrites. The automated segmentation algorithms used in the EM data for this project are not perfect, and so proofreading is necessary to correct errors in the segmentation.\nHowever, while automated proofreading methods are starting to appear, in general the proofreading process is time consuming and involves a lot of manual attention. A concerted effort was made to do proofreading on a subset of neurons within the dataset, and so some cells can be understood to be more biologically accurate and complete than others. Some cells in the MICrONs data are thus very well proofread, others are virtually untouched, and many are somewhere in between.\nEach kind of cell can be useful, but different approaches must be taken for each.\nIt is useful to distinguish between two kinds of proofreading errors: splits and merges.\nThese two kinds of errors impact analysis differently. While split errors reduce the number of correct synaptic connections, merge errors add incorrect connections.\nThe frequency of errors is roughly related to the size of the process, and thus axons and dendrites have extremely different error profiles. Because they are thicker, dendrites are largely well-segmented, with only a few split errors and most merge errors being with small axons rather than other dendrites.\nEven without proofreading a neuron’s dendrites, we thus expect that most of the input synapses onto the dendrites are correct (since the vast majority of merge errors are with axons that only have output synapses), though some may be missing due to split errors at tips or missing dendritic spines.\nIn contrast, axons, because they are thinner, have many more errors of both kinds. We thus do not trust axonal connectivity at all without additional proofreading.",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Proofreading and Data Quality"
    ]
  },
  {
    "objectID": "proofreading.html#proofreading-categories",
    "href": "proofreading.html#proofreading-categories",
    "title": "Proofreading and Data Quality",
    "section": "Proofreading categories",
    "text": "Proofreading categories\nProofreading is not an all-or-nothing process. Because of the reasons described above, we distinguish between the proofread status of the axon and the dendrite for each neuron separately. In addition, we consider three levels of proofreading:\n\nUnproofread: The arbor (dendrite or axon) has not been comprehensively proofread, although edits may have happened here and there.\nClean: The arbor (dendrite or axon) has been comprehensively proofread to remove all merge errors, but may still have split errors. This means that the synapses that are there are correct, but incomplete.\nExtended: The arbor (dendrite or axon) has been comprehensively proofread to remove all merge and split errors. This means that the synapses that are there are correct and as-complete-as-possible by expert proofreaders. Note that even a well-proofread neuron often has some tips that cannot be followed due to the limitations of the underlying imagery, and of course processes may leave the segmented volume.\n\nThe upshot is that unproofread axons are not of a quality suitable for analysis, you can trust the targets of a clean or extended axon because the default dendrite quality is good. However, if your analysis truly demands knowing if a particular neuron is connected to another neuron or not (rather than connecting to a population), the proofreading standards are particularly high and possibly require additional checks.\n\nProofreading status\n\n\n\n\n\n\n\nPre Axon Status\nPost Dendrite Status\nAnalyzability\n\n\n\n\nUnproofread\nUnproofread\nNot analyzable\n\n\nUnproofread\nClean / Extended\nNot analyzable\n\n\nClean\nUnproofread / Clean / Extended\nAnalyzable but connectivity could be highly biased by limited axonal extent\n\n\nExtended\nUnproofread\nAnalyzable and connectivity is close to complete\n\n\n\nSince a single proofread axon can have many hundreds to ten thousand synaptic outputs, the vast majority onto local dendrites, each single axon provides a tremendous amount of data about that cell’s postsynaptic targets.",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Proofreading and Data Quality"
    ]
  },
  {
    "objectID": "proofreading.html#proofreading-efforts",
    "href": "proofreading.html#proofreading-efforts",
    "title": "Proofreading and Data Quality",
    "section": "Proofreading efforts",
    "text": "Proofreading efforts\nThere have been several different proofreading efforts in the MICrONS data with different goals and levels of comprehensiveness. Importantly, neurons were not selected for proofreading randomly, but rather chosen based on various criteria. A few of the most signifcant proofreading efforts are described below.\n\nExcitatory neuron proofreading for functional connectomics. Excitatory neurons in retinotopically matched ares of VISp and RL were proofread to enable functional connectomics analysis, looking for the relationship between functional similarity and synaptic connectivity.\nMartinotti cell proofreading. Extremely extensive reconstructions of several layer 5 Martinotti cells in VISp were performed to enable analysis of the relationship between morphology and transcriptomics.\nMinnie V1 Column. A 100 micron square area of VISp for a comprehensive census of its neuronal population across all layers. Originally, excitatory neurons in this column had dendritic cleaning and extension and inhibitory neurons had comprehensive cleaning and substantail but incomplete extension. However, this population has become a hub for proofreading and cell typing, and will be discussed further in the section below.\n\nThe NIH-funded Virtual Observatory of the Cortex (VORTEX) program supports continued proofreading efforts. Supported efforts include proofreading of: of excitatory functionally-coregistered cells, inhibitory neurons, and glial cell types including astrocytes and oligodendrocyte precursor cells.\nTo learn more about our supported efforts, see: Virtual Observatory of the Cortex on MICrONS Tutorials.\nAccess our request submission system at:\nmicrons-explorer.org/requests | Scientific Request Form\nWe are also conducting a community survey. If you are using, have used, or plan to use the MICrONS data, give us some feedback!\nMICrONS Community Survey",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Proofreading and Data Quality"
    ]
  },
  {
    "objectID": "proofreading.html#proofreading-tables",
    "href": "proofreading.html#proofreading-tables",
    "title": "Proofreading and Data Quality",
    "section": "Proofreading tables",
    "text": "Proofreading tables\nTo keep track of the state of proofreading, the table proofreading_status_and_strategy in the annotation database has a row for each proofread neuron and provides a summary of the proofreading status of the axon and dendrite.\nTable name: proofreading_status_and_strategy\nThe table proofreading_status_and_strategy describes the status of cells that have undergone manual proofreading.\nBecause of the inherent difference in the difficulty and time required for different kinds of proofreading, we describe the status of axons and dendrites separately.\nEach compartment status may be either:\n\nFALSE: indicates no comprehensive proofreading has been performed, or is not applicable.\n\nTRUE: indicates that false merges have been comprehensively removed, and the compartment is at least ‘clean’. Consult the strategy column if completeness of the compartment is relevant to your research.\n\nAn axon or dendrite labeled as status=TRUE can be trusted to be correct, but may not be complete. The degree of completion can be read from the strategy column. For more information, please see Proofreading and Data Quality; or also the microns-explorer page on proofreading strategies.\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made. NOTE: if this does not match the pt_root_id then the cell has undergone further changes. This is usually and improvement in proofreading, but proceed with caution.\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. May be TRUE or FALSE\n\n\nstatus_axon\nThe status of the axon proofreading. May be TRUE or FALSE\n\n\nstrategy_dendrite\nThe strategy employed to proofread the dendrite. See strategy table below for details\n\n\nstrategy_axon\nThe strategy employed to proofread the axon. See strategy table below for details\n\n\n\nThe specific strategies are as follows (and will update over time):\n\nProofreading Strategies\n\nProofreading Strategy Table\n\n\n\n\n\n\nStrategy\nDescription\n\n\n\n\nnone\nNo cleaning, and no extension. Indicates an entry in proofreading_status that is FALSE for that compartment\n\n\ndendrite_clean\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. The dendrite may be incorrectly truncated by segmentation error. Not all dendrite tips have been checked for extension. No comprehensive attempt was made to re-attach spine heads.\n\n\ndendrite_extended\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. Every tip was identified, manually inspected, and extended if possible. No comprehensive attempt was made to re-attach spine heads.\n\n\naxon_column_truncated\nAThe axon was extended within the V1 cortical column, with a preference for local connections. In some cases the axon was cut at the column boundary and/or the layer boundary, especially the boundary between layers 2/3 and layer 4. Output synapses represent a sampling of potential partners\n\n\naxon_interareal\nThe axon was extended with a preference for branches that projected to other brain areas. Some axon branches were fully extended, but local connections may be incomplete. Output synapses represent a sampling of potential partners.\n\n\naxon_partially_extended\nThe axon was extended outward from the soma, following each branch to its termination. Output synapses represent a sampling of potential partners.\n\n\naxon_fully_extended\nAxon was extended outward from the soma, following each branch to its termination. After initial extension, every endpoint was identified, manually inspected, and extended again if possible. Output synapses represent a largely complete sampling of partners.\n\n\n\nThis table, proofreading_status_and_strategy, supercedes proofreading_status_public_release.\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Proofreading and Data Quality"
    ]
  },
  {
    "objectID": "proofreading.html#minnie-column",
    "href": "proofreading.html#minnie-column",
    "title": "Proofreading and Data Quality",
    "section": "Minnie Column",
    "text": "Minnie Column\nThe Minnie Column is a collection of more than 1,300 neurons whose nucleus centroid fell within a 100 x 100 micron square (viewed from the top) and extending across all layers.\n\n\n\n\n\nLocation of the Minnie column (yellow) within the dataset. Top view.\n\n\n\n\n\n\n\nReconstruction of the V1 column cells\n\n\n\n\nThis was originally considered a region of interest for “Inhibitory specificity from a connectomic census of mouse visual cortex” (Schneider-Mizell et al. 2025), and thus all neurons in this region were proofread to some extent and manually evaulated.\nAll excitatory neurons had their dendrites cleaned and extended to assess dendritic morphology and synaptic inputs, while inhibitory neurons had both their dendrites cleaned and extended, and then their axons were extensively cleaned and extended to capture their targeting properties. Efforts were made to extend inhibitory axons to every major region of their arbor, but not every individual tip was followed and the resulting analysis focused on population-level connectivity rather than individual connections.\nMorphological and connectivity-based cell typing was performed on these cells in terms of both classical and novel categories, and considerable expert attention ensured that the baseline data quality was high. Cell type results can be found in several of the cell type tables available. These cell types were used for training the various whole-dataset cell typing classifiers.\nIn addition, the proofreading and multiple levels of cell typing applied to this dense and diverse population has made it a useful reference, and spawned further analysis. Subsequent work has cleaned many of the excitatory axons from layers 2–5, albeit to a variety of degrees of extension.",
    "crumbs": [
      "Introduction",
      "Versioning and Data Quality",
      "Proofreading and Data Quality"
    ]
  },
  {
    "objectID": "programmatic_access/functional_tutorial/em_functional_intro.html",
    "href": "programmatic_access/functional_tutorial/em_functional_intro.html",
    "title": "MICrONs Tutorial",
    "section": "",
    "text": "(em:functional-data)= # MICrONS Functional Data\nThe MICrONs mouse went through a battery of functional imaging experiments before be prepared for electron microscopy. Excitatory neurons were imaged with 2-photon calcium imaging using GCaMP6s. Because of the size of the volume, different populations of neurons were imaged in different sessions, spanning several days. Each neuronal {term}`ROI`` from the functional data is uniquely determined based on the combination of image session, scan index, and ROI unit id. During each imaging session, the mouse was head-fixed and presented a variety of visual stimuli to the left visual field, including both natural movies and synthetic parametric movies. In addition to functional 2p imaging, the treadmill rotation, left eye position, and left pupil diameter were recorded to provide behavioral context."
  },
  {
    "objectID": "programmatic_access/functional_tutorial/em_functional_intro.html#stimuli-and-digital-twin",
    "href": "programmatic_access/functional_tutorial/em_functional_intro.html#stimuli-and-digital-twin",
    "title": "MICrONs Tutorial",
    "section": "Stimuli and “Digital twin”",
    "text": "Stimuli and “Digital twin”\nThe visual stimuli presented to the mouse included many natural movies, as well as parametric stimuli that were designed to emphasize specific visual features, such as orientation and spatial frequency. Because imaging sessions were performed over several days and imaging time in any one location was limited, comparisons between cells in different sessions are not straightforward. To get around this, the principle goal of this collection of stimuli was to be used as training data for deep neural networks that were trained to predict the response of each cell to an arbitrary stimulus. These so-called “digital twins” were designed to be used to probe functional responses to cells to stimuli that were outside the original training set. More details about this digital twin approach can be found in Wang et al. 2023.\nAs a measure of the reliability of visual responses, a collection of six movies totalling one minute were presented to the mouse ten times per imaging session. An oracle score was computed based on the signal correlation of a given cell to the oracle stimuli across the imaging session, where higher numbers indicate more reliable responses. Specifically, the oracle score is the mean signal correlation of the response of each presentation to the average of the other nine presentations."
  },
  {
    "objectID": "programmatic_access/functional_tutorial/em_functional_intro.html#coregistration",
    "href": "programmatic_access/functional_tutorial/em_functional_intro.html#coregistration",
    "title": "MICrONs Tutorial",
    "section": "Coregistration",
    "text": "Coregistration\nThe process of aligning the location of cells from the functional imaging with the same cells in the EM imaging is called coregistration. It is a challenging problem due to the need for micron-scale alignment of image volumes, despite signifiant differences in the imaging modalities, tissue deformations under different conditions and potential distortions introduced by sample preparation.\nThe dataset contains two approaches to coregistration, one semi-manual and one fully automatic. In the semi-manual approach, a transform between the EM data and the 2p data was generated based on fiducual points such as blood vessels and cell bodies. This transform was then applied to the functional data to identify a location in the EM space, and a human annotator then identified the cell body in the EM data that was a best match to the functional ROI based on location and context. In the fully automatic approach, blood vessels were segmented in both 2p and EM volumes, and a transform was generated based on matching this 3d structure. More details can be found in the MICrONS dataset preprint.\n\nCoregistration Quality Metrics\n\n\n\n```sqgrypnf img/coreg-metrics.png\n\n\n\n\nalign: center\n\n\n\nCartoon illustrating the coregistration metrics.\n\nTwo values are available to help assess the quality of the coregistration.\nThe **residual** indicates the distance between the location of an ROI after transformation to the EM space and the location of the matched cell body in the EM data.\nThe **score** (or separation score) is the distance between the matched cell body and the nearest other cell body in the EM data.\nThis attempts to measure how the residual compares with the distance to other potential matches in the data.\nLarger values indicate fewer potential matches and therefore a more confident match, in general.\n\nWhen using the automated coregistration, it is important to filter the data based on assignment confidence.\nA guide for this can be found by comparing the subset of cells matched by both the automated and manual coregistration methods.\n\n```{figure} img/coreg-agreement.png\n---\nalign: center\n---\nRelationship between separation threshold (left) and residual (right) and the accuracy of automated coregistration compared to manual coregistration. Orange curves depict the fraction of cells that remain after filtering out those matches beyond the threshold indicated on the x-axis.\n\n\nMatching EM to Function\nThe combination of session index, scan index, and ROI unit id uniquely identifies each ROI in the functional data. The annotation database contains tables with the results of each of the coregistration methods. Each row in each table contains the nucleus id, centroid, and root ID of an EM cell as well as the scan/session/unit indices required to match it. In additoin, the residual and score metrics for each match are provided to filter by quality. For manual coregistration, the table is called coregistration_manual_v3 and for automated coregistration, the table is called apl_functional_coreg_forward_v5.\nFull column definitions can be found on Annotation Tables page."
  },
  {
    "objectID": "programmatic_access/functional_tutorial/em_functional_intro.html#functional-data",
    "href": "programmatic_access/functional_tutorial/em_functional_intro.html#functional-data",
    "title": "MICrONs Tutorial",
    "section": "Functional data",
    "text": "Functional data\nA collection of in silico model responses of neurons to a variety of visual stimuli has been precomputed. The data can be found in a collection of files:\n\n:header-rows: 1 * - Filename - Format - Info * - nat_movie.npy - numpy.ndarray - dimensions=[clip, frame, height, width] * - nat_resp.npy - numpy.ndarray - dimensions=[unit, bin] * - nat_unit.csv - importable into pandas.DataFrame - columns=[animal_id, scan_session, scan_idx, unit_id, row_idx] * - monet_resp.npy - numpy.ndarray - dimensions=[unit, direction, trial] * - monet_dir.npy - numpy.ndarray - dimensions=[direction] * - monet_unit.csv - importable into pandas.DataFrame - columns=[animal_id, scan_session, scan_idx, unit_id]\n\nThe model response data is organized into two sets of files, one for natural movies and one for Monet stimuli, a type of parametric stimulus that measures orientation tuning. The .npy files can be read with the numpy function np.load and the .csv files with the pandas function pd.read_csv.\n\n\n\n```sqgrypnf img/function-stimulus.png\n\n\n\n\nalign: center\n\n\n\nExample images from a variety fo the stimuli used to probe functional responses. Natural movies include scenes from cinema, POV nature videos, and rendered 3d scenes. Monet stimuli are a parametic textured stimulus that varies in orientation and spatial frequency of correlated motion. ```\nThe natural movie data is organized into three files:\n\nnat_movie contains 250 10-sec clips of natural movies. Movies were shown to the model at a resolution of 72 * 128 pixels at 30 hz. This is downsampled from the in vivo presentation resolution of 144 * 256 pixels.\nnat_resp contains 104,171 units’ binned responses to natural movies. Responses were binned into 500 msec non-overlapping bins. This results in 250 clips * 10 sec / 500 msec = 5000 bins, with every binned response corresponding to 15 frames of natural movies.\nnat_unit contains the mapping from rows in nat_resp to functional unit keys. Since there is only a single animal for the EM data, use the session, scan, and unit id values to match to the EM data via one of the coregistration tables described above.\n\nThe Monet stimulus data is similarly organized into three files:\n\nmonet_resp contains 104,171 units’ mean responses to Monet stimuli. 120 trials of 16-direction Monet stimuli were shown to the model. The 120 trials were generated with different random seeds.\nmonet_dir contains the direction (in units of degrees) for the Monet stimulus shown to the model. The order of directions matches the order in the direction dimension of monet_resp. Here, 0 degrees is vertical feature moving leftwards, with degrees increases in the anti-clockwise direction.\nmonet_unit contains the functional unit keys of the 104,171 units. The order of the unit keys matches the order in the unit dimension of monet_resp. Since there is only a single animal for the EM data, use the session, scan, and unit id values to match to the EM data via one of the coregistration tables described above."
  },
  {
    "objectID": "neuroglancer-basic.html",
    "href": "neuroglancer-basic.html",
    "title": "Neuroglancer",
    "section": "",
    "text": "Note\n\n\n\nNeuroglancer works best in Chrome and Firefox, and does not always work as expected in Safari.\nNeuroglancer is a WebGL-based viewer developed by Jeremy Maitin-Shephard at the Google Connectomics team to visualize very large volumetric data, designed in large part for connectomics. We often use Neuroglancer to quickly explore data, visualize results in context, and share data.\nTo look at the MICrONS data in Neuroglancer, click this link. Note that you will need to authenticate with the same Google-associated account that you use to set up CAVEclient.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "neuroglancer-basic.html#interface-basics",
    "href": "neuroglancer-basic.html#interface-basics",
    "title": "Neuroglancer",
    "section": "Interface Basics",
    "text": "Interface Basics\nThe Neuroglancer interface is divided into panels. In the default view, one panel shows the imagery in the X/Y plane (left), one shows a 3d view centered at the same location, and the narrow third panel provides information about the specific layer. Note that at the center of the each panel is a collection of axis-aligned red, blue and, green lines. The intersection and direction of each of these lines is consistent across all panels.\nAlong the top left of the view, you can see tabs with different names. Neuroglancer organizes data into layers, where each layer tells Neuroglancer about a different aspect of the data. The default view has three layers:\n\nimg describes how to render imagery.\nseg describes how to render segmentation and meshes.\nann is a manual annotation layer, allowing the user to add annotations to the data.\n\nYou can switch between layers by right clicking on the layer tab. You will see the panel at the right change to provide controls for each layer as you click it.\nThe collection of all layers, the user view, and all annotations is stored as a JSON object called the state.\nThe basic controls for navigation are:\n\nsingle click/drag slides the imagery in X/Y and rotates the 3d view.\nscroll wheel up/down moves the imagery in Z.\nright click jumps the 3d view to the clicked location in either the imagery or on a segmented object.\ndouble click selects a segmentation and loads its mesh into the 3d view. Double clicking on a selected neuron deselects it.\ncontrol-scrool zooms the view under the cursor in or out.\nz snaps the view to the closest right angle.\n\nYou can paste a position into Neuroglancer by clicking the x, y, z coordinate in the upper left corner and pasting a space or comma-separated list of numbers and hitting enter. Note that Neuroglancer always works in voxel units, and you can see the resolution of the voxels in the extreme upper left corner.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "neuroglancer-basic.html#selecting-objects",
    "href": "neuroglancer-basic.html#selecting-objects",
    "title": "Neuroglancer",
    "section": "Selecting objects",
    "text": "Selecting objects\nThe most direct way to select a neuron is to double click in the imagery to select the object under your cursor. This will load all the voxels associated with that object and also display its mesh in the 3d view.\nTo see the list of selected objects, you can select the segmentation tab (right click on the seg tab). Underneath the list of options, there is a list of selected root ids and the color assigned to them in the view. You can change colors of all neurons randomly by pressing l or individually change colors as desired. In addition, you can press the checkbox to hide a selected object while keeping it in the list, or deselect it by clicking on the number itself. You can also copy a root id by pressing the clipboard icon next to its number, or copy all selected root ids by pressing the clipboard icon above the list.\nThis selection list also allows you to select objects by pasting one or more root ids into the text box at the top of the list and pressing enter.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "neuroglancer-basic.html#annotations",
    "href": "neuroglancer-basic.html#annotations",
    "title": "Neuroglancer",
    "section": "Annotations",
    "text": "Annotations\nAnnotations are stored in an annotation layer. The default state has an annotation layer called ann, but you can always add new annotation layers by command-clicking the + button to the right of the layer tabs.\nTo create an annotation, select the layer (right click on the tab), and then click the icon representing the type of annotation you want to use. The most basic annotation is a point, which is the icon to the left of the list. The icon will change to having a green background when selected.\nNow if you control-click in either the imagery or the 3d view, you will create a point annotation at the clicked location. The annotation will appear in the list to the right, with its coordinate (in voxels, not nanometers) displayed. Clicking any annotation in the list will jump to that annotation in 3d space. Each annotation layer can have one color, which you can change with the color picker to the left of the annotation list.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "neuroglancer-basic.html#navigating-annotations",
    "href": "neuroglancer-basic.html#navigating-annotations",
    "title": "Neuroglancer",
    "section": "Navigating Annotations",
    "text": "Navigating Annotations\nAnnotations in an annotation layer can be right-clicked on to jump to them, but can also be navigated as a list.\n\nspelunkerneuroglancer.neuvuengl.microns-explorer\n\n\nTo see the list of annotations, select the annotation layer (right click on the tab).\nEach annotation is listed as its location in Neuroglancer voxel coordinates.\nClicking on an row in this annotation list will not only jump to it in the view, but also select it. You can see the information about the selected neuron in the lower right corner.\n\n\nTo see the list of annotations, select the annotation layer (right click on the tab).\nEach annotation is listed as its location in Neuroglancer voxel coordinates.\nClicking on an row in this annotation list will not only jump to it in the view, but also select it. You can see the information about the selected neuron in the lower right corner.\n\n\n\n\n\n\nImportant\n\n\n\nThe following sections are not accurate for spelunker version of neuroglancer, but do represent planned feature addtions.\n\n\nOnce an annotation is selected, any associated root ids are loaded. The keys [ and ] will jump to the previous and next annotations in the list, respectively.\nEach annotation can have a full-text description associated with it for adding notes. This can be added in the lower right corner.\n\n\n\nAnnotation tags after populating them manually.\n\n\nHowever, the most convenient way to label data quickly is through Tags.\nTo add tags, click on the Shortcuts tab within the Annotation widget on the right, and then click on the + button to add a new tag.\nEach tag gets a text label and a key command to activate or deactivate it for a given annotation.\nBy default, the first tag is activated by pressing shift-q, the second by pressing shift-w, and so on down the qwerty line.\n\n\n\nAnnotation tags applied to annotations. The labels with with the # symbol indicate a tag is present.\n\n\nNow when you select an annotation, you can press the key command to attach that tag to it.\nPressing the same key command will remove the tag.\nAny number of tags can be added to each annotation.\nTogether with the [ and ] keys to navigate the list, this allows you to quickly label a large number of annotations.\n\n\nTo see the list of annotations, select the annotation layer (right click on the tab).\nEach annotation is listed as its location in Neuroglancer voxel coordinates.\nClicking on an row in this annotation list will not only jump to it in the view, but also select it. You can see the information about the selected neuron in the lower right corner.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "neuroglancer-basic.html#saving-and-sharing-states",
    "href": "neuroglancer-basic.html#saving-and-sharing-states",
    "title": "Neuroglancer",
    "section": "Saving and sharing states",
    "text": "Saving and sharing states\nLike many other websites that require logins, you cannot simply send your URL ot another person to have them see the view. Instead, to save the current state and make it available to yourself or others in the future, you need to save the state with the Share button at the top right corner. This will then give you a URL that you can copy and share with others or paste yourself. A typical sharing URL looks like the following:\nhttps://neuroglancer.neuvue.io/?json_url=https://global.daf-apis.com/nglstate/api/v1/4684616269037568\nThe first part is the URL for the Neuroglancer viewer, while the part after the ?json_url= is a URL that points to a JSON file that contains the state. The number at the end of the URL is used to uniquely identify the state and can be used programatically to retrieve information.\n\n\n\n\n\n\nWarning\n\n\n\nIf a URL contains ?local_id= instead of ?json_url, that means that it cannot be viewed by anyone else or even in another browser on your own computer.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Neuroglancer"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MICrONs Tutorials",
    "section": "",
    "text": "Welcome to the MICrONs-Explorer data ecosystem.\nThe MICrONS open-access dataset is a functional connectome containing 200,000 cells, 75,000 neurons with physiology, and 523 million synapses.\nThis resource provides interactive visualizations of anatomical and functional data that span all 6 layers of mouse primary visual cortex and 3 higher visual areas (LM, AL, RL) within a cubic millimeter volume.\nHighlights:\n\nMotivation of and introduction to the MICrONS dataset: Introduction\nOverview of dataset collection, methods, and outputs: Background\nOngoing proofreading efforts and analysis support through the: VORTEX program\nProgrammatic access quickstart: Python Tools\nData Release information: Data Inventory, including Annotation Tables and Release Manifests\n\n\n\n\nRender of 3D reconstruction of pyramidal cells in mouse visual cortex, as seen in Neuroglancer\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dataset-basics.html",
    "href": "dataset-basics.html",
    "title": "Dataset Basics",
    "section": "",
    "text": "One of the more complicated things about using connectomics data is that it’s both highly multimodal — there are images, tables, meshes, and more — and that it has been designed to be dynamic, as the segmentation changes with proofeading and carries changes to synaptic connectivity and more. This section will cover what types of data exist and how to access them.\nThe types of data we will describe here are:\nIn addition, there are three primary ways to access the data:",
    "crumbs": [
      "Introduction",
      "Dataset Basics"
    ]
  },
  {
    "objectID": "dataset-basics.html#dataset-information-hub",
    "href": "dataset-basics.html#dataset-information-hub",
    "title": "Dataset Basics",
    "section": "Dataset Information Hub",
    "text": "Dataset Information Hub\nIf you need to look up any particular values or links for the dataset — how to open a Neuroglancer link for it, what annotation tables exist, how to find Dash Apps — the public MICrONs datastack has a useful information page. Go there if you need to look up basic details about the dataset.\nParticularly useful links here are: * the Spelunker neuroglancer link * the public Dash Apps * the list of all materialization versions * and the annotation tables associated with the most recent release version (v1300).",
    "crumbs": [
      "Introduction",
      "Dataset Basics"
    ]
  },
  {
    "objectID": "dataset-basics.html#how-to-reference-a-neuron",
    "href": "dataset-basics.html#how-to-reference-a-neuron",
    "title": "Dataset Basics",
    "section": "How to reference a neuron",
    "text": "How to reference a neuron\nThe dynamic nature of the data means that there is more than one way to reference a neuron, and each way has a slightly different meaning.\nThe most frequent way to reference a specific neuron is its root id, a unique integer that identifies a specific segmentation, and can be used in numerous places. However, as proofreading occurs, the segmentation of a cell can change, and each edit creates a new “version” of a given neuron that is given its own unique root id. Root ids thus refer to not only a specific cell, but a specific version of that cell.\nWhen downloading meshes, querying the database, or browsing in Neuroglancer, we use the root id of a cell.\nHowever, when using data from different points in time, however, root ids can change and the data might not be consistent across two time points. One way to make this easier is to only use data from one moment in time.\nThis is the case for the public database that will be used in the course — all data from the public database is synched up to a single moment in time, but it does mean that the analysis cannot respond to any changes in segmentation. In some cases, in some cases, you might see the root id called a “segment id” or an “object id,” but they all refer to the same thing.\nFor segmentations with a cell body in the volume, we also have a cell id (or soma id), which is a unique integer that identifies each nucleus in the dataset. The advantage of the soma id is that it remains constant across all versions of a cell, so it can be used to track a cell across time. The disadvantage is that to look up the id at any given time or to load into Neuroglancer, you need to look up the root id.\nCell ids can be found in the nucleus_detection_v0 table in the database, as well as many of the tables that reference cell types.",
    "crumbs": [
      "Introduction",
      "Dataset Basics"
    ]
  },
  {
    "objectID": "dash-table-viewer.html",
    "href": "dash-table-viewer.html",
    "title": "Dash Apps: Table Viewer",
    "section": "",
    "text": "We have created two webapps to help explore and visualize the data. These apps were created using the Dash framework, hence the name.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Dash Apps: Table Viewer"
    ]
  },
  {
    "objectID": "dash-table-viewer.html#table-viewer",
    "href": "dash-table-viewer.html#table-viewer",
    "title": "Dash Apps: Table Viewer",
    "section": "Table Viewer",
    "text": "Table Viewer\nThe Table Viewer app allows you to query individual tables in the database, filter them, and visualize the results in Neuroglancer.\n\n\n\nTable viewer input options\n\n\nIn the Table Viewer, you first select one of the tables from a dropdown in the upper left side.\nThe Live Query option is not relevent for the public data.\nFilter options allow you to limit the query to a subset of the data, if desired. Note that filtering can occur in the tables afterward as well.\nThe Cell IDs field lets you put in one or more root IDs, cell IDs (i.e. “Nucleus ID”), or IDs from the annotation table being queried. Note that if you use this option, you havet to set the dropdown to the appropriate ID type.\nThe Value Search field lets you search for a particular value, such as a specific cell type.\nOnce you have selected your table and set any pre-query filters, click the Submit button to run the query. When the query is done, a green bar will appear below the query input stating that it’s showing the state of the table you queried.\nThe first set of controls allow you to build a Neuroglancer link for the whole table using the “Generate Link” button.\nOptionally, if you would like to make a link where different values of a given column — example, different cell types — are shown in different layers, you can turn on the “Group Annotations” toggle on the right and select the column you want to use for groupings.\nAfter setting these controls, click the “Generate Link” button to create the Neuroglancer link.\n\n\n\nTable viewer query results.\n\n\nThe table itself is shown below the controls. You can sort the table by clicking on the column headers, and filter the table by typing in the filter box below the column headers.\nFor example, if you want to find only cells with cell type “5P-NP” in a cell type column, simply type 5P-NP in the box under the column header. Similarly, to find only cells whose nucleus volume is greater than 500, type &gt;500 in the box under the volume column header in a table with nucleus information.\nIn addition, individual rows can be selected by clicking on the checkbox to the left of the row.\nThe left button along the top is a Neuroglancer link specific to the selected rows, if present, or the table as filtered if no rows are selected.\nThe orange button will deselect all checkboxes, and the “Export to CSV” will download the table as filtered to a CSV.\n\n\n\n\n\n\nWarning\n\n\n\nIf the table is too large (well beyond 10,000 rows), there are too many annotations to show in Neuroglancer and no link will be automatically generated.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Dash Apps: Table Viewer"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "Background",
    "section": "",
    "text": "A 3D rendering of cells in the MICrONs dataset.\n\n\nThis IARPA MICrONS dataset spans a 1.4mm x .87mm x .84 mm volume of cortex in a P87 mouse. It is 400x larger than the previously released Phase 1 dataset, which can be found here. The dataset was imaged using two-photon microscopy, microCT, and serial electron microscopy, and then reconstructed using a combination of AI and human proofreading.\nFunctional imaging data of an estimated 75,000 pyramidal neurons in the volume contains the single cell responses to a variety of visual stimuli. That same tissue was imaged with high resolution electron microscopy and processed through a machine learning driven reconstruction pipeline to generate a large scale anatomical connectome. The anatomical data contains more than an estimated 200,000 cells, and 120,000 neurons (details). Automated synapse detection measured more than 523 million synapses.\nThis makes this dataset remarkable in a few key ways\n\nIt was the largest multi-modal connectomics dataset released and the largest connectomics dataset by minimum dimension, number of cells, and number of connections detected (as of releas in July 2021)\nIt is the first EM reconstruction of a mammalian circuit across multiple functional brain areas.\n\nOn this set of websites, you can find visualization tools, as well as instructions tutorials for how to query and download portions of the dataset programmatically. To “git started” analyzing data, checkout the MICrONS Programmatic Tutorials for anatomical data and the MICrONS Neural Data Access (NDA) repository for functional data.\nThe pre-print with details about the dataset is:\n\nMICrONs Consortium et al. Functional connectomics spanning multiple areas of mouse visual cortex. bioRxiv 2021.07.28.454025; doi: https://doi.org/10.1101/2021.07.28.454025",
    "crumbs": [
      "Introduction",
      "Background"
    ]
  },
  {
    "objectID": "background.html#a-functional-connectomics-dataset-spanning-multiple-visual-areas",
    "href": "background.html#a-functional-connectomics-dataset-spanning-multiple-visual-areas",
    "title": "Background",
    "section": "",
    "text": "A 3D rendering of cells in the MICrONs dataset.\n\n\nThis IARPA MICrONS dataset spans a 1.4mm x .87mm x .84 mm volume of cortex in a P87 mouse. It is 400x larger than the previously released Phase 1 dataset, which can be found here. The dataset was imaged using two-photon microscopy, microCT, and serial electron microscopy, and then reconstructed using a combination of AI and human proofreading.\nFunctional imaging data of an estimated 75,000 pyramidal neurons in the volume contains the single cell responses to a variety of visual stimuli. That same tissue was imaged with high resolution electron microscopy and processed through a machine learning driven reconstruction pipeline to generate a large scale anatomical connectome. The anatomical data contains more than an estimated 200,000 cells, and 120,000 neurons (details). Automated synapse detection measured more than 523 million synapses.\nThis makes this dataset remarkable in a few key ways\n\nIt was the largest multi-modal connectomics dataset released and the largest connectomics dataset by minimum dimension, number of cells, and number of connections detected (as of releas in July 2021)\nIt is the first EM reconstruction of a mammalian circuit across multiple functional brain areas.\n\nOn this set of websites, you can find visualization tools, as well as instructions tutorials for how to query and download portions of the dataset programmatically. To “git started” analyzing data, checkout the MICrONS Programmatic Tutorials for anatomical data and the MICrONS Neural Data Access (NDA) repository for functional data.\nThe pre-print with details about the dataset is:\n\nMICrONs Consortium et al. Functional connectomics spanning multiple areas of mouse visual cortex. bioRxiv 2021.07.28.454025; doi: https://doi.org/10.1101/2021.07.28.454025",
    "crumbs": [
      "Introduction",
      "Background"
    ]
  },
  {
    "objectID": "background.html#electron-microscopy-methods",
    "href": "background.html#electron-microscopy-methods",
    "title": "Background",
    "section": "Electron Microscopy methods",
    "text": "Electron Microscopy methods\nThe general approach to EM involves staining tissue with heavy metals and using electrons to imaging thin sections of tissue. Sections can either be created by cutting tissue into slices (“serial section”) followed by imaging, or by imaging the face of a block and shaving off a thin layer of tissue after each run (“block face”). In both approaches, each section corresponds to only 30–50 nanometers of thickness (although it can be a millimeter wide), and thus assembling a substantial volumes involves thousands or tens of thousands of sections in a row. Finally, computational methods are used to align and assemble the sections into a single volume and then to segment the volume into individual cells and synapses.\nWhile the resolution afforded by EM has been used to study the nervous system since 1959, the first true “connectomics” project that merged dense mapping of neurons and synaptic connectivity was the study of the nematode C. elegans led by Sydney Brenner, John White, and others. Advances in EM methods and computational methods in the last ten years have allowed tremendous scaling of connectomics datasets. The largest current mammalian datasets, including the MICrONs volume will consider here, now span more than a cubic millimeter, and invertebrate datasets now can cover the entire brain of a fruit fly.",
    "crumbs": [
      "Introduction",
      "Background"
    ]
  },
  {
    "objectID": "background.html#data-use-cases",
    "href": "background.html#data-use-cases",
    "title": "Background",
    "section": "Data Use Cases",
    "text": "Data Use Cases\n\nCell Types\n\n\n\n\n\nCell types: pyramidal cell, astrocyte, neurogliaform cell, bipolar cell\n\n\n\n\nYou can use this dataset to explore the morphology and connectivity of a very diverse variety of excitatory, inhibitory, and non-neuronal cell types throughout the volume.\nClick these links for example cells: bipolar cells, basket cells, a chandelier cell, martinotti cells, a layer 5 thick tufted, a layer 5 NP cell, layer 4 cells, layer 2/3 cells, blood vessels, astrocytes, microglia. For programmatic access of more systematic cell type information released thus far, explore MICrONS Tutorials.",
    "crumbs": [
      "Introduction",
      "Background"
    ]
  },
  {
    "objectID": "background.html#synaptic-connectivity",
    "href": "background.html#synaptic-connectivity",
    "title": "Background",
    "section": "Synaptic Connectivity",
    "text": "Synaptic Connectivity\n\n\n\n\n\nSynapses, inset\n\n\n\n\nQuery from hundreds of millions of synapses in the volume from any of the segmented cells. To see an example visualization with one cell’s inputs and outputs annotated click the button below. We are releasing an app that you can use to query this data non-programmatically and generate a synaptic visualization for any cell. To learn how to query the data programmatically yourself, look at this notebook on MICrONS Tutorials.\nSynaptic Visualization (neuroglancer)\nLaunch Connectivity Viewer (web app)\n\n\n\nFunctional Data\n\n\n\n\n\nDeconvolved activity traces\n\n\n\n\nAll the functional recordings from 115,372 regions of interest and 75,902 estimated neurons are available, including the response of those units to a variety of visual stimuli. You can download the database of functional recordings to explore how cells responded to visual stimuli. For a subset of the neurons, we have generated a correspondence between their functional recording and their anatomical reconstruction.\nTo learn how to query the functional databases and link them to the anatomical data, you can view this notebook on MICrONS NDA, or access a hosted database and interactive notebooks by following the instructions at this repository.",
    "crumbs": [
      "Introduction",
      "Background"
    ]
  },
  {
    "objectID": "annotation-tables.html",
    "href": "annotation-tables.html",
    "title": "Annotation Tables",
    "section": "",
    "text": "The minnie65_public data release includes a number of annotation tables that help label the dataset. This section describes the content of each of these tables — see here for instructions for how to query and filter tables.\nUnless otherwise specificied (i.e. via desired_resolution), all positions are in units of 4,4,40 nm/voxel resolution.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "annotation-tables.html#common-fields",
    "href": "annotation-tables.html#common-fields",
    "title": "Annotation Tables",
    "section": "Common Fields",
    "text": "Common Fields\nSeveral fields (or column names) are common to many tables. These fall into two main classes: the spatial point columns that are how we assign annotations to cells via points in the 3d space and book-keeping columns, that are used internally to track the state of the data.\n\nSpatial Point Columns\nMost tables have one or more Bound Spatial Points, which is a location in the 3d space that tells the annotation to remain associated with the root id at that location.\nBound spatial points have will have one prefix, usually pt (i.e. “point”) and three associated columns with different suffixes: _position, _supervoxel_id, and _root_id.\nFor a given prefix {pt}, the three columns are as follows:\n\nThe {pt}_position indicates the location of the point in 3d space.\nThe {pt}_supervoxel_id indicates a unique identifier in the segmentation, and is mostly internal bookkeeping.\nThe {pt}_root_id indicates the root id of the annotation at that location.\n\n\n\nBook-keeping Columns\nSeveral columns are common to many or all tables, and mostly used as internal book-keeping. Rather than describe these for every table, they will just be mentioned briefly here:\n\nCommon columns\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nA unique ID specific to the annotation within that table\n\n\ncreated\nInternal bookkeeping column, should always be t for data you can download\n\n\nvalid\nA unique ID specific to the annotation within that table\n\n\ntarget_id\nSome tables reference other tables, particularly the nucleus table. If present, this column will be the same as id\n\n\ncreated_ref / valid_ref / id_ref (optional)\nFor reference tables, the data shows both the created/valid/id of the reference annotation and the target annotation. The values with the _ref suffix are those of the reference table (usually something like proofreading state or cell type) and the values without a suffix ar ethose of the target table (usually a nucleus)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "annotation-tables.html#synapse-table",
    "href": "annotation-tables.html#synapse-table",
    "title": "Annotation Tables",
    "section": "Synapse Table",
    "text": "Synapse Table\nTable name: synapses_pni_v2\nThe only synapse table is synapses_pni_v2. This is by far the largest table in the dataset with 337 million entries, one for each synapse. It contains the following columns:\n\nSynapse table column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npre_pt_position / pre_pt_supervoxel_id / pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse.\n\n\npost_pt_position / post_pt_supervoxel_id / post_pt_root_id\nIThe bound spatial point data for the postsynaptic side of the synapse.\n\n\nsize\nThe size of the synapse in voxels. This correlates well, but not perfectly, with the surface area of synapse.\n\n\nctr_pt_position\nA position in the center of the detected synaptic junction. Of all points in the synapse table, this is usually the closest point to the surface (and thus mesh) of both neurons. Because it is at the edge of cells, it is not associated with a root id.\n\n\n\n# Synapse query: outputs\nclient.materialize.synapse_query(pre_ids=example_root_id)\n\n# Synapse query: inputs\nclient.materialize.synapse_query(post_ids=example_root_id)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "annotation-tables.html#nucleus-tables",
    "href": "annotation-tables.html#nucleus-tables",
    "title": "Annotation Tables",
    "section": "Nucleus tables",
    "text": "Nucleus tables\nThe ‘nucleus centroid’ of a cell is unlikely to change with proofreading, and so is a useful static identifier for a given cell. The results of automatic nucleus segmentation and neuron-detection are avialable in the following tables. These tables are often the ‘reference’ table for other annotations.\n\nNucleus Detection Table\nTable name: nucleus_detection_v0\nNucleus detection has been used to define unique cells in the dataset. Distinct from the neuronal segmentation, a convolutional neural network was trained to segment nuclei. Each nucleus detection was given a unique ID, and the centroid of the nucleus was recorded as well as its volume. Many other tables in the dataset are reference tables on nucleus_detection_v0, meaning they are linked by the same annotation id. The id of the segmented nucelus, a 6-digit integer, is static across data versions and for this reason is the preferred method to identify the same ‘cell’ across time.\nThe key columns of nucleus_detection_v0 are:\n\nNucleus table column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\n6-digit number of the segmentation for that nucleus; ‘nucleus ID’\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\n\nNote that the id column is the nucleus ID, also called the ‘soma ID’ or the ‘cell ID’.\n# Standard query\nclient.materialize.query_table('nucleus_detection_v0')\n\n# Content-aware query\nclient.materialize.tables.nucleus_detection_v0(id=example_nucleus_id).query()\n\n\nNucleus brain area assignment\nTable name: nucleus_functional_area_assignment\nGiven the nucleus detection table nucleus_detection_v0 and the transformation of 2-photon in vivo imaging to the EM structural space (see functional-coregistration-tables ), each cell in the volume has been assigned to one of four visual cortical areas.\nThe inferred functional brain area based on the position of the nucleus in the EM volume. Area boundaries estimated from the area-membership assignments of the 2P recorded cells, after transformation to EM space.\nThe table nucleus_functional_area_assignment is a reference table on nucleus_detection_v0 and adds the following columns:\n\nFunctional area assignment\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\ntag\nthe brain area label (one of V1, AL, RL, LM)\n\n\nvalue\nthe distance to the area boundary (in um), calculated as the mean-distance of the 10 nearest neighbors in a non-matching brain area. A larger value is further from the area boundaries, and can be interpretted as higher confidence in area assignment.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('nucleus_functional_area_assignment')\n\n# Content-aware query\nclient.materialize.tables.nucleus_functional_area_assignment(tag='V1').query()\n\n\n\n\n\n\nNeuron-Nucleus Table\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis table is superseded by other cell typing below, but remains here for convenient reference as it is relevant to other tables such as aibs_metamodel_celltypes_v661.\n\n\nTable name: nucleus_ref_neuron_svm\nWhile the table of centroids for all nuclei is nucleus_detection_v0, this includes neuronal nuclei, non-neuronal nuclei, and some erroneous detections. The table nucleus_ref_neuron_svm shows the results of a classifier that was trained to distinguish neuronal nuclei from non-neuronal nuclei and errors. For the purposes of analysis, we recommend using the nucleus_ref_neuron_svm table to get the most broad collection of neurons in the dataset.\nThe key columns of nucleus_ref_neuron_svm are:\n\nNucleus table column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\nclassification-system\nDescribes how the classification was done. All values will be is_neuron for this table\n\n\ncell_type\nThe output of the classifier. All values will be either neuron or not-neuron (glia or error) for this table\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('nucleus_ref_neuron_svm')\n\n# Content-aware query\nclient.materialize.tables.nucleus_ref_neuron_svm(target_id=example_nucleus_id).query()",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "annotation-tables.html#cell-type-tables",
    "href": "annotation-tables.html#cell-type-tables",
    "title": "Annotation Tables",
    "section": "Cell Type Tables",
    "text": "Cell Type Tables\nThere are several tables that contain information about the cell type of neurons in the dataset, with each table representing a different method of doing the classificaiton. Because each method requires a different kind of information, not all cells are present in all tables. Each of the cell types tables has the same format and in all cases the id column references the nucleus id of the cell in question.\n\nManual Cell Types (V1 Column)\nTable name: allen_v1_column_types_slanted_ref\nA subset of nucleus detections in a 100 um column (n=2204) in VISp were manually classified by anatomists at the Allen Institute into categories of cell subclasses, first distinguishing cells into classes of non-neuronal, excitatory and inhibitory; then into subclasses.\nFor the non-neuronal subclasses, see aibs_column_nonneuronal_ref\nThe key columns are:\n\nAIBS Manual Cell Types, V1 Column\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nOne of aibs_coarse_excitatory or aibs_coarse_inhibitory for detected neurons, or aibs_coarse_nonneuronal for non-neurons (glia/pericytes).\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\nThe cell types in the table are:\n\n\n\n\n\n\nManual Cell Types (neurons)\n\n\n\n\n\n\nAIBS Manual Cell Type definitions (neurons)\n\n\n\n\n\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\n23P\nExcitatory\nLayer 2/3 cells\n\n\n4P\nExcitatory\nLayer 4 cells\n\n\n5P-IT\nExcitatory\nLayer 5 intratelencephalic cells\n\n\n5P-ET\nExcitatory\nLayer 5 extratelencephalic cells\n\n\n5P-NP\nExcitatory\nLayer 5 near-projecting cells\n\n\n6P-IT\nExcitatory\nLayer 6 intratelencephalic cells\n\n\n6P-CT\nExcitatory\nLayer 6 corticothalamic cells\n\n\nBC\nInhibitory\nBasket cell\n\n\nBPC\nInhibitory\nBipolar cell. In practice, this was used for all cells thought to be VIP cell, not only those with a bipolar dendrite\n\n\nMC\nInhibitory\nMartinotti cell. In practice, this label was used for all inhibitory neurons that appeared to be Somatostatin cell, not only those with a Martinotti cell morphology\n\n\nUnsure\nInhibitory\nUnsure. In practice, this label also is used for all likely-inhibitory neurons that did not match other types\n\n\n\n\n\n\n# Standard query\nclient.materialize.query_table('allen_v1_column_types_slanted_ref')\n\n# Content-aware query\nclient.materialize.tables.allen_v1_column_types_slanted_ref(id=example_nucleus_id).query()\n\n\n\n\n\n\nManual Cell Types (non-neurons)\n\n\n\n\n\n\nAIBS Manual Cell Type definitions (non-neurons)\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\nOPC\nNon-neuronal\nOligodendrocyte precursor cell\n\n\nastrocyte\nNon-neuronal\nAstrocyte\n\n\nmicroglia\nNon-neuronal\nMicroglia\n\n\npericyte\nNon-neuronal\nPericyte\n\n\noligo\nNon-neuronal\nOligodendrocyte\n\n\n\n\n\n\n# Standard query\nclient.materialize.query_table('aibs_column_nonneuronal_ref')\n\n# Content-aware query\nclient.materialize.tables.aibs_column_nonneuronal_ref(id=example_nucleus_id).query()\n\n\nPredictions from soma/nucleus features\nTable name: aibs_metamodel_celltypes_v661\nThis table contains the results of a hierarchical classifier trained on features of the cell body and nucleus of cells. This was applied to most cells in the dataset that had complete cell bodies (e.g. not cut off by the edge of the data), detected by nucleus_detection_v0, with small-objects and multi-soma errors removed. The model was run with cell-based features as of version 661 of the dataset. For more details, see (Elabbady et al. 2025). In general, this does a good job, but sometimes confuses layer 5 inhibitory neurons as being excitatory:\nThe key columns are:\n\nAIBS Soma Nuc Metamodel Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nOne of excitatory_neuron or inhibitory_neuron for detected neurons, or nonneuron for non-neurons (glia/pericytes).\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\nThe cell types in the table are:\n\n\n\n\n\n\nSoma Nuc Metamodel Cell types\n\n\n\n\n\n\nAIBS Soma Nuc Metamodel: Cell Type definitions\n\n\n\n\n\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\n23P\nExcitatory\nLayer 2/3 cells\n\n\n4P\nExcitatory\nLayer 4 cells\n\n\n5P-IT\nExcitatory\nLayer 5 intratelencephalic cells\n\n\n5P-ET\nExcitatory\nLayer 5 extratelencephalic cells\n\n\n5P-NP\nExcitatory\nLayer 5 near-projecting cells\n\n\n6P-IT\nExcitatory\nLayer 6 intratelencephalic cells\n\n\n6P-CT\nExcitatory\nLayer 6 corticothalamic cells\n\n\nBC\nInhibitory\nBasket cell\n\n\nBPC\nInhibitory\nBipolar cell. In practice, this was used for all cells thought to be VIP cell, not only those with a bipolar dendrite\n\n\nMC\nInhibitory\nMartinotti cell. In practice, this label was used for all inhibitory neurons that appeared to be Somatostatin cell, not only those with a Martinotti cell morphology\n\n\nNGC\nInhibitory\nNeurogliaform cell. In practice, this label also is used for all inhibitory neurons in layer 1, many of which may not be neurogliaform cells although they might be in the same molecular family\n\n\nOPC\nNon-neuronal\nOligodendrocyte precursor cell\n\n\nastrocyte\nNon-neuronal\nAstrocyte\n\n\nmicroglia\nNon-neuronal\nMicroglia\n\n\npericyte\nNon-neuronal\nPericyte\n\n\noligo\nNon-neuronal\nOligodendrocyte\n\n\n\n\n\n\n# Standard query\nclient.materialize.query_table('aibs_metamodel_celltypes_v661')\n\n# Content-aware query\nclient.materialize.tables.aibs_metamodel_celltypes_v661(id=example_nucleus_id).query()\nPrevious versions of this table include: aibs_soma_nuc_metamodel_preds_v117 (run on a subset of data, the V1 column) and aibs_soma_nuc_exc_mtype_preds_v117 (using training data labeled by another classifier: see mtypes below).\n\n\nCoarse prediction from spine detection\nTable name: baylor_log_reg_cell_type_coarse_v1\nThis table contains the results of a logistic regression classifier trained on properties of neuronal dendrites. This was applied to many cells in the dataset, but required more data than soma and nucleus features alone and thus more cells did not complete the pipeline. It has very good performance on excitatory vs inhibitory neurons because it focuses on dendritic spines, a characteristic property of excitatory neurons. It is a good table to double check E/I classifications if in doubt.\nFor details, see (Celii et al. 2025).\nThe key columns are:\n\nBaylor Dendrite Feature Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nbaylor_log_reg_cell_type_coarse for all entries\n\n\ncell_type\nexcitatory or inhibitory\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('baylor_log_reg_cell_type_coarse_v1')\n\n# Content-aware query\nclient.materialize.tables.baylor_log_reg_cell_type_coarse_v1(id=example_nucleus_id).query()\n\n\nFine prediction from dendritic features\nTable name: aibs_metamodel_mtypes_v661_v2\nExcitatory neurons and inhibitory neurons were distinguished with the soma-nucleus model (Elabbady et al. 2025), and subclasses were assigned based on a data-driven clustering of the neuronal features. Inhibitory neurons were classified based on how they distributed they synaptic outputs onto target cells, while exictatory neurons were classified based on a collection of dendritic features.\nThis was applied to most detected neurons in the dataset that had complete cell bodies (e.g. not cut off by the edge of the data), detected by nucleus_detection_v0, with small-objects and multi-soma errors removed. The model was run with cell-based features as of version 661 of the dataset.For more details, see (Schneider-Mizell et al. 2025).\nNote that all cell-type labels in this table come from a clustering specific to this paper, and while they are intended to align with the broader literature they are not a direct mapping or a well-established convention.\nFor a more conventional set of labels on the same set of cells, look at the manual table allen_v1_column_types_slanted_ref. Cell types in that table align with those in the aibs_metamodel_celltypes_v661.\nThe key columns are:\n\nAllen Motif-type (mtype) Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nexcitatory or inhibitory\n\n\ncell_type\nOne of several cell types, detailed below\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\nThe cell types in the table are:\n\n\n\n\n\n\nM-type Cell Type definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\nCell Type\nSubclass\nDescription\n\n\n\n\nL2a\nExcitatory\nA cluster of layer 2 (upper layer 2/3) excitatory neurons\n\n\nL2b\nExcitatory\nA cluster of layer 2 (upper layer 2/3) excitatory neurons\n\n\nL3a\nExcitatory\nA cluster of excitatory neurons transitioning between upper and lower layer 2/3\n\n\nL3b\nExcitatory\nA cluster of layer 3 (upper layer 2/3) excitatory neurons\n\n\nL3c\nExcitatory\nA cluster of layer 3 (upper layer 2/3) excitatory neurons\n\n\nL4a\nExcitatory\nThe largest cluster of layer 4 excitatory neurons\n\n\nL4b\nExcitatory\nAnother cluster of layer 4 excitatory neurons\n\n\nL4c\nExcitatory\nA cluster of layer 4 excitatory neurons along the border with layer 5\n\n\nL5a\nExcitatory\nA cluster of layer 5 IT neurons at the top of layer 5\n\n\nL5b\nExcitatory\nA cluster of layer 5 IT neurons throughout layer 5\n\n\nL5ET\nExcitatory\nThe cluster of layer 5 ET neurons\n\n\nL5NP\nExcitatory\nThe cluster of layer 5 NP neurons\n\n\nL6a\nExcitatory\nA cluster of layer 6 IT neurons at the top of layer 6\n\n\nL6b\nExcitatory\nA cluster of layer 6 IT neurons throughout layer 6. Note that this is different than the label “Layer 6b” which refers to a narrow band at the border between layer 6 and white matter\n\n\nL6c\nExcitatory\nA cluster of tall layer 6 cells (unsure if IT or CT)\n\n\nL6CT\nExcitatory\nA cluster of tall layer 6 cells matching manual CT labels\n\n\nL6wm\nExcitatory\nA cluster of layer 6 cells along the border with white matter\n\n\nPTC\nInhibitory\nPerisomatic targeting cells, a cluster of inhibitory neurons that target the soma and proximal dendrites of excitatory neurons. Approximately corresponds to basket cell\n\n\nDTC\nInhibitory\nDendrite targeting cells, a cluster of inhibitory neurons that target the distal dendrites of excitatory neurons. Most SST cells would be DTC\n\n\nSTC\nInhibitory\nSparsely targeting cells, a cluster of inhibitory neurons that don’t concentrate multiple synapses onto the same target neurons. Many neurogliaform cells and layer 1 interneurons fall into this category\n\n\nITC\nInhibitory\nInhibitory targeting cells, a cluster of inhibitory neurons that preferntially target other inhibitory neurons. Most VIP cells would be ITCs\n\n\n\n\n\n\n# Standard query\nclient.materialize.query_table('aibs_metamodel_mtypes_v661_v2')\n\n# Content-aware query\nclient.materialize.tables.aibs_metamodel_mtypes_v661_v2(id=example_nucleus_id).query()\nPrevious versions of this table include: allen_column_mtypes_v1 (run on a subset of data, the V1 column)",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "annotation-tables.html#proofreading-tables",
    "href": "annotation-tables.html#proofreading-tables",
    "title": "Annotation Tables",
    "section": "Proofreading Tables",
    "text": "Proofreading Tables\n\nProofreading Status and Strategy\nTable name: proofreading_status_and_strategy\nThe table proofreading_status_and_strategy describes the status of cells that have undergone manual proofreading.\nBecause of the inherent difference in the difficulty and time required for different kinds of proofreading, we describe the status of axons and dendrites separately.\nEach compartment status may be either:\n\nFALSE: indicates no comprehensive proofreading has been performed, or is not applicable.\n\nTRUE: indicates that false merges have been comprehensively removed, and the compartment is at least ‘clean’. Consult the strategy column if completeness of the compartment is relevant to your research.\n\nAn axon or dendrite labeled as status=TRUE can be trusted to be correct, but may not be complete. The degree of completion can be read from the strategy column. For more information, please see Proofreading and Data Quality; or also the microns-explorer page on proofreading strategies.\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made. NOTE: if this does not match the pt_root_id then the cell has undergone further changes. This is usually and improvement in proofreading, but proceed with caution.\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. May be TRUE or FALSE\n\n\nstatus_axon\nThe status of the axon proofreading. May be TRUE or FALSE\n\n\nstrategy_dendrite\nThe strategy employed to proofread the dendrite. See strategy table below for details\n\n\nstrategy_axon\nThe strategy employed to proofread the axon. See strategy table below for details\n\n\n\nThe specific strategies are as follows (and will update over time):\n\nProofreading Strategies\n\nProofreading Strategy Table\n\n\n\n\n\n\nStrategy\nDescription\n\n\n\n\nnone\nNo cleaning, and no extension. Indicates an entry in proofreading_status that is FALSE for that compartment\n\n\ndendrite_clean\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. The dendrite may be incorrectly truncated by segmentation error. Not all dendrite tips have been checked for extension. No comprehensive attempt was made to re-attach spine heads.\n\n\ndendrite_extended\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. Every tip was identified, manually inspected, and extended if possible. No comprehensive attempt was made to re-attach spine heads.\n\n\naxon_column_truncated\nAThe axon was extended within the V1 cortical column, with a preference for local connections. In some cases the axon was cut at the column boundary and/or the layer boundary, especially the boundary between layers 2/3 and layer 4. Output synapses represent a sampling of potential partners\n\n\naxon_interareal\nThe axon was extended with a preference for branches that projected to other brain areas. Some axon branches were fully extended, but local connections may be incomplete. Output synapses represent a sampling of potential partners.\n\n\naxon_partially_extended\nThe axon was extended outward from the soma, following each branch to its termination. Output synapses represent a sampling of potential partners.\n\n\naxon_fully_extended\nAxon was extended outward from the soma, following each branch to its termination. After initial extension, every endpoint was identified, manually inspected, and extended again if possible. Output synapses represent a largely complete sampling of partners.\n\n\n\nThis table, proofreading_status_and_strategy, supercedes proofreading_status_public_release.\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()\n\n\n\n\n\n\nProofreading status at public release\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis table is out-of-date, and remains here for convenient reference\n\n\nTable name: proofreading_status_public_release\nThe table proofreading_status_public_release describes the status of cells selected for manual proofreading.\nBecause of the inherent difference in the challenge and time required for different kinds of proofreading, we describe the status of axons and dendrites separately. Further, we distinguish three different categories of proofreading:\n\nnon: No proofreading has been comprehensively performed.\nclean: Proofreading has comprehensively removed false merges, but not necessarily added missing parts.\nextended: Proofreading has comprehensively removed false merges and attempted to add all or most missing parts.\n\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. One of the three categories described above\n\n\nstatus_axon\nThe status of the axon proofreading. One of the three categories described above\n\n\n\nThis table, proofreading_status_public_release, is superceded by proofreading_status_and_strategy and will be deprecated.\n# Set version to 1078 or before\nclient.version = 1078\n\n# Standard query\nclient.materialize.query_table('proofreading_status_public_release')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_public_release(status_axon='extended').query()",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "annotation-tables.html#functional-coregistration-tables",
    "href": "annotation-tables.html#functional-coregistration-tables",
    "title": "Annotation Tables",
    "section": "Functional Coregistration Tables",
    "text": "Functional Coregistration Tables\nTo relate the structural data to functional data, cell bodies must be coregistered between the functional imaging and EM volumes. As with cell-typing, there are manual and automated methods for doing this, with the former having higher accuracy but lower throughput.\n\ncoregistration_manual_v4 : The results of manually verified coregistration. This table is well-verified, but contains fewer ROIs (N=15,352 root ids, 19,181 ROIs).\ncoregistration_auto_phase3_fwd_apl_vess_combined_v2 : The results of automated functional matching between the EM and 2-p functional data. This table is not manually verified, but contains more ROIs (N=35,466 root ids, N=83,046 ROIs).\n\n\nManual coregistration\nTable name: coregistration_manual_v4\nA table of EM nucleus centroids manually matched to Baylor functional units. A unique functional unit is identified by its session, scan_idx and unit_id. An EM nucleus centroid may have matched to more than one functional unit if it was scanned on more than one imaging field.\nThe key columns are:\n\nCoregistration table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nsession\nThe session index from functional imaging\n\n\nscan_idx\nThe scan index from functional imaging\n\n\nunit_id\nThe functional unit index from imaging. Only unique within scan and session\n\n\nfield\nThe field index from functional imaging\n\n\nresidual\nThe residual distance between the functional and the assigned structural points after transformation, in microns\n\n\nscore\nA separation score, measuring the difference between the residual distance to the assigned neuron and the distance to the nearest non-assigned neuron, in microns. This can be negative if the non-assigned neuron is closer than the assigned neuron. Larger values indicate fewer nearby neurons that could be confused with the assigned neuron.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis table includes duplicate entries for the same ‘pt_root_id’ and nucleus id if the coregistered cell has multiple unit recordings\n\n\n# Standard query\nclient.materialize.query_table('coregistration_manual_v4')\n\n# Content-aware query\nclient.materialize.tables.coregistration_manual_v4(id=example_nucleus_id).query()\nThis table coregistration_manual_v4 supercedes previous iterations of this table:\n\ncoregistration_manual_v3\ncoregistration_manual\n\n\n\nAutomated coregistration\nTable name: coregistration_auto_phase3_fwd_apl_vess_combined_v2\nA table of EM nucleus centroids automatically matched to Baylor functional units. This table reconciles the following two tables that both make a best match of the of registration using different techniques: coregistration_auto_phase3_fwd_v2 and apl_functional_coreg_vess_fwd. A unique functional unit is identified by its session, scan_idx and unit_id. An EM nucleus centroid may have matched to more than one functional unit if it was scanned on more than one imaging field.\nThe key columns are:\n\nCoregistration table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nsession\nThe session index from functional imaging\n\n\nscan_idx\nThe scan index from functional imaging\n\n\nunit_id\nThe functional unit index from imaging. Only unique within scan and session\n\n\nfield\nThe field index from functional imaging\n\n\nresidual\nThe residual distance between the functional and the assigned structural points after transformation, in microns\n\n\nscore\nA separation score, measuring the difference between the residual distance to the assigned neuron and the distance to the nearest non-assigned neuron, in microns. This can be negative if the non-assigned neuron is closer than the assigned neuron. Larger values indicate fewer nearby neurons that could be confused with the assigned neuron.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis table includes duplicate entries for the same pt_root_id and nucleus id if the coregistered cell has multiple unit recordings\n\n\n# Standard query\nclient.materialize.query_table('coregistration_auto_phase3_fwd_apl_vess_combined_v2')\n\n# Content-aware query\nclient.materialize.tables.coregistration_auto_phase3_fwd_apl_vess_combined_v2(id=example_nucleus_id).query()\nThis table coregistration_auto_phase3_fwd_apl_vess_combined_v2 supercedes previous iterations of this table:\n\ncoregistration_auto_phase3_fwd_apl_vess_combined_v2\napl_functional_coreg_forward_v5\n\n\n\nFunctional properties\nTable name: functional_properties_v3_bcm\nA summary of the functional properties for each of the coregistered neurons (as of coregistration_manual_v3). For details, see (Ding et al. 2025)\nThe key columns are:\n\nFuctional properties of coregistered neurons\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\nsession\nThe session index from functional imaging\n\n\nscan_idx\nThe scan index from functional imaging\n\n\nunit_id\nThe functional unit index from imaging. Only unique within scan and session\n\n\npref_ori\npreferred orientation in radians (0 - pi), horizontal bar moving upward is 0 and orientation increases clockwise, extracted from model responses to oriented noise stimuli\n\n\npref_dir\npreferred direction in radians (0 - 2pi), horizontal bar moving upward is 0 and orientation increases clockwise, extracted from model responses to oriented noise stimuli\n\n\ngOSI\nglobal orientation selectivity index\n\n\ngDSI\nglobal direction selectivity index\n\n\ncc_abs\nprediction performance of the model, higher is better\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis table includes duplicate entries for the same ‘pt_root_id’ and nucleus id if the coregistered cell has multiple unit recordings\n\n\n# Standard query\nclient.materialize.query_table('functional_properties_v3_bcm')\n\n# Content-aware query\nclient.materialize.tables.functional_properties_v3_bcm(id=example_nucleus_id).query()",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "annotation-tables.html#overview-of-relevant-tables",
    "href": "annotation-tables.html#overview-of-relevant-tables",
    "title": "Annotation Tables",
    "section": "Overview of relevant tables",
    "text": "Overview of relevant tables\n\nheard you like tables–here’s a table for your tables\n\n\n\n\n\n\n\nTable Name\nNumber of Annotations\nDescription\n\n\n\n\nsynapses_pni_v2\n337,312,429\nThe locations of synapses and the segment ids of the pre and post-synaptic automated synapse detection\n\n\nnucleus_detection_v0\n144,120\nThe locations of nuclei detected via a fully automated method\n\n\nnucleus_alternative_points\n8,388\nA reference annotation table marking alternative segment_id lookup locations for a subset of nuclei in nucleus_detection_v0 that is more accurate than the centroid location listed there\n\n\nnucleus_ref_neuron_svm\n144,120\nreference annotation indicating the output of a model detecting which nucleus detections are neurons versus which are not 1\n\n\ncoregistration_manual_v4\n19,181\nA table indicating the association between individual units in the functional imaging data and nuclei in the structural data, derived from human powered matching. Includes residual and separation scores to help assess confidence\n\n\ncoregistration_auto_phase3_fwd_apl_vess_combined_v2\n83,046\nA table indicating the association between individual units in the functional imaging data and nuclei in the structural data, derived from the automated procedure. Includes residuals and separation scores to help assess confidence\n\n\nproofreading_status_and_strategy\n1865\nA table indicating which neurons have been proofread on their axons or dendrites\n\n\naibs_column_nonneuronal_ref\n542\nCell type reference annotations from a human expert of non-neuronal cells located amongst the Minnie Column\n\n\nallen_v1_column_types_slanted_ref\n1,357\nNeuron cell type reference annotations from human experts of neuronal cells located amongst the Minnie Column\n\n\nallen_column_mtypes_v1\n1,357\nNeuron cell type reference annotations from data driven unsupervised clustering of neuronal cells\n\n\naibs_metamodel_mtypes_v661_v2\n72,158\nReference annotations indicating the output of a model predicting cell types across the dataset based on the labels from allen_column_mtypes_v1.1\n\n\naibs_metamodel_celltypes_v661\n94,014\nReference annotations indicating the output of a model predicting cell classes based on the labels from allen_v1_column_types_slanted_ref and aibs_column_nonneuronal_ref\n\n\nbaylor_log_reg_cell_type_coarse_v1\n55,063\nReference annotations indicated the output of a logistic regression model predicting whether the nucleus is part of an excitatory or inhibitory cell\n\n\nbaylor_gnn_cell_type_fine_model_v2\n49,051\nReference annotations indicated the output of a graph neural network model predicting the cell type based on the human labels in allen_v1_column_types_slanted_ref\n\n\nvortex_astrocyte_proofreading_status\n12\nThis table reports the status of a manually selected subset of astrocytes within the VISP column. Astrocyte seelection and proofreading performed as part of VORTEX.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Annotation Tables"
    ]
  },
  {
    "objectID": "dash-connectivity.html",
    "href": "dash-connectivity.html",
    "title": "Dash Apps: Connectivity Viewer",
    "section": "",
    "text": "We have created two webapps to help explore and visualize the data. These apps were created using the Dash framework, hence the name.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Dash Apps: Connectivity Viewer"
    ]
  },
  {
    "objectID": "dash-connectivity.html#connectivity-and-cell-type-viewer",
    "href": "dash-connectivity.html#connectivity-and-cell-type-viewer",
    "title": "Dash Apps: Connectivity Viewer",
    "section": "Connectivity and Cell Type Viewer",
    "text": "Connectivity and Cell Type Viewer\nThe Connectivity Viewer is designed to let you glance at the synaptic output or input of a given cell and group connectivity by cell type or other annotations.\n\n\n\nConnectivity viewer input options.\n\n\nAlong the top, you can enter an Cell ID (either a root ID or a cell ID) and, optionally, a table to use to define cell types for synaptic partners.\n\n\n\n\n\n\nNote\n\n\n\nNote that if you want to use cell IDs from the nucleus detection table, you must select the Nucleus ID option from the dropdown next to the entry box.\n\n\nSelect the data version from the dropdown. (the Live Query option is not relevent for the public data)\nFor the optional Table dropdown, there are several tables to choose from, though the most informative will be one of the cell type classification tables. The aibs_metamodel_celltypes_v661 has the greatest coverage of the dataset, and is described in Perisomatic ultrastructure efficiently classifies cells in mouse cortex” (Elabbady et al. 2025).\nThe table aibs_metamodel_mtypes_v661_v2 is the most current version version of the cell type classification, described in “A connectomic census of mouse visual cortex” (Schneider-Mizell et al. 2025).\nPressing “Submit” will query the database for the cell and its synapses.\nOnce completed, you will see the bar under the input options turn green and report the data that it is displaying. For example, if we query root id 864691135408247241 and the default cell type table, we will see the following:\n\nImportantly, the data displayed below will match whatever is in this bar — if your query takes a long time or fails for some reason (i.e.bad internet, server error), the tables displayed might be stale.\nThere are two locations the resulting data is displayed, as we’ll see below. First, a bar of tabs allows you to quickly visualize the data in different ways. Second, the table can present synaptic partners in a more detailed way.\n\nVisualization tabs\nThe first and default option in the tabs is Tables Only which doesn’t actually show any data. This is merely a placeholder to let you look directly at the tablular data below.\nThe next tab, Plots lets you visualize the synaptic output of a neuron.\nWe focus on synaptic output only right now, because the data quality is such that automated dendritic reconstructions are reliable, but automated axon reconstructions are not (see Proofreading section). Because of that, if a neuron has been proofread we trust the cell types of its postsynaptic partners as a group but do not trust the cell types of its presynaptic partners.\n\n\n\nPlot view. Left, violin plot of the synaptic distribution of the queried cell. Blue/left is the distribution of the cortical depth of input synapses along, red/right is the distribution of cortical depth of synaptic outputs. Approximate layers are shown. Center, a scatterplot with a dot for each synaptic output of the queried cell that is onto a target with a unique cell body (i.e. no dendritic fragments and no cells erroneously merged into other cells). The y-axis is the depth of the synapse itself, while the x-axis is the depth of the cell body of the target neuron. If a cell body categroy is selected, the dots are colored by the cell type of the target neuron. Right, a bar plot showing of the number of synapses onto target cell types.\n\n\nThese plots are designed to give you a quick overview of a cell’s synaptic outputs.\nTo add a particularly cell type column, use the Color by value: dropdown below the row of plots to select a column to color by.\nThese plots are handled in Plotly, so you can hover over the plots to see more information, and you can click and drag to zoom in on a particular region and save by clicking on the camera icon. For the scatterplot, you can also turn on and off individual types by clicking on the legend.\nThe next tab, Neuroglancer Links lets you visualize the synapses of a neuron in Neuroglancer.\n\n\n\nNeuroglancer links. From left to right: Show all synaptic inputs to the cell, show all synaptic inputs to the cell grouped into different layers by value in the associated table (e.g. cell type), show all synaptic outputs of the cell, show all synaptic outputs of the cell grouped into different layers by value in the associated table (e.g. cell type). Below the dropdown specifies which column to use if grouping annotations. The Include No Type toggle specifies whether to show synapses with cells that have no cell type annotation in the grouped cases.\n\n\nThe Neuroglancer links generated by these buttons will span all synapses of the queried cell, and do not reflect the filtering in the table below. The synapse annotations are also associated with the synaptic partners, making it easy to browse through a broad sampling of synaptic connectivity.\n\n\nConnectivity Table\nThe table below shows a table of all synaptic partners.\n\n\n\nTable view options.\n\n\nOne of the first things you can see in the table viewer is that it is split into two tabs, Input and Output, with the tab names for each showing the total number of synapses in each category.\nTo move between them, just click the tab you want.\nAbove the Input/Ouput tabs is a link to the Neuroglancer view of the synapses show in the table, after filtering and/or selecting a subset of rows.\nThe table view here is similar to in the Annotation Table Viewer described above. You can sort the table by clicking column headers, filter the table using the row underneath the headers, and select individual rows. If you select individual rows, the Neuroglancer link will group all synapses from each partner together, making it particularly easy to see how synaptic connectivity relates to neuronal anatomy.\nThe default sorting of the table is by total number of synapses, but note that both summed total synapse size (net_size) and average synapse size (mean_size) are also shown. These can be particularly important values when thinking about connectivity between excitatory neurons.\nA CSV file of the entire table can be downloaded by clicking the “Export to CSV” button.",
    "crumbs": [
      "Introduction",
      "Online Tools",
      "Dash Apps: Connectivity Viewer"
    ]
  },
  {
    "objectID": "examples/skeleton_load_and_generate.html",
    "href": "examples/skeleton_load_and_generate.html",
    "title": "Neuron Skeletons",
    "section": "",
    "text": "Created 7/2/2024 by Bethanny Danskin\nMICrONS Tutorial for VORTEX\nDepends on the following two packages: CAVEclient and Skeleton-Plot (version&gt;0.0.9)\nSkeletons and meshworks are loaded as MeshParty meshwork object\npcg-skel and cloudvolume are necessary if generating your own skeletons from the meshes\n\nUsing CAVEclient requires having set up a CAVE auth token. See how to set up your CAVEclient token here.\n\n\n# !pip install caveclient\n# !pip install skeleton-plot\n\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n\nfrom caveclient import CAVEclient\n# Initialize a client for the \"minnie65_public\" datastack.\nclient = CAVEclient(datastack_name='minnie65_public') \n\n# see the available materialization versions\nclient.materialize.get_versions()\n\n# Uncomment to get more details on each release version\n# client.materialize.get_versions_metadata()\n\n[1078, 117, 661, 343, 795, 943]\n\n\n\n# set materialization version, for consistency\nmaterialization = 1078 # current public as of 6/5/2024\n# materialization = 661 # version at which skeletons were pre-generated\nclient.version = materialization\n\n\nimport skeleton_plot as skelplot\nimport skeleton_plot.skel_io as skel_io\n\n\n\nThe skeletons of the meshes are calculated at specific timepoints. The last collection of all neurons in the dataset was at materialization version 661.\nBoth the .swc skeletons and .h5 meshwork objects are available in the BossDB repository\n\n# path to the skeleton .swc files\nskel_path = \"s3://bossdb-open-data/iarpa_microns/minnie/minnie65/skeletons/v661/skeletons/\"\n\n# path to the skeleton and meshwork .h5 files\nmesh_path = \"s3://bossdb-open-data/iarpa_microns/minnie/minnie65/skeletons/v661/meshworks/\"\n\n\n\n\n\n# Skeleton\nnucleus_id = 292685\nsegment_id = 864691135122603047\nskel_filename = f\"{segment_id}_{nucleus_id}.swc\"\n\n# load the .swc skeleton\nsk = skel_io.read_skeleton(skel_path, skel_filename)\n\n\n# load the meshwork (may take minutes to locate, if using public credentials)\nmesh_filename = f\"{segment_id}_{nucleus_id}.h5\"\nmw = skel_io.load_mw(mesh_path, mesh_filename)\n\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_skel(\n    sk,\n    title=nucleus_id,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\n\n\n\nThe meshwork object (h5 file) includes additional information about the input and output synapse positions along the skeleton\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_mw_skel(\n    mw, \n    title=nucleus_id,\n    pull_radius = True,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n    plot_presyn = True,\n    plot_postsyn = True,)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s say you have observed a cell in neuroglancer that you want to work with, like this pyramidal cell (https://neuroglancer.neuvue.io/?json_url=https://global.daf-apis.com/nglstate/api/v1/6320379988541440)\n\nroot_id_v1078 = 864691135356151759\n\nThe segment root_id may change with time, but the 6-digit nucleus id is much more static. Look up the nucleus id from the current root id\n\nnuc_df = client.materialize.tables.nucleus_detection_v0(pt_root_id=root_id_v1078).query()\nnucleus_id = nuc_df.id.item()\nprint(nucleus_id)\n\n490689\n\n\nNow use the nucleus id to look up the previous segmentation root id for v661\n(this is necessary to find the exact filename match. But alternately, you can use directory search to find the matching filenames with just the nucleus id)\n\nsegment_id = client.materialize.tables.nucleus_detection_v0(id=nucleus_id).query(\n    materialization_version=661).pt_root_id.item()\nsegment_id\n\n864691135851524807\n\n\n\n# Load Skeleton\nskel_filename = f\"{segment_id}_{nucleus_id}.swc\"\n\nsk = skel_io.read_skeleton(skel_path, skel_filename)\n\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_skel(\n    sk,\n    title=nucleus_id,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\nNote that this skeleton has no axon compartment. That is because the axon has only been proofread after the skeleton was created. Non-proofread cells do not have their axon statistics generated.\n\n\n\n\nCells that have undergone proofreading in the dataset are annotated in the proofreading_status_and_strategy table. For more information, see the MICrONS-Explorer proofreading documentation\n\n# Load cells that have undergone proofreading\nproof_df = client.materialize.tables.proofreading_status_and_strategy(status_axon=True).query(\n    select_columns = ['pt_root_id','pt_position','status_dendrite','status_axon','strategy_dendrite','strategy_axon'],\n    materialization_version=1078)\n\n# This gives you the list of cells with axonal proofreading, anywhere in the dataset\nproof_df.head()\n\nTable Owner Notice on proofreading_status_and_strategy: NOTE: this table supercedes 'proofreading_status_public_release'. For more details, see: www.microns-explorer.org/manifests/mm3-proofreading.\n\n\n\n\n\n\n\n\n\npt_root_id\nstatus_dendrite\nstatus_axon\nstrategy_dendrite\nstrategy_axon\npt_position\n\n\n\n\n0\n864691135617152361\nt\nt\ndendrite_extended\naxon_partially_extended\n[185152, 185344, 21255]\n\n\n1\n864691136090326071\nt\nt\ndendrite_extended\naxon_fully_extended\n[192080, 190064, 22297]\n\n\n2\n864691135082864887\nt\nt\ndendrite_extended\naxon_interareal\n[303659, 166262, 17349]\n\n\n3\n864691136195284556\nt\nt\ndendrite_extended\naxon_fully_extended\n[173184, 217472, 21929]\n\n\n4\n864691135565870679\nt\nt\ndendrite_clean\naxon_fully_extended\n[184384, 108896, 21755]\n\n\n\n\n\n\n\nNote: for v661, the current preferred proofreading table did not exist. Use proofreading_status_public_release instead\n\n# Load cells that have underwent proofreading for v661\nproof_df = client.materialize.tables.proofreading_status_public_release().query(\n    select_columns = ['pt_root_id','pt_position','status_dendrite','status_axon'],\n    materialization_version=661)\n\n# This gives you the list of cells with axonal proofreading, anywhere in the dataset\nproof_df.head()\n\nTable Owner Notice on proofreading_status_public_release: NOTE: this table is deprecated and no longer receiving updates; please use 'proofreading_status_and_strategy' which is available in datastack version &gt;= 1078 (datastack = minnie65_public or minnie65_phase3_v1).\n\n\n\n\n\n\n\n\n\npt_root_id\nstatus_dendrite\nstatus_axon\npt_position\n\n\n\n\n0\n864691134884807418\nextended\nextended\n[299067, 123129, 22993]\n\n\n1\n864691134885430010\nextended\nnon\n[181280, 223040, 21399]\n\n\n2\n864691134885645050\nextended\nnon\n[172288, 222528, 21607]\n\n\n3\n864691134918370314\nclean\nclean\n[170528, 226848, 20316]\n\n\n4\n864691134918461194\nclean\nclean\n[189760, 127520, 20540]\n\n\n\n\n\n\n\n\n\n\nThis will let you create skeletons for more recently proofread cells that do not exist in the publicly pregenerated .swc files. However, these skeletons will come without apical dendrite labels\npcg-skel documentation\n\n# !pip install pcg-skel\n# !pip install cloud-volume\n\n\nimport cloudvolume\nimport pcg_skel\n\n# specify the materialization version\nclient.materialize.version = 1078\n\n# initialize cloudvolume client\ncv_minnie = cloudvolume.CloudVolume(client.info.segmentation_source(), use_https=True)\n\n\n# pcg-skel pipeline code\ninput_id = 864691135639556411\nid_is_nuc = False\n\n# Cell identification meta\nsynapse_table = client.info.get_datastack_info()['synapse_table']\nif id_is_nuc:\n    id_col = 'id'\nelse:\n    id_col = 'pt_root_id'\nuse_view = True\nif use_view:\n    row = client.materialize.query_view('nucleus_detection_lookup_v1', \n                filter_equal_dict = {id_col: input_id})\nelse:\n    row = client.materialize.query_table('nucleus_detection_v0', \n                filter_equal_dict = {id_col: input_id})\n    \n    row = row.drop('created', axis = 1)\n\nnuc_id = int(row['id']) \nroot_id = int(row['pt_root_id'])\nroot_point = row['pt_position'].values[0]\n\nprint(f'starting on body {nuc_id}, {root_id}')\n\n# create whole neuron with radius info\nresample_spacing = 1510\ncollapse_soma = True\ncollapse_radius = 10_000\nres = [4, 4, 40]\nnrn =  pcg_skel.coord_space_meshwork(root_id,\n                                     client=client,\n                                     root_point=root_point,\n                                     root_point_resolution=res,\n                                     collapse_soma=collapse_soma,\n                                     collapse_radius=collapse_radius,\n                                     synapses='all',\n                                     synapse_table=synapse_table,\n                                     cv = cv_minnie)\n\n# add radius properties df to annotations \npcg_skel.features.add_volumetric_properties(nrn, client)\nprint('adding segment properties')\npcg_skel.features.add_segment_properties(nrn)\nprint('segment properties added')\n\nstarting on body 267207, 864691135639556411\nadding segment properties\nsegment properties added\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\n\nskelplot.plot_tools.plot_verts(nrn.skeleton.vertices, \n                               nrn.skeleton.edges, \n                               nrn.skeleton.radius,\n                               plot_soma=True,\n                               soma_node=nrn.skeleton.root,\n                               x=\"z\",\n                               y=\"y\")\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\nplt.gca().invert_yaxis()\n\n\n\n\n\n\n\n\n\n\n\n# create compartment label\ncompartment_labels = np.zeros(len(nrn.skeleton.vertices)).astype(int)\n\n# add soma label\ncompartment_labels[int(nrn.skeleton.root)]=1\n\n# get the mesh volume properties\nvolume_df = nrn.anno.segment_properties.df\n\n# add column indicating skel index \nvolume_df['skel_index'] = nrn.anno.segment_properties.mesh_index.to_skel_index_padded\nsk_volume_df = volume_df.drop_duplicates('skel_index').sort_values('skel_index').reset_index()\n\n# set map for skel index -&gt; radius\nradius_labels = np.array(sk_volume_df['r_eff']) / 1000\n\n# metadata dictionary for keeping vertex variables together\nskeleton_properties = {}\nskeleton_properties['compartment'] = compartment_labels\nskeleton_properties['vertices'] = nrn.skeleton.vertices\nskeleton_properties['edges'] = nrn.skeleton.edges\nskeleton_properties['radius'] = radius_labels\n\n\nnrn.skeleton.export_to_swc(filename = 'test.swc', \n                           node_labels=compartment_labels, \n                           radius=radius_labels,\n                           avoid_root=True, \n                           resample_spacing = resample_spacing)"
  },
  {
    "objectID": "examples/skeleton_load_and_generate.html#download-visualize-and-generate-neuron-skeletons",
    "href": "examples/skeleton_load_and_generate.html#download-visualize-and-generate-neuron-skeletons",
    "title": "Neuron Skeletons",
    "section": "",
    "text": "Created 7/2/2024 by Bethanny Danskin\nMICrONS Tutorial for VORTEX\nDepends on the following two packages: CAVEclient and Skeleton-Plot (version&gt;0.0.9)\nSkeletons and meshworks are loaded as MeshParty meshwork object\npcg-skel and cloudvolume are necessary if generating your own skeletons from the meshes\n\nUsing CAVEclient requires having set up a CAVE auth token. See how to set up your CAVEclient token here.\n\n\n# !pip install caveclient\n# !pip install skeleton-plot\n\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n\nfrom caveclient import CAVEclient\n# Initialize a client for the \"minnie65_public\" datastack.\nclient = CAVEclient(datastack_name='minnie65_public') \n\n# see the available materialization versions\nclient.materialize.get_versions()\n\n# Uncomment to get more details on each release version\n# client.materialize.get_versions_metadata()\n\n[1078, 117, 661, 343, 795, 943]\n\n\n\n# set materialization version, for consistency\nmaterialization = 1078 # current public as of 6/5/2024\n# materialization = 661 # version at which skeletons were pre-generated\nclient.version = materialization\n\n\nimport skeleton_plot as skelplot\nimport skeleton_plot.skel_io as skel_io\n\n\n\nThe skeletons of the meshes are calculated at specific timepoints. The last collection of all neurons in the dataset was at materialization version 661.\nBoth the .swc skeletons and .h5 meshwork objects are available in the BossDB repository\n\n# path to the skeleton .swc files\nskel_path = \"s3://bossdb-open-data/iarpa_microns/minnie/minnie65/skeletons/v661/skeletons/\"\n\n# path to the skeleton and meshwork .h5 files\nmesh_path = \"s3://bossdb-open-data/iarpa_microns/minnie/minnie65/skeletons/v661/meshworks/\"\n\n\n\n\n\n# Skeleton\nnucleus_id = 292685\nsegment_id = 864691135122603047\nskel_filename = f\"{segment_id}_{nucleus_id}.swc\"\n\n# load the .swc skeleton\nsk = skel_io.read_skeleton(skel_path, skel_filename)\n\n\n# load the meshwork (may take minutes to locate, if using public credentials)\nmesh_filename = f\"{segment_id}_{nucleus_id}.h5\"\nmw = skel_io.load_mw(mesh_path, mesh_filename)\n\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_skel(\n    sk,\n    title=nucleus_id,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\n\n\n\nThe meshwork object (h5 file) includes additional information about the input and output synapse positions along the skeleton\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_mw_skel(\n    mw, \n    title=nucleus_id,\n    pull_radius = True,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n    plot_presyn = True,\n    plot_postsyn = True,)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s say you have observed a cell in neuroglancer that you want to work with, like this pyramidal cell (https://neuroglancer.neuvue.io/?json_url=https://global.daf-apis.com/nglstate/api/v1/6320379988541440)\n\nroot_id_v1078 = 864691135356151759\n\nThe segment root_id may change with time, but the 6-digit nucleus id is much more static. Look up the nucleus id from the current root id\n\nnuc_df = client.materialize.tables.nucleus_detection_v0(pt_root_id=root_id_v1078).query()\nnucleus_id = nuc_df.id.item()\nprint(nucleus_id)\n\n490689\n\n\nNow use the nucleus id to look up the previous segmentation root id for v661\n(this is necessary to find the exact filename match. But alternately, you can use directory search to find the matching filenames with just the nucleus id)\n\nsegment_id = client.materialize.tables.nucleus_detection_v0(id=nucleus_id).query(\n    materialization_version=661).pt_root_id.item()\nsegment_id\n\n864691135851524807\n\n\n\n# Load Skeleton\nskel_filename = f\"{segment_id}_{nucleus_id}.swc\"\n\nsk = skel_io.read_skeleton(skel_path, skel_filename)\n\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\nskelplot.plot_tools.plot_skel(\n    sk,\n    title=nucleus_id,\n    line_width=1,\n    plot_soma=True,\n    invert_y=True,\n    pull_compartment_colors=True,\n    x=\"z\",\n    y=\"y\",\n    skel_color_map = { 3: \"firebrick\",4: \"salmon\",2: \"black\",1: \"olive\" },\n)\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\n# ax.axis('off')\n\n\n\n\n\n\n\n\nNote that this skeleton has no axon compartment. That is because the axon has only been proofread after the skeleton was created. Non-proofread cells do not have their axon statistics generated.\n\n\n\n\nCells that have undergone proofreading in the dataset are annotated in the proofreading_status_and_strategy table. For more information, see the MICrONS-Explorer proofreading documentation\n\n# Load cells that have undergone proofreading\nproof_df = client.materialize.tables.proofreading_status_and_strategy(status_axon=True).query(\n    select_columns = ['pt_root_id','pt_position','status_dendrite','status_axon','strategy_dendrite','strategy_axon'],\n    materialization_version=1078)\n\n# This gives you the list of cells with axonal proofreading, anywhere in the dataset\nproof_df.head()\n\nTable Owner Notice on proofreading_status_and_strategy: NOTE: this table supercedes 'proofreading_status_public_release'. For more details, see: www.microns-explorer.org/manifests/mm3-proofreading.\n\n\n\n\n\n\n\n\n\npt_root_id\nstatus_dendrite\nstatus_axon\nstrategy_dendrite\nstrategy_axon\npt_position\n\n\n\n\n0\n864691135617152361\nt\nt\ndendrite_extended\naxon_partially_extended\n[185152, 185344, 21255]\n\n\n1\n864691136090326071\nt\nt\ndendrite_extended\naxon_fully_extended\n[192080, 190064, 22297]\n\n\n2\n864691135082864887\nt\nt\ndendrite_extended\naxon_interareal\n[303659, 166262, 17349]\n\n\n3\n864691136195284556\nt\nt\ndendrite_extended\naxon_fully_extended\n[173184, 217472, 21929]\n\n\n4\n864691135565870679\nt\nt\ndendrite_clean\naxon_fully_extended\n[184384, 108896, 21755]\n\n\n\n\n\n\n\nNote: for v661, the current preferred proofreading table did not exist. Use proofreading_status_public_release instead\n\n# Load cells that have underwent proofreading for v661\nproof_df = client.materialize.tables.proofreading_status_public_release().query(\n    select_columns = ['pt_root_id','pt_position','status_dendrite','status_axon'],\n    materialization_version=661)\n\n# This gives you the list of cells with axonal proofreading, anywhere in the dataset\nproof_df.head()\n\nTable Owner Notice on proofreading_status_public_release: NOTE: this table is deprecated and no longer receiving updates; please use 'proofreading_status_and_strategy' which is available in datastack version &gt;= 1078 (datastack = minnie65_public or minnie65_phase3_v1).\n\n\n\n\n\n\n\n\n\npt_root_id\nstatus_dendrite\nstatus_axon\npt_position\n\n\n\n\n0\n864691134884807418\nextended\nextended\n[299067, 123129, 22993]\n\n\n1\n864691134885430010\nextended\nnon\n[181280, 223040, 21399]\n\n\n2\n864691134885645050\nextended\nnon\n[172288, 222528, 21607]\n\n\n3\n864691134918370314\nclean\nclean\n[170528, 226848, 20316]\n\n\n4\n864691134918461194\nclean\nclean\n[189760, 127520, 20540]\n\n\n\n\n\n\n\n\n\n\nThis will let you create skeletons for more recently proofread cells that do not exist in the publicly pregenerated .swc files. However, these skeletons will come without apical dendrite labels\npcg-skel documentation\n\n# !pip install pcg-skel\n# !pip install cloud-volume\n\n\nimport cloudvolume\nimport pcg_skel\n\n# specify the materialization version\nclient.materialize.version = 1078\n\n# initialize cloudvolume client\ncv_minnie = cloudvolume.CloudVolume(client.info.segmentation_source(), use_https=True)\n\n\n# pcg-skel pipeline code\ninput_id = 864691135639556411\nid_is_nuc = False\n\n# Cell identification meta\nsynapse_table = client.info.get_datastack_info()['synapse_table']\nif id_is_nuc:\n    id_col = 'id'\nelse:\n    id_col = 'pt_root_id'\nuse_view = True\nif use_view:\n    row = client.materialize.query_view('nucleus_detection_lookup_v1', \n                filter_equal_dict = {id_col: input_id})\nelse:\n    row = client.materialize.query_table('nucleus_detection_v0', \n                filter_equal_dict = {id_col: input_id})\n    \n    row = row.drop('created', axis = 1)\n\nnuc_id = int(row['id']) \nroot_id = int(row['pt_root_id'])\nroot_point = row['pt_position'].values[0]\n\nprint(f'starting on body {nuc_id}, {root_id}')\n\n# create whole neuron with radius info\nresample_spacing = 1510\ncollapse_soma = True\ncollapse_radius = 10_000\nres = [4, 4, 40]\nnrn =  pcg_skel.coord_space_meshwork(root_id,\n                                     client=client,\n                                     root_point=root_point,\n                                     root_point_resolution=res,\n                                     collapse_soma=collapse_soma,\n                                     collapse_radius=collapse_radius,\n                                     synapses='all',\n                                     synapse_table=synapse_table,\n                                     cv = cv_minnie)\n\n# add radius properties df to annotations \npcg_skel.features.add_volumetric_properties(nrn, client)\nprint('adding segment properties')\npcg_skel.features.add_segment_properties(nrn)\nprint('segment properties added')\n\nstarting on body 267207, 864691135639556411\nadding segment properties\nsegment properties added\n\n\n\nf, ax = plt.subplots(figsize=(7, 10))\n\nskelplot.plot_tools.plot_verts(nrn.skeleton.vertices, \n                               nrn.skeleton.edges, \n                               nrn.skeleton.radius,\n                               plot_soma=True,\n                               soma_node=nrn.skeleton.root,\n                               x=\"z\",\n                               y=\"y\")\n\nax.spines['right'].set_visible(False) \nax.spines['left'].set_visible(False) \nax.spines['top'].set_visible(False) \nax.spines['bottom'].set_visible(False)\nplt.gca().invert_yaxis()\n\n\n\n\n\n\n\n\n\n\n\n# create compartment label\ncompartment_labels = np.zeros(len(nrn.skeleton.vertices)).astype(int)\n\n# add soma label\ncompartment_labels[int(nrn.skeleton.root)]=1\n\n# get the mesh volume properties\nvolume_df = nrn.anno.segment_properties.df\n\n# add column indicating skel index \nvolume_df['skel_index'] = nrn.anno.segment_properties.mesh_index.to_skel_index_padded\nsk_volume_df = volume_df.drop_duplicates('skel_index').sort_values('skel_index').reset_index()\n\n# set map for skel index -&gt; radius\nradius_labels = np.array(sk_volume_df['r_eff']) / 1000\n\n# metadata dictionary for keeping vertex variables together\nskeleton_properties = {}\nskeleton_properties['compartment'] = compartment_labels\nskeleton_properties['vertices'] = nrn.skeleton.vertices\nskeleton_properties['edges'] = nrn.skeleton.edges\nskeleton_properties['radius'] = radius_labels\n\n\nnrn.skeleton.export_to_swc(filename = 'test.swc', \n                           node_labels=compartment_labels, \n                           radius=radius_labels,\n                           avoid_root=True, \n                           resample_spacing = resample_spacing)"
  },
  {
    "objectID": "examples/skeleton_load_and_generate.html#optional-generate-myelin-labels-from-myelin-table",
    "href": "examples/skeleton_load_and_generate.html#optional-generate-myelin-labels-from-myelin-table",
    "title": "Neuron Skeletons",
    "section": "Optional: generate myelin labels from myelin table",
    "text": "Optional: generate myelin labels from myelin table\nFor the subset of neurons with manual labeling of myelination, see CAVE table and documentaiton for vortex_manual_myelination_v0\n\n# Skeleton utility functions\nfrom tqdm.notebook import tqdm, trange\ntqdm.pandas()\ndef get_myelin_at_vertex(mw, myelin_df, properties_dict, client, cv):\n    # given a df of all myelinated points on the axon, return the corresponding skeleton labels to the properties dictionary\n\n    # get level2 nodes at myelinated positions\n    myelin_lvl2 = myelin_df.progress_apply(pd_get_level2_point, axis=1, client=client, cv=cv)\n\n    # generate index lookup\n    mw_index_lookup = get_meshwork_index_lookup(mw)\n\n    # merge myelin_lvl2 to index lookup\n    myelin_merge = pd.merge(myelin_lvl2, mw_index_lookup, on='lvl2_id', how='inner')\n    \n    # empty myelin labels\n    try:\n        vertices = properties_dict['vertices']\n    except:\n        print('no vertex properties found; call get_skeleton_features_from_meshwork() first') \n        \n    myelin = np.zeros(len(vertices))\n    \n    # where myelin is present, set to 1\n    myelin[myelin_merge.skel_ind.values] = 1\n\n    properties_dict['myelin'] = myelin\n\n    return properties_dict\n\ndef pd_get_level2_point(row, client, cv, voxel_resolution=[4,4,40]):\n    point, root_id = row[['pt_position','valid_id']]\n\n    try:\n        lvl2_id = pcg_skel.chunk_tools.get_closest_lvl2_chunk(point,\n                                                              root_id,\n                                                              client,\n                                                              voxel_resolution=voxel_resolution,\n                                                              radius=200)\n        row['lvl2_id'] = lvl2_id\n    except:\n        row['lvl2_id'] = np.nan\n        \n    return row\n\ndef get_meshwork_index_lookup(mw):\n    # generate index lookup with lvl2, mesh, and skeleton indices\n    mw_index_lookup = mw.anno.lvl2_ids.df\n    mw_index_lookup['skel_ind'] = mw.mesh_indices.to_skel_index_padded\n    mw_index_lookup.set_index('mesh_ind_filt', inplace=True)\n\n    return mw_index_lookup\n\n\n# get myelin-labeled points (exists for a subset of manually annotated cells)\nmyelin_df = client.materialize.tables.vortex_manual_myelination_v0(valid_id=input_id, tag='t').query(\n    select_columns=['valid_id','tag','pt_position'],\n    materialization_version=1078,\n)\n\nprint(len(myelin_df))\n\nTable Owner Notice on vortex_manual_myelination_v0: Myelination status assessed for the axon of the VALID_ID, not the pt_root_id.\n\n\n197\n\n\n\n# Add myelin info (this takes upwards of 10 minutes to convert points to vertices)\nskeleton_properties = get_myelin_at_vertex(nrn, myelin_df, skeleton_properties, client=client, cv=cv_minnie)\n\n\n\n\n\nprint(skeleton_properties)\n\n{'compartment': array([0, 0, 0, ..., 0, 0, 1]), 'vertices': array([[ 395328.,  547312.,  924200.],\n       [ 396992.,  546184.,  923360.],\n       [ 397712.,  545840.,  922760.],\n       ...,\n       [1313160.,  846424.,  797800.],\n       [1314944.,  847008.,  792760.],\n       [ 717824.,  771840.,  863120.]]), 'edges': array([[ 842,  866],\n       [ 866,  867],\n       [ 867,  868],\n       ...,\n       [4602, 4595],\n       [4595, 4596],\n       [4596, 4597]], dtype=int64), 'radius': array([0.12334736, 0.12334736, 0.12334736, ..., 0.13528796, 0.13528796,\n       5.54981555]), 'myelin': array([0., 0., 0., ..., 0., 0., 0.])}"
  },
  {
    "objectID": "examples/skeleton_load_and_generate.html#addendum-useful-chunkedgraph-functions-getting-segments-across-time",
    "href": "examples/skeleton_load_and_generate.html#addendum-useful-chunkedgraph-functions-getting-segments-across-time",
    "title": "Neuron Skeletons",
    "section": "Addendum: useful chunkedgraph functions (getting segments across time)",
    "text": "Addendum: useful chunkedgraph functions (getting segments across time)\nBecause the segment id may have changed since the skeletons were calculated, find the past segment id of this object\nChunkedgraph documentation and functions\n\nclient.materialize.get_versions()\n\n[1078, 117, 661, 343, 795, 943]\n\n\n\n# Get info on version 661\nclient.materialize.get_versions_metadata()[2]\n\n{'datastack': 'minnie65_public',\n 'is_merged': False,\n 'status': 'AVAILABLE',\n 'version': 661,\n 'id': 688,\n 'valid': True,\n 'time_stamp': datetime.datetime(2023, 4, 6, 20, 17, 9, 199182, tzinfo=datetime.timezone.utc),\n 'expires_on': datetime.datetime(2125, 3, 27, 19, 17, 9, 199182, tzinfo=datetime.timezone.utc)}\n\n\n\ntimestamp_v661 = client.materialize.get_versions_metadata()[2]['time_stamp']\ntimestamp_latest = client.materialize.get_versions_metadata()[0]['time_stamp']\n\n\nclient.chunkedgraph.suggest_latest_roots(segment_id, timestamp=timestamp_latest)\n\n864691135356151759\n\n\n\nclient.chunkedgraph.get_root_timestamps(root_id_v1078)\n\narray([datetime.datetime(2024, 4, 10, 4, 7, 54, 947000, tzinfo=&lt;UTC&gt;)],\n      dtype=object)\n\n\n\nclient.chunkedgraph.is_latest_roots(864691135808631069)\n\narray([ True])"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "A 3D rendering of cells in the MICrONs dataset.\nThe Machine Intelligence from Cortical Networks (MICrONS) program aimed to close the performance gap between human analysts and automated pattern recognition systems by reverse-engineering the algorithms of the brain. This program mapped the function and connectivity of cortical circuits, using high throughput imaging technologies, with the goal of providing insights into the computational principles that underlie cortical function in order to advance the next generation of machine learning algorithms.\nMICrONS assembled the largest (multi-petabyte) extant dataset of co-registered neurophysiological and neuroanatomical data from the mammalian brain, spanning 1 mm3 and encompassing 100,000 neurons. These data include large scale electron microscopy based reconstructions of cortical circuitry from mouse visual cortex, with corresponding functional imaging data from those same neurons.\nThis multi-year project was conducted by a consortium of laboratories, led by groups at the Allen Institute for Brain Science, Princeton University, and Baylor College of Medicine, with support from a broad array of teams coordinated and funded by the IARPA MICrONS program"
  },
  {
    "objectID": "introduction.html#electron-microscopy-approach-to-connectomics",
    "href": "introduction.html#electron-microscopy-approach-to-connectomics",
    "title": "Introduction",
    "section": "Electron Microscopy approach to Connectomics",
    "text": "Electron Microscopy approach to Connectomics\nThe function of the nervous system arises out of a combination of the properties of individual neurons and the properties of how they are connected into a larger network. The central goal of connectomics is to produce complete maps of the connectivity of the nervous system with synaptic resolution and analyze them to better understand the organization, development, and function of the nervous system. Electron microscopy (EM) has been a central tool in achieving these aims for two key reasons:\n\nEM can easily image nanometer-scale structures such as synapses. This allows one to unambiguously observe chemical synapses and, if pushed to high enough resolution, gap junctions. (Note that the MICrONs dataset is not high enough resolution to unambiguously observe gap junctions).\nDense staining methods allow the complete observation of a of rich collection of cellular features, including membranes, nuclei, mitochondria, and much more. This allows the reconstruction of the complete morphology of cells."
  },
  {
    "objectID": "introduction.html#what-questions-can-be-addressed-by-connectomic-data",
    "href": "introduction.html#what-questions-can-be-addressed-by-connectomic-data",
    "title": "Introduction",
    "section": "What questions can be addressed by connectomic data?",
    "text": "What questions can be addressed by connectomic data?\nFor invertebrates like the fruit fly, the nervous system is highly stereotyped in both the composition of cells and how they connect to one another. Indeed, in the fly many cell types exist as a single pair of cells, one from the left and one from the right hemisphere. Connectomic maps from one individual can thus act as a nearly universal map of the brain and powerfully inform theoretical and experimental studies.\nIn mouse cortex, this same type of stereotypy does not apply. There is no one-to-one match between any given cell in the brain of one mouse and another, and thousands of cells in a cortical area can belong to the same cell type. Because of this, the types of questions one can ask differ, but dense reconstruction of anatomy and connectivity still can be a powerful source of insight into questions like:\n\nWhat morphological or connectivity-based cell types can be found?\nWhat are the properties of the excitatory networks in cortex and how might they relate to learning rules?\nWhat rules dictate how inhibitory cells distribute their synaptic output across target cell types?\n\nIn some datasets, such as the MICrONs dataset, calcium imaging of the same tissue was performed prior to preparing the structural EM volume. In that case, we can address additional questions like:\n\nHow do functional responses relate to the morphology of cells?\nHow do functional responses relate to the connectivity of cells?\n\nIn addition, a whole host of non-neuronal cells are also present, such as astrocytes, oligodendrocytes, microglia, and pericytes. Many people are interested in studying the relationship between these cells and neurons, and connectomic datasets can be a powerful tool to discovery this structural context."
  },
  {
    "objectID": "introduction.html#the-microns-cubic-millimeter",
    "href": "introduction.html#the-microns-cubic-millimeter",
    "title": "Introduction",
    "section": "The MICrONS Cubic Millimeter",
    "text": "The MICrONS Cubic Millimeter\n\n\n\n\n\n\nNote\n\n\n\nFor more complete details about the generation of the volume, please see “Functional connectomics spanning multiple areas of mouse visual cortex” (The MICrONS Consortium et al. 2025).\n\n\nThe MICrONs Cubic Millimeter dataset is an EM volume of mouse visual cortex that spans all cortical layers, and extends approximately 1 mm in width by 0.5 mm in depth, collected as a collaboration between the Allen Institute for Brain Science, Baylor College of Medicine, and Princeton University. The goal of this dataset was to produce a so-called “functional connectomics” volume, where the same population of neurons could be measured with both calcium imaging to record functional properties and EM to describe synapse-resolution structural properties.\nTo link structure and function, excitatory neurons were genetically labeled with GCaMPs and imaged under a battery of natural movies and parametric stimuli by the Tolias lab, then at Baylor College of Medicine. The mouse was shipped to the Allen Institute and the same cortical region was prepared for EM, with more than 25,000 sections cut from the block and imaged across a small fleet of electron microscopes. In order to capture the interactions between different visual regions, the volume location is at the edge of primary visual cortex (VISp) and extends into higher visual areas VISal and VISrl.\nImagery was then aligned and segmented by the team of Sebastian Seung at Princeton, who also automatically detected synapses and nuclei. A team of proofreaders at Johns Hopkins Applied Physics Laboratory helped correct the largest types of errors in the segmentation, leaving virtually every neuron as a distinct object, and coregister tens of thousands of neurons to the functional data. Further analysis at the Allen Institute and elsewhere has focused on curating a map of cell types across the entire dataset, and generating a database to link synapses, cell types, and more to the morphology of individual cells.\nThe result is a cubic millimeter scale dataset with a rich collection of information and the potential for a range of studies, with strengths and limitations due to the current state of proofreading and needs of the functional data collection."
  },
  {
    "objectID": "programmatic_access/ultrastructure_tutorial/em_ultrastructure.html",
    "href": "programmatic_access/ultrastructure_tutorial/em_ultrastructure.html",
    "title": "Ultrastructure",
    "section": "",
    "text": "Under construction"
  },
  {
    "objectID": "programmatic_access/ultrastructure_tutorial/em_ultrastructure.html#ultrastructure-resources",
    "href": "programmatic_access/ultrastructure_tutorial/em_ultrastructure.html#ultrastructure-resources",
    "title": "Ultrastructure",
    "section": "Ultrastructure Resources",
    "text": "Ultrastructure Resources\nElectron microscopy allows one to see very high resolution features called the “ultrastructure” of the cell, such as organelles and microtubules. However, when first seeing EM images, it can be difficult to know the biological meaning of what you are looking at.\nHere are a couple of resources with several examples of particular structures imaged with EM:\n\nSynapseWeb. This website from the Harris lab at UT Austin offers a number of annotated examples of EM images, in particular the Atlas of Ultrastructural Neurocytology.\nBrain Ultrastructure: Putting the Pieces Together. This short paper has some particularly clear pictures of less common structures like neurites undergoing autophagy.\n\nHowever, note that EM imagery can look very different when imaged with different imaging techniques, sectioning methods or staining protocols, and thus the exact appearance can vary between datasets. In addition, the same structure imaged in different planes can look different, particularly in anisotropic approaches like serial section EM where the x/y resolution is much finer than the z resolution."
  },
  {
    "objectID": "quickstart_notebooks/00_skeletons.html",
    "href": "quickstart_notebooks/00_skeletons.html",
    "title": "Skeletons",
    "section": "",
    "text": "Under construction\n\n\n\nTo add interactive visualization and code outputs"
  },
  {
    "objectID": "quickstart_notebooks/00_skeletons.html#working-with-meshwork-files",
    "href": "quickstart_notebooks/00_skeletons.html#working-with-meshwork-files",
    "title": "Skeletons",
    "section": "Working with Meshwork Files",
    "text": "Working with Meshwork Files\nLoading a meshwork file imports the level 2 graph (the “mesh”), the skeleton, and a collection of associated annotations.\n\nfrom meshparty import meshwork\nnrn = meshwork.load_meshwork(mesh_filename)\n\nThe main three properties of the meshwork object are:\n\nnrn.mesh : The l2graph representation of the reconstruction.\nnrn.skeleton : The skeleton representation of the reconstruction.\nnrn.anno : A table of annotation dataframes and associated metadata that links them to specific vertices in the mesh and skeleton."
  },
  {
    "objectID": "quickstart_notebooks/00_skeletons.html#meshwork-nrn.mesh-vs-nrn.skeleton",
    "href": "quickstart_notebooks/00_skeletons.html#meshwork-nrn.mesh-vs-nrn.skeleton",
    "title": "Skeletons",
    "section": "Meshwork nrn.mesh vs nrn.skeleton",
    "text": "Meshwork nrn.mesh vs nrn.skeleton\nSkeletons are “tree-like”, where every vertex (except the root vertex) has a single parent that is closer to the root than it, and any number of child vertices. Because of this, for a skeleton there are well-defined directions “away from root” and “towards root” and few types of vertices have special names:\n\nBranch point: vertices with two or more children, where a neuronal process splits.\nEnd point: vertices with no childen, where a neuronal process ends.\nRoot point: The one vertex with no parent node. By convention, we typically set the root vertex at the cell body, so these are equivalent to “away from soma” and “towards soma”.\nSegment: A collection of vertices along an unbranched region, between one branch point and the next end point or branch point downstream.\n\nMeshes are arbitrary collections of vertices and edges, but do not have a notion of “parent” or “child” “branch point” or “end point”. Here, this means the “mesh” used here includes a vertex for every level 2 chunk, even where it is thick like at a cell body or very thick dendrite. However, by default this means that there is not always a well-defined notion of parent or child nodes, or towards or away from root.\nIn contrast “Meshes” (really, graphs of connected vertices) do not have a unique “inward” and “outward” direction. For the sake of rapid skeletonization, the “meshes” we use here are really the graph of level 2 vertices as described above. These aren’t a mesh in the visualization sense of the section on downloading Meshes, but have the same data representation.\nTo handle this, the meshwork object associates each mesh vertices with a single nearby skeleton vertex, and each skeleton vertex is associated with one or more mesh vertices. By representing data this way, annotations like synapses can be directly associated with a mesh vertex (because synapses can be anywhere on the object) and then mapped to the skeleton in order to enjoy the topological benefits of the skeleton representation.\n\n# By the definition of skeleton vs mesh, we would expect that mesh contains more vertices than the skeleton. \n# We can see this by looking at the size of the skeleton vertex location array vs the size of the mesh vertex location array.\n\nprint('Skeleton vertices array length:', len(nrn.skeleton.vertices))\nprint('Mesh vertices array length:', len(nrn.mesh.vertices))\n\n\n# Let us try to visualize the skeleton:\n# Visualize the whole skeleton \n\n# here's a simple way to plot vertices of the skeleton\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\n%matplotlib notebook \n\nfig = plt.figure(figsize=(6, 6))\nax = plt.axes(projection='3d')\nax.scatter3D(nrn.skeleton.vertices[:,0], nrn.skeleton.vertices[:,1], nrn.skeleton.vertices[:,2], s=1)\n\n\n\n\nScatterplot of skeleton vertices as a point cloud"
  },
  {
    "objectID": "quickstart_notebooks/00_skeletons.html#skeleton-properties-and-methods",
    "href": "quickstart_notebooks/00_skeletons.html#skeleton-properties-and-methods",
    "title": "Skeletons",
    "section": "Skeleton Properties and Methods",
    "text": "Skeleton Properties and Methods\nTo plot this skeleton in a more sophisticated way, you have to start thinking of it as a graph, and the meshwork object has a bunch of tools and properties to help you utilize the skeleton graph.\nLet’s list some of the most useful ones below You access each of these with nrn.skeleton.* Use the ? to read more details about each one\nProperties\n\n`branch_points``: a list of skeleton vertices which are branches\nroot: the skeleton vertice which is the soma\ndistance_to_root: an array the length of vertices which tells you how far away from the root each vertex is\nroot_position: the position of the root node in nanometers\nend_points: the tips of the neuron\ncover_paths: a list of arrays containing vertex indices that describe individual paths that in total cover the neuron without repeating a vertex. Each path starts at an end point and continues toward root, stopping once it gets to a vertex already listed in a previously defined path. Paths are ordered to start with the end points farthest from root first. Each skeleton vertex appears in exactly one cover path.\ncsgraph: a scipy.sparse.csr.csr_matrix containing a graph representation of the skeleton. Useful to do more advanced graph operations and algorithms. https://docs.scipy.org/doc/scipy/reference/sparse.csgraph.html\nkdtree: a scipy.spatial.ckdtree.cKDTree containing the vertices of skeleton as a kdtree. Useful for quickly finding points that are nearby. https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.cKDTree.html\n\nMethods\n\npath_length(paths=None): the path length of the whole neuron if no arguments, or pass a list of paths to get the path length of that. A path is just a list of vertices which are connected by edges.\npath_to_root(vertex_index): returns the path to the root from the passed vertex\npath_between(source_index, target_index): the shortest path between the source vertex index and the target vertex index\nchild_nodes(vertex_indices): a list of arrays listing the children of the vertex indices passed in\nparent_nodes(vertex_indices): an array listing the parent of the vertex indices passed in\n\n\n# A better way to plot a neuron is to use cover_paths\n# and plot those as 3d lines\ndef plot_neuron_skeleton(neuron, ax, c='b', linewidth=1):\n\n    for cover_path in neuron.skeleton.cover_paths:\n        path_verts = neuron.skeleton.vertices[cover_path,:]\n        ax.plot(path_verts[:,0], path_verts[:,1], path_verts[:,2], c=c, linewidth=linewidth)\n\nfig = plt.figure(figsize=(6, 6))\nax = plt.axes(projection='3d')\nplot_neuron_skeleton(nrn, ax)"
  },
  {
    "objectID": "quickstart_notebooks/00_skeletons.html#indexing-and-selecting-sets-of-points",
    "href": "quickstart_notebooks/00_skeletons.html#indexing-and-selecting-sets-of-points",
    "title": "Skeletons",
    "section": "Indexing And Selecting Sets of Points",
    "text": "Indexing And Selecting Sets of Points\nThe meshworks object contains a mesh with lots of vertices and a skeleton which holds a subset of these vertices. Therefore, in python these points have different “indices” in the mesh and skeleton. For example, if the mesh contains 10000 vertices, the indexing of those would run from 0 - 9999. The skeleton, which contains a subset of 100 of these would have indexing from 0-99. How would you figure out which of the mesh vertices these correspond to?\nLuckily, we have some really nifty functions that help us distinguish those:\nLet us first look at some attributes in the meshworks objects:\nA few nifty function for subselecting points: downstream points and `path_between``. For a given point, downstream points are defined as points on paths from endpoints to the root which are further than the given point. For example, if the skeleton path is : A-B-C-D-E where A is the root, D and E are downstream points of C. With branching, this can be more complex. To find the downstream points from say the 9th branch point, we can do:\n\n# Downstream points\nnrn.downstream_of(nrn.branch_points[9])\n\npath_between returns the vertices from one point to another. For example, we can get the every mesh vertex from the end point 5 to the root point. As a quick visualization, we can look at the distance to root along the, showing that it is descreasing.\n\nfig, ax = plt.subplots()\nax.plot(\n    nrn.distance_to_root(\n        nrn.path_between(\n            nrn.end_points[5],\n            nrn.root,\n        )\n    ) / 1_000,\n)\n\nax.set_ylabel('Distance from root ($\\mu m$)')\nax.set_ylabel('Vertex along path')\n\n\n\n\nDistance to root along graph path indicated above. Note that this flattens out at zero because “distance” is computed along the skeleton and many graph vertices in the level 2 graph are associated with the soma vertex of the skeleton"
  },
  {
    "objectID": "quickstart_notebooks/00_skeletons.html#masking",
    "href": "quickstart_notebooks/00_skeletons.html#masking",
    "title": "Skeletons",
    "section": "Masking",
    "text": "Masking\nJust like meshes, we can mask the meshwork object. Like all basic meshwork functions, the expected input is in mesh vertices. Importantly, doing so will be synchronized across the mesh, the skeleton, and annotations.\n\n# Let's now use masking to highlight one particular cover path.\n\nfig = plt.figure(figsize=(6, 6))\nax = plt.axes(projection=\"3d\")\n\nplot_neuron_skeleton(nrn, ax, \"b\")\n\nnrn.reset_mask()  # This just makes sure we are working with the same baseline object.\n\n# This will make a mask with only the vertices in the first cover path.\nwith neuron.mask_context(nrn.skeleton.cover_paths[0].to_mesh_mask) as nrnf:\n  plot_neuron_skeleton(nrnf, ax, \"r\", linewidth=2)\n  ax.scatter(\n      nrnf.skeleton.root_position[0],\n      nrnf.skeleton.root_position[1],\n      nrnf.skeleton.root_position[2],\n  )\n\nWhile mask_context acts to mask the skeleton and then unmasks it at the end of operations, you can also just mask a skeleton and let it stay that way. In that case, use the nrn.apply_mask function.\n\n\n\n\n\n\nWarning\n\n\n\nDo not use the nrn.mesh.apply_mask or nrn.skeleton.apply_mask functions, which will not synchronize the mask across the mesh, skeleton, and annotations."
  },
  {
    "objectID": "quickstart_notebooks/00_skeletons.html#annotations",
    "href": "quickstart_notebooks/00_skeletons.html#annotations",
    "title": "Skeletons",
    "section": "Annotations",
    "text": "Annotations\nnrn.anno has set of annotation tables containing some additional information for analysis. Each annotation table has both a Pandas DataFrame object storing data and additional information that allow the rows of the DataFrame to be mapped to mesh and skeleton vertices. For the neurons that have been pre-computed, there is a consisent set of annotation tables:\n\npost_syn: Postsynaptic sites (inputs) for the cell\npre_syn: Presynaptic sites (outputs) for the cell\nis_axon: List of vertices that have been labeled as part of the axon\nlvl2_ids: Gives the PCG level 2 id for each mesh vertex, largely for book-keeping reasons.\nsegment_properties: For each vertex, information about the approximate radius, surface area, volume, and length of the segment it is on.\nvol_prop: For every vertex, information about the volume and surface area of the level 2 id it is associated with.\n\nTo access one of the DataFrames, use the name of the table as an attribute of the anno object and then get the .df property. For example, to get the postsynaptic sites, we can do:\n\nfrom meshparty import meshwork\nnrn = meshwork.load_meshwork('data/864691134940133219_skel.h5')\nnrn.anno.post_syn.df.head()\n\nHowever, in addition to these rows, you can also easily get the mesh vertex index or skeleton vertex index that a row corresponds to with nrn.anno.post_syn.mesh_index or nrn.anno.post_syn.skel_index respectively. This seems small, but it allows you to integrate skeleton-like measurements with annotations trivially.\nFor example, to get the distance from the cell body for each postsynaptic site, we can do:\n\nnrn.distance_to_root(nrn.anno.post_syn.mesh_index)\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the nrn.distance_to_root, like all basic meshwork object functions, expects a mesh vertex index rather than a skeleton vertex index.\n\n\nA common pattern is to copy a synapse dataframe and then add columns to it. For example, to add the distance to root to the postsynaptic sites, we can do:\n\nsyn_df = nrn.anno.pre_syn.df.copy()\nsyn_df['dist_to_root'] = nrn.distance_to_root(nrn.anno.pre_syn.mesh_index)\n\n\nFiltering Annotations by Skeleton Arbor\nEach annotation table has a ‘filter_query’ method that takes a boolean mesh mask and returns only those rows of the dataframe associated with those particular locations on the mesh.\nLet’s use what we learned above in two examples: first, getting all input synapses within 50 microns of the root and second, getting all input synapses on one particular branch off of the soma.\n\ndtr = nrn.distance_to_root() / 1_000   # Convert from nanometers to microns\nnrn.anno.post_syn.filter_query( dtr &lt; 50).df\n\nWe can also use one set of annotations as an input to filter query from another set of annotations. For example, due to errors in segmentation or mistakes in synapse detection, there can be synaptic outputs on the dendrite. However, if we have an is_axon annotation that simply contains a collection of vertices that correspond to the cell’s axon.\nWe can use this annotation to create a mask and filter out all of the synapses that are not on the axon.\n\naxon_mask = nrn.anno.is_axon.mesh_mask\nnrn.anno.pre_syn.filter_query(~axon_mask).df # The \"~\" is a logical not operation that flips True and False\n\nAs a sanity check, we can use nglui to see if these synapses we have labeled as being on the axon are all where we expect.\n\nimport os\nfrom caveclient import CAVEclient\nfrom nglui.statebuilder.helpers import make_synapse_neuroglancer_link\n\nclient = CAVEclient('minnie65_public')\n\nmake_synapse_neuroglancer_link(\n    nrn.anno.pre_syn.filter_query(axon_mask).df,\n    client,\n    return_as=\"html\"\n)\n\n(Click one of the synapse annotations to load the neuron mesh).\nAnother common example might be to pick one of the child nodes of the soma and get all of the synapses on that branch. We can do this by using the nrn.skeleton.get_child_nodes function to get the skeleton vertex indices of the child nodes and then use that to filter the synapses.\n\nbranch_index = 0 # Let's just use the first child vertex of the root node, which is at the soma by default.\nbranch_inds = nrn.downstream_of(nrn.child_index(nrn.root)[branch_index])\nbranch_mask = branch_inds.to_mesh_mask\n\nmake_synapse_neuroglancer_link(\n    nrn.anno.post_syn.filter_query(branch_mask).df,\n    client,\n    return_as=\"html\"\n)"
  },
  {
    "objectID": "quickstart_notebooks/02-cave-query-cell-types.html",
    "href": "quickstart_notebooks/02-cave-query-cell-types.html",
    "title": "CAVE Query: Cell Types",
    "section": "",
    "text": "The Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Cell Types"
    ]
  },
  {
    "objectID": "quickstart_notebooks/02-cave-query-cell-types.html#caveclient-setup",
    "href": "quickstart_notebooks/02-cave-query-cell-types.html#caveclient-setup",
    "title": "CAVE Query: Cell Types",
    "section": "CAVEclient Setup",
    "text": "CAVEclient Setup\nMost programmatic access to the CAVE services occurs through CAVEclient, a Python client to access various types of data from the online services.\nFull documentation for CAVEclient is available here.\nTo initialize a caveclient, we give it a datastack, which is a name that defines a particular combination of imagery, segmentation, and annotation database. For the MICrONs public data, we use the datastack name minnie65_public.\n\nfrom caveclient import CAVEclient\ndatastack_name = 'minnie65_public'\nclient = CAVEclient(datastack_name)\n\n# Show the description of the datastack\nclient.info.get_datastack_info()['description']\n\n'This is the publicly released version of the minnie65 volume and segmentation. '\n\n\n\nMaterialization versions\nData in CAVE is timestamped and periodically versioned - each (materialization) version corresponds to a specific timestamp. Individual versions are made publicly available. The materialization service provides annotation queries to the dataset. It is available under client.materialize.\nPeriodic updates are made to the public datastack, which will include updates to the available tables. Some cells will have different pt_root_id because they have undergone proofreading.\nIt is worth checking the version of the data you are using, and specifying the version for analysis consistency.\n\n# see the available materialization versions\nclient.materialize.get_versions()\n\n[1300, 1078, 117, 661, 343, 1181, 795, 943]\n\n\nAnd these are their associated timestamps (all timestamps are in UTC):\n\nfor version in client.materialize.get_versions():\n    print(f\"Version {version}: {client.materialize.get_timestamp(version)}\")\n\nVersion 1300: 2025-01-13 10:10:01.286229+00:00\nVersion 1078: 2024-06-05 10:10:01.203215+00:00\nVersion 117: 2021-06-11 08:10:00.215114+00:00\nVersion 661: 2023-04-06 20:17:09.199182+00:00\nVersion 343: 2022-02-24 08:10:00.184668+00:00\nVersion 1181: 2024-09-16 10:10:01.121167+00:00\nVersion 795: 2023-08-23 08:10:01.404268+00:00\nVersion 943: 2024-01-22 08:10:01.497934+00:00\n\n\n\n# set materialization version, for consistency\nmaterialization = 1300 # current public as of 1/13/2025\nclient.version = materialization",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Cell Types"
    ]
  },
  {
    "objectID": "quickstart_notebooks/02-cave-query-cell-types.html#caveclient-basics",
    "href": "quickstart_notebooks/02-cave-query-cell-types.html#caveclient-basics",
    "title": "CAVE Query: Cell Types",
    "section": "CAVEclient Basics",
    "text": "CAVEclient Basics\nThe most frequent use of the CAVEclient is to query the database for annotations like synapses. All database functions are under the client.materialize property. To see what tables are available, use the get_tables function:\n\nclient.materialize.get_tables()\n\n['baylor_gnn_cell_type_fine_model_v2',\n 'nucleus_alternative_points',\n 'allen_column_mtypes_v2',\n 'bodor_pt_cells',\n 'aibs_metamodel_mtypes_v661_v2',\n 'allen_v1_column_types_slanted_ref',\n 'aibs_column_nonneuronal_ref',\n 'nucleus_ref_neuron_svm',\n 'apl_functional_coreg_vess_fwd',\n 'vortex_compartment_targets',\n 'baylor_log_reg_cell_type_coarse_v1',\n 'functional_properties_v3_bcm',\n 'gamlin_2023_mcs',\n 'l5et_column',\n 'pt_synapse_targets',\n 'coregistration_manual_v4',\n 'cg_cell_type_calls',\n 'synapses_pni_2',\n 'nucleus_detection_v0',\n 'vortex_manual_nodes_of_ranvier',\n 'vortex_astrocyte_proofreading_status',\n 'bodor_pt_target_proofread',\n 'nucleus_functional_area_assignment',\n 'coregistration_auto_phase3_fwd_apl_vess_combined_v2',\n 'synapse_target_structure',\n 'coregistration_auto_phase3_fwd_v2',\n 'gamlin_2023_mcs_met_types',\n 'vortex_manual_myelination_v0',\n 'proofreading_status_and_strategy',\n 'synapse_target_predictions_ssa',\n 'aibs_metamodel_celltypes_v661']\n\n\nFor each table, you can see the metadata describing that table. For example, let’s look at the nucleus_detection_v0 table:\n\nclient.materialize.get_table_metadata('nucleus_detection_v0')\n\n{'schema': 'nucleus_detection',\n 'aligned_volume': 'minnie65_phase3',\n 'table_name': 'nucleus_detection_v0',\n 'valid': True,\n 'created': '2020-11-02T18:56:35.530100',\n 'id': 55544,\n 'schema_type': 'nucleus_detection',\n 'user_id': '121',\n 'description': 'A table of nuclei detections from a nucleus detection model developed by Shang Mu, Leila Elabbady, Gayathri Mahalingam and Forrest Collman. Pt is the centroid of the nucleus detection. id corresponds to the flat_segmentation_source segmentID. Only included nucleus detections of volume&gt;25 um^3, below which detections are false positives, though some false positives above that threshold remain. ',\n 'notice_text': None,\n 'reference_table': None,\n 'flat_segmentation_source': 'precomputed://https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/nuclei',\n 'write_permission': 'PRIVATE',\n 'read_permission': 'PUBLIC',\n 'last_modified': '2022-10-25T19:24:28.559914',\n 'segmentation_source': '',\n 'pcg_table_name': 'minnie3_v1',\n 'last_updated': '2025-03-27T01:00:00.156007',\n 'voxel_resolution': [4.0, 4.0, 40.0]}\n\n\nYou get a dictionary of values. Two fields are particularly important: the description, which offers a text description of the contents of the table and voxel_resolution which defines how the coordinates in the table are defined, in nm/voxel.\n\n\n\n\n\n\nAnnotation tables\n\n\n\nYou can also find a semantic description of the most commonly used tables at the Annotation Tables page.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Cell Types"
    ]
  },
  {
    "objectID": "quickstart_notebooks/02-cave-query-cell-types.html#querying-tables",
    "href": "quickstart_notebooks/02-cave-query-cell-types.html#querying-tables",
    "title": "CAVE Query: Cell Types",
    "section": "Querying Tables",
    "text": "Querying Tables\nTo get the contents of a table, use the query_table function. This will return the whole contents of a table without any filtering, up to for a maximum limit of 200,000 rows. The table is returned as a Pandas DataFrame and you can immediately use standard Pandas function on it.\n\ncell_type_df = client.materialize.query_table('nucleus_detection_v0')\ncell_type_df.head()\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n730537\n2020-09-28 22:40:41.780734+00:00\nNaN\nt\n32.307937\n0\n0\n[381312, 273984, 19993]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n373879\n2020-09-28 22:40:41.781788+00:00\nNaN\nt\n229.045043\n96218056992431305\n864691136090135607\n[228816, 239776, 19593]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n601340\n2020-09-28 22:40:41.782714+00:00\nNaN\nt\n426.138010\n0\n0\n[340000, 279152, 20946]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n201858\n2020-09-28 22:40:41.783784+00:00\nNaN\nt\n93.753836\n84955554103121097\n864691135373893678\n[146848, 213600, 26267]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n600774\n2020-09-28 22:40:41.785273+00:00\nNaN\nt\n135.189791\n0\n0\n[339120, 276112, 19442]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nWhile most tables are small enough to be returned in full, the synapse table has hundreds of millions of rows and is too large to download this way\n\n\nTables have a collection of columns, some of which specify point in space (columns ending in _position), some a root id (ending in _root_id), and others that contain other information about the object at that point. Before describing some of the most important tables in the database, it’s useful to know about a few advanced options that apply when querying any table.\n\ndesired_resolution : This parameter allows you to convert the columns specifying spatial points to different resolutions. Many tables are stored at a resolution of 4x4x40 nm/voxel, for example, but you can convert to nanometers by setting desired_resolution=[1,1,1].\nsplit_positions : This parameter allows you to split the columns specifying spatial points into separate columns for each dimension. The new column names will be the original column name with _x, _y, and _z appended.\nselect_columns : This parameter allows you to get only a subset of columns from the table. Once you know exactly what you want, this can save you some cleanup.\nlimit : This parameter allows you to limit the number of rows returned. If you are just testing out a query or trying to inspect the kind of data within a table, you can set this to a small number to make sure it works before downloading the whole table. Note that this will show a warning so that you don’t accidentally limit your query when you don’t mean to.\n\nFor example, using all of these together:\n\ncell_type_df = client.materialize.query_table('nucleus_detection_v0', split_positions=True, desired_resolution=[1,1,1], select_columns=['pt_position', 'pt_root_id'], limit=10)\ncell_type_df\n\n\n\n\n\n\n\n\npt_position_x\npt_position_y\npt_position_z\npt_root_id\n\n\n\n\n0\n241856.0\n374464.0\n838720.0\n0\n\n\n1\n227200.0\n389120.0\n797160.0\n0\n\n\n2\n230144.0\n422336.0\n795320.0\n0\n\n\n3\n239488.0\n386432.0\n794120.0\n0\n\n\n4\n239744.0\n423488.0\n803120.0\n864691136050815731\n\n\n5\n245888.0\n384512.0\n800120.0\n0\n\n\n6\n249792.0\n391680.0\n807080.0\n0\n\n\n7\n243328.0\n403008.0\n794280.0\n0\n\n\n8\n247872.0\n386816.0\n805320.0\n0\n\n\n9\n260352.0\n416640.0\n802360.0\n864691135013273238",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Cell Types"
    ]
  },
  {
    "objectID": "quickstart_notebooks/02-cave-query-cell-types.html#filtering-queries",
    "href": "quickstart_notebooks/02-cave-query-cell-types.html#filtering-queries",
    "title": "CAVE Query: Cell Types",
    "section": "Filtering Queries",
    "text": "Filtering Queries\nFiltering tables so that you only get data about certain rows back is a very common operation. While there are filtering options in the query_table function (see documentation for more details), a more unified filter interface is available through a “table manager” interface.\nRather than passing a table name to the query_table function, client.materialize.tables has a subproperty for each table in the database that can be used to filter that table.\nThe general pattern for usage is\nclient.materialize.tables.{table_name}({filter options}).query({format and timestamp options})\nwhere {table_name} is the name of the table you want to filter, {filter options} is a collection of arguments for filtering the query, and {format and timestamp options} are those parameters controlling the format and timestamp of the query.\n\n\n\n\n\n\nCaution\n\n\n\nUse of this functionality will show a brief warning that the interface is experimental. This is because the interface is still being developed and may change in the near future in response to user feedback.\n\n\nFor example, let’s look at the table aibs_metamodel_celltypes_v661, which has cell type predictions across the dataset. We can get the whole table as a DataFrame:\n\ncell_type_df = client.materialize.tables.aibs_metamodel_celltypes_v661().query()\ncell_type_df.head()\n\n\n\n\n\n\n\n\nid_ref\ncreated_ref\nvalid_ref\ntarget_id\nclassification_system\ncell_type\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n36916\n2023-12-19 22:47:18.659864+00:00\nt\n336365\nexcitatory_neuron\n5P-IT\n336365\n2020-09-28 22:42:48.966292+00:00\nt\n272.488202\n93606511657924288\n864691136274724621\n[209760, 180832, 27076]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n1070\n2023-12-19 22:38:00.472115+00:00\nt\n110648\nexcitatory_neuron\n23P\n110648\n2020-09-28 22:45:09.650639+00:00\nt\n328.533443\n79385153184885329\n864691135489403194\n[106448, 129632, 25410]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n1099\n2023-12-19 22:38:00.898837+00:00\nt\n112071\nexcitatory_neuron\n23P\n112071\n2020-09-28 22:43:34.088785+00:00\nt\n272.929423\n79035988248401958\n864691136147292311\n[103696, 149472, 15583]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n13259\n2023-12-19 22:41:14.417986+00:00\nt\n197927\nnonneuron\noligo\n197927\n2020-09-28 22:43:10.652649+00:00\nt\n91.308851\n84529699506051734\n864691136050858227\n[143600, 186192, 26471]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n13271\n2023-12-19 22:41:14.685474+00:00\nt\n198087\nnonneuron\nastrocyte\n198087\n2020-09-28 22:41:36.677186+00:00\nt\n161.744978\n83756261929388963\n864691135809440972\n[137952, 190944, 27361]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\nand we can add similar formatting options as in the last section to the query function:\n\ncell_type_df = client.materialize.tables.aibs_metamodel_celltypes_v661().query(split_positions=True, desired_resolution=[1,1,1], select_columns=['pt_position', 'pt_root_id', 'cell_type'], limit=10)\ncell_type_df\n\n\n\n\n\n\n\n\ncell_type\npt_position_x\npt_position_y\npt_position_z\npt_root_id\n\n\n\n\n0\n23P\n257600.0\n487936.0\n802760.0\n864691135724233643\n\n\n1\n23P\n260992.0\n493568.0\n801560.0\n864691136436395166\n\n\n2\nNGC\n256256.0\n466432.0\n831040.0\n864691135462260637\n\n\n3\n23P\n255744.0\n480640.0\n833200.0\n864691136723556861\n\n\n4\n23P\n262144.0\n505856.0\n824880.0\n864691135776658528\n\n\n5\n23P\n257536.0\n521728.0\n804440.0\n864691135941166708\n\n\n6\n23P\n251840.0\n552896.0\n832320.0\n864691135545065768\n\n\n7\n23P\n251136.0\n546048.0\n821320.0\n864691135479369926\n\n\n8\n23P\n256000.0\n626368.0\n814000.0\n864691135697633557\n\n\n9\nastrocyte\n324096.0\n417920.0\n658880.0\n864691135937358133\n\n\n\n\n\n\n\nHowever, now we can also filter the table to get only cells that are predicted to have cell type \"BC\" (for “basket cell”).\n\nmy_cell_type = \"BC\"\nclient.materialize.tables.aibs_metamodel_celltypes_v661(cell_type=my_cell_type).query()\n\n\n\n\n\n\n\n\nid_ref\ncreated_ref\nvalid_ref\ntarget_id\nclassification_system\ncell_type\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n12051\n2023-12-19 22:40:57.133228+00:00\nt\n193846\ninhibitory_neuron\nBC\n193846\n2020-09-28 22:40:41.897904+00:00\nt\n306.148966\n82838443188669165\n864691135684976823\n[131568, 168496, 16452]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n43009\n2023-12-19 22:48:53.577191+00:00\nt\n369908\ninhibitory_neuron\nBC\n369908\n2020-09-28 22:40:41.814964+00:00\nt\n332.862751\n96002690286851358\n864691136145011252\n[227104, 207840, 20841]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n83044\n2023-12-19 22:58:50.269173+00:00\nt\n615735\ninhibitory_neuron\nBC\n615735\n2020-09-28 22:40:41.957345+00:00\nt\n314.539540\n112181247505371364\n864691136311774525\n[344880, 161104, 17084]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3\n82324\n2023-12-19 22:58:39.896999+00:00\nt\n613047\ninhibitory_neuron\nBC\n613047\n2020-09-28 22:40:41.982376+00:00\nt\n242.159780\n113234168401651200\n864691136065413528\n[352688, 141616, 25312]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n4\n48951\n2023-12-19 22:50:24.710643+00:00\nt\n402885\ninhibitory_neuron\nBC\n402885\n2020-09-28 22:40:41.994716+00:00\nt\n279.232348\n97621720621533350\n864691135645529583\n[238848, 211712, 16471]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3360\n8968\n2023-12-19 22:40:09.246333+00:00\nt\n170777\ninhibitory_neuron\nBC\n170777\n2020-09-28 22:45:25.310708+00:00\nt\n499.103662\n81230957054577082\n864691135065994564\n[119600, 250560, 15373]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3361\n79472\n2023-12-19 22:57:53.993099+00:00\nt\n591219\ninhibitory_neuron\nBC\n591219\n2020-09-28 22:45:25.526753+00:00\nt\n567.517839\n110216764830845707\n864691135279126177\n[330320, 204752, 25060]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3362\n15548\n2023-12-19 22:41:48.382554+00:00\nt\n208056\ninhibitory_neuron\nBC\n208056\n2020-09-28 22:45:25.401800+00:00\nt\n521.621668\n84540007091735344\n864691135801456226\n[143472, 262944, 23693]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3363\n55791\n2023-12-19 22:52:02.582669+00:00\nt\n438586\ninhibitory_neuron\nBC\n438586\n2020-09-28 22:45:25.430745+00:00\nt\n529.501389\n99807894274485381\n864691135395662581\n[254912, 247440, 23680]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n3364\n50504\n2023-12-19 22:50:48.576826+00:00\nt\n419363\ninhibitory_neuron\nBC\n419363\n2020-09-28 22:45:25.436862+00:00\nt\n530.642698\n99716496901116512\n864691136691390838\n[254416, 90336, 20469]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n3365 rows × 15 columns\n\n\n\nor maybe we just want the cell types for a particular collection of root ids:\n\nmy_root_ids = [864691135771677771, 864691135560505569, 864691136723556861]\nclient.materialize.tables.aibs_metamodel_celltypes_v661(pt_root_id=my_root_ids).query()\n\n\n\n\n\n\n\n\nid_ref\ncreated_ref\nvalid_ref\ntarget_id\nclassification_system\ncell_type\nid\ncreated\nvalid\nvolume\npt_supervoxel_id\npt_root_id\npt_position\nbb_start_position\nbb_end_position\n\n\n\n\n0\n11282\n2023-12-19 22:40:43.249642+00:00\nt\n19116\nexcitatory_neuron\n23P\n19116\n2020-09-28 22:41:51.767906+00:00\nt\n301.426115\n74737997899501359\n864691135771677771\n[72576, 108656, 20291]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n1\n15681\n2023-12-19 22:41:50.365399+00:00\nt\n21783\nexcitatory_neuron\n23P\n21783\n2020-09-28 22:41:59.966574+00:00\nt\n263.637074\n75795590176519004\n864691135560505569\n[80128, 124000, 16563]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n2\n50080\n2023-12-19 22:50:42.474168+00:00\nt\n4074\nexcitatory_neuron\n23P\n4074\n2020-09-28 22:42:41.341179+00:00\nt\n313.678234\n73543309863605007\n864691136723556861\n[63936, 120160, 20830]\n[nan, nan, nan]\n[nan, nan, nan]\n\n\n\n\n\n\n\nYou can get a list of all parameters than be used for querying with the standard IPython/Jupyter docstring functionality, e.g. client.materialize.tables.aibs_metamodel_celltypes_v661.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Cell Types"
    ]
  },
  {
    "objectID": "quickstart_notebooks/04-cave-query-synapses.html",
    "href": "quickstart_notebooks/04-cave-query-synapses.html",
    "title": "CAVE Query: Synaptic Connectivity",
    "section": "",
    "text": "The Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data.\nThe connectome data (synapses, cell types, etc.) can be accessed from the cloud via CAVE. However, because of the size of the connectivity tables, it is often preferable to download and compile the features of interest (in this case synapses) to work with offline. This notebook steps through downloading the synapses of the proofread neurons, as of materialization version 1181.\nfrom caveclient import CAVEclient\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Synaptic Connectivity"
    ]
  },
  {
    "objectID": "quickstart_notebooks/04-cave-query-synapses.html#initialize-caveclient-with-a-datastack",
    "href": "quickstart_notebooks/04-cave-query-synapses.html#initialize-caveclient-with-a-datastack",
    "title": "CAVE Query: Synaptic Connectivity",
    "section": "Initialize CAVEclient with a datastack",
    "text": "Initialize CAVEclient with a datastack\nDatasets in CAVE are organized as datastacks. These are a combination of an EM dataset, a segmentation and a set of annotations. The datastack for MICrONS public release is minnie65_public. When you instantiate your client with this datastack, it loads all relevant information to access it.\n\nclient = CAVEclient(\"minnie65_public\")",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Synaptic Connectivity"
    ]
  },
  {
    "objectID": "quickstart_notebooks/04-cave-query-synapses.html#materialization-versions",
    "href": "quickstart_notebooks/04-cave-query-synapses.html#materialization-versions",
    "title": "CAVE Query: Synaptic Connectivity",
    "section": "Materialization versions",
    "text": "Materialization versions\nData in CAVE is timestamped and periodically versioned - each (materialization) version corresponds to a specific timestamp. Individual versions are made publicly available. The materialization service provides annotation queries to the dataset. It is available under client.materialize.\nCurrently the following versions are publicly available (in this tutorial we will be using 1181):\n\nclient.materialize.get_versions()\n\n[1300, 1078, 117, 661, 343, 1181, 795, 943]\n\n\nAnd these are their associated timestamps (all timestamps are in UTC):\n\nfor version in client.materialize.get_versions():\n    print(f\"Version {version}: {client.materialize.get_timestamp(version)}\")\n\nVersion 1300: 2025-01-13 10:10:01.286229+00:00\nVersion 1078: 2024-06-05 10:10:01.203215+00:00\nVersion 117: 2021-06-11 08:10:00.215114+00:00\nVersion 661: 2023-04-06 20:17:09.199182+00:00\nVersion 343: 2022-02-24 08:10:00.184668+00:00\nVersion 1181: 2024-09-16 10:10:01.121167+00:00\nVersion 795: 2023-08-23 08:10:01.404268+00:00\nVersion 943: 2024-01-22 08:10:01.497934+00:00\n\n\nThe client will automatically query the latest materialization version. You can specify a materialization_version for every query if you want to access a specific version.\nFor instance, this example uses version 1181, and not default latest (version 1300, at time of writing)\n\nclient.version=1181",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Synaptic Connectivity"
    ]
  },
  {
    "objectID": "quickstart_notebooks/04-cave-query-synapses.html#querying-synapses",
    "href": "quickstart_notebooks/04-cave-query-synapses.html#querying-synapses",
    "title": "CAVE Query: Synaptic Connectivity",
    "section": "Querying Synapses",
    "text": "Querying Synapses\nWhile synapses are stored as any other table in the database, in this case synapses_pni_2, this table is much larger than any other table at more than 337 million rows, and it works best when queried in a different way.\nThe synapse_query function allows you to query the synapse table in a more convenient way than most other tables. In particular, the pre_ids and post_ids let you specify which root id (or collection of root ids) you want to query, with pre_ids indicating the collection of presynaptic neurons and post_ids the collection of postsynaptic neurons.\nUsing both pre_ids and post_ids in one call is effectively a logical AND, returning only those synapses from neurons in the list of pre_ids that target neurons in the list of post_ids.\nLet’s look at one particular example.\n\nmy_root_id = 864691135808473885\nsyn_df = client.materialize.synapse_query(pre_ids=my_root_id)\nprint(f\"Total number of output synapses for {my_root_id}: {len(syn_df)}\")\nsyn_df.head()\n\nTotal number of output synapses for 864691135808473885: 1498\n\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\nsize\npre_pt_supervoxel_id\npre_pt_root_id\npost_pt_supervoxel_id\npost_pt_root_id\npre_pt_position\npost_pt_position\nctr_pt_position\n\n\n\n\n0\n158405512\n2020-11-04 06:48:59.403833+00:00\nNaN\nt\n420\n89385416926790697\n864691135808473885\n89385416926797494\n864691135546540484\n[179076, 188248, 20233]\n[179156, 188220, 20239]\n[179140, 188230, 20239]\n\n\n1\n185549462\n2020-11-04 06:49:10.903020+00:00\nNaN\nt\n4832\n91356016507479890\n864691135808473885\n91356016507470163\n864691135884799088\n[193168, 190452, 19262]\n[193142, 190404, 19257]\n[193180, 190432, 19254]\n\n\n2\n138110803\n2020-11-04 06:49:46.758528+00:00\nNaN\nt\n3176\n87263084540201919\n864691135808473885\n87263084540199587\n864691135759983182\n[163440, 104292, 19808]\n[163498, 104348, 19806]\n[163460, 104356, 19804]\n\n\n3\n157378264\n2020-11-04 07:38:27.332669+00:00\nNaN\nt\n412\n89374490395905686\n864691135808473885\n89374490395921430\n864691135446953106\n[179218, 107132, 19372]\n[179204, 107010, 19383]\n[179196, 107072, 19380]\n\n\n4\n174798776\n2020-11-04 10:10:59.416878+00:00\nNaN\nt\n1796\n90089104301487245\n864691135808473885\n90089104301487089\n864691135572292333\n[184038, 188292, 19753]\n[183920, 188202, 19754]\n[183998, 188216, 19755]\n\n\n\n\n\n\n\nNote that synapse queries always return the list of every synapse between the neurons in the query, even if there are multiple synapses between the same pair of neurons.\nA common pattern to generate a list of connections between unique pairs of neurons is to group by the root ids of the presynaptic and postsynaptic neurons and then count the number of synapses between them. For example, to get the number of synapses from this neuron onto every other neuron, ordered\n\nsyn_df.groupby(\n  ['pre_pt_root_id', 'post_pt_root_id']\n).count()[['id']].rename(\n  columns={'id': 'syn_count'}\n).sort_values(\n  by='syn_count',\n  ascending=False,\n)\n# Note that the 'id' part here is just a way to quickly extract one column.\n# This could be any of the remaining column names, but `id` is often convenient \n# because it is common to all tables.\n\n\n\n\n\n\n\n\n\nsyn_count\n\n\npre_pt_root_id\npost_pt_root_id\n\n\n\n\n\n864691135808473885\n864691135865557118\n20\n\n\n864691135214122296\n16\n\n\n864691136578647572\n15\n\n\n864691136066504856\n13\n\n\n864691135841325283\n11\n\n\n...\n...\n\n\n864691136926552138\n1\n\n\n864691136952088543\n1\n\n\n864691135241125665\n1\n\n\n864691136952690399\n1\n\n\n864691136974682652\n1\n\n\n\n\n1035 rows × 1 columns\n\n\n\nWe can query the synapse table directly. However, it is too large to query all at once. CAVE limits to queries to 500,000 rows at once and will display a warning when that happens. Here, we demonstrate this with the limit set to 10:\n\nsynapse_table_name = client.info.get_datastack_info()[\"synapse_table\"]\nsyn_df = client.materialize.query_table(synapse_table_name, limit=10, desired_resolution=[1, 1, 1], split_positions=True)\nsyn_df\n\n\n\n\n\n\n\n\nid\ncreated\nsuperceded_id\nvalid\npre_pt_position_x\npre_pt_position_y\npre_pt_position_z\npost_pt_position_x\npost_pt_position_y\npost_pt_position_z\nctr_pt_position_x\nctr_pt_position_y\nctr_pt_position_z\nsize\npre_pt_supervoxel_id\npre_pt_root_id\npost_pt_supervoxel_id\npost_pt_root_id\n\n\n\n\n0\n4456\n2020-11-04 13:02:08.388988+00:00\nNaN\nt\n211448.0\n409744.0\n801440.0\n211448.0\n409744.0\n801440.0\n211612.0\n410172.0\n801400.0\n2956\n72063160986635724\n864691135533713769\n72063160986635724\n864691135533713769\n\n\n1\n4503\n2020-11-04 12:09:33.286834+00:00\nNaN\nt\n212456.0\n408032.0\n800360.0\n212456.0\n408032.0\n800360.0\n212168.0\n408088.0\n800400.0\n344\n72063092267156962\n864691135087527094\n72063092267156962\n864691135087527094\n\n\n2\n4508\n2020-11-04 13:02:13.024144+00:00\nNaN\nt\n212448.0\n411696.0\n801440.0\n212448.0\n411696.0\n801440.0\n212224.0\n411800.0\n801560.0\n344\n72063229706111827\n864691135533713769\n72063229706111827\n864691135533713769\n\n\n3\n4568\n2020-11-04 13:44:08.085705+00:00\nNaN\nt\n213392.0\n415448.0\n802920.0\n213392.0\n415448.0\n802920.0\n213096.0\n415176.0\n802880.0\n13816\n72133735889250131\n864691134530418554\n72133735889250131\n864691134530418554\n\n\n4\n4581\n2020-11-04 07:29:12.917622+00:00\nNaN\nt\n213552.0\n417184.0\n800800.0\n213552.0\n417184.0\n800800.0\n213240.0\n417080.0\n801080.0\n10436\n72133804608718799\n864691134745062676\n72133804608718799\n864691134745062676\n\n\n5\n4582\n2020-11-04 13:02:17.694701+00:00\nNaN\nt\n212880.0\n409120.0\n801440.0\n212880.0\n409120.0\n801440.0\n213016.0\n408832.0\n801520.0\n1344\n72063160986636743\n864691135533713769\n72063160986636743\n864691135533713769\n\n\n6\n4588\n2020-11-04 12:20:12.290593+00:00\nNaN\nt\n213200.0\n421120.0\n805520.0\n213200.0\n421120.0\n805520.0\n213064.0\n421000.0\n805600.0\n7128\n72133942047682150\n864691134609767690\n72133942047682150\n864691134609767690\n\n\n7\n4590\n2020-11-04 13:20:01.875310+00:00\nNaN\nt\n213504.0\n406440.0\n805160.0\n213504.0\n406440.0\n805160.0\n213336.0\n406596.0\n805200.0\n6572\n72133461011344162\n864691135091400630\n72133461011344162\n864691135091400630\n\n\n8\n4606\n2020-11-04 07:24:39.038223+00:00\nNaN\nt\n213384.0\n413792.0\n800800.0\n213384.0\n413792.0\n800800.0\n213256.0\n413976.0\n801040.0\n2100\n72133667169766499\n864691134609872906\n72133667169766499\n864691134609872906\n\n\n9\n4611\n2020-11-04 07:24:37.800341+00:00\nNaN\nt\n213336.0\n415304.0\n800960.0\n213336.0\n415304.0\n800960.0\n213192.0\n415604.0\n800960.0\n492\n72133735889243887\n864691134609872906\n72133735889243887\n864691134609872906\n\n\n\n\n\n\n\nInstead we need to limit our query to a few neurons. The next section will load the proofread cells, and merge their cell type information for some connectivity mapping",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Synaptic Connectivity"
    ]
  },
  {
    "objectID": "quickstart_notebooks/04-cave-query-synapses.html#query-proofread-cells-and-connectivity",
    "href": "quickstart_notebooks/04-cave-query-synapses.html#query-proofread-cells-and-connectivity",
    "title": "CAVE Query: Synaptic Connectivity",
    "section": "Query proofread cells and connectivity",
    "text": "Query proofread cells and connectivity\n\nProofread neurons\nThe table proofreading_status_and_strategy contains proofreading information about ~1,300 neurons. This manifest on microns-explorer.org provides the most detailed overview. In brief, axons annotated with any strategy_axon were cleaned of false mergers but not all were fully extended. The most important distinction is axons annotated with axon_column_truncated were only proofread within a certain volume wheras others were proofread without such bias.\n\nproof_df = client.materialize.tables.proofreading_status_and_strategy(status_axon='t').query(desired_resolution=[1, 1, 1], split_positions=True)\nproof_df[\"strategy_axon\"].value_counts()\n\nstrategy_axon\naxon_partially_extended    979\naxon_column_truncated      233\naxon_interareal            144\naxon_fully_extended         80\nName: count, dtype: int64\n\n\n\n\nQuery synapses between proofread neurons\nWe can query the graph spanned by the neurons with proofread axons using the filter_in_dict parameter (takes ~3 mins):\n\n%%time \n# This takes 3-5 minutes to complete\n\nsynapse_table_name = client.info.get_datastack_info()[\"synapse_table\"]\nsyn_proof_only_df = client.materialize.query_table(synapse_table_name, desired_resolution=[1, 1, 1], split_positions=True,\n                                                   filter_in_dict={\"pre_pt_root_id\": proof_df[\"pt_root_id\"], \n                                                                   \"post_pt_root_id\": proof_df[\"pt_root_id\"]})\n\n# remove autapses\nsyn_proof_only_df = syn_proof_only_df[syn_proof_only_df[\"pre_pt_root_id\"] != syn_proof_only_df[\"post_pt_root_id\"]]\nprint(len(syn_proof_only_df))\n\n132438\nCPU times: total: 297 ms\nWall time: 2min 23s\n\n\n\n\nPlot connectivity as binarized heatmap\nNow lets plot the connectivity between every proofread cell and every other cell\n\nsyn_mat = syn_proof_only_df.pivot_table(index=\"pre_pt_root_id\", columns=\"post_pt_root_id\", \n                                      values=\"size\", aggfunc=lambda x: float(np.sum(x) &gt; 0)).fillna(0)\nsyn_mat = syn_mat.reindex(columns=np.array(syn_mat.index))\n\n\nfig, ax = plt.subplots(figsize=(7, 5))\nsns.heatmap(syn_mat, cmap=\"gray_r\", xticklabels=[], yticklabels=[], \n            ax=ax, square=True,\n            cbar_kws={\"label\": \"Connected - binary\"})\n\n\n\n\n\n\n\n\nThere is some structure of highly interconnected cells. By adding information about the type of cells, we might infer more about the connectivity patterns",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Synaptic Connectivity"
    ]
  },
  {
    "objectID": "quickstart_notebooks/04-cave-query-synapses.html#add-cell-type-information-to-connectivity",
    "href": "quickstart_notebooks/04-cave-query-synapses.html#add-cell-type-information-to-connectivity",
    "title": "CAVE Query: Synaptic Connectivity",
    "section": "Add cell type information to connectivity",
    "text": "Add cell type information to connectivity\n\nQuerying cell type information\nThere are two distinct ways cell types were classified in the MICrONS dataset: manual and automated. Manual annotations are available for ~1,000 neurons (allen_v1_column_types_slanted_ref), automated classifications are available for all cell bodies based on these manual annotations (aibs_metamodel_celltypes_v661). Because they are annotating an existing annotations, these annotations are introduced as a “reference” table:\n\n\n\n\n\n\nAnnotation tables\n\n\n\nFor more on cell types and their tables, see the Annotation Tables page.\n\n\n\nct_manual_df = client.materialize.query_table(\"allen_v1_column_types_slanted_ref\", desired_resolution=[1, 1, 1], split_positions=True)\n\n# rename the reference column for clarity\nct_manual_df.rename(columns={'target_id': 'nucleus_id'}, inplace=True)\n\n# remove segments with multiple cell bodies\nct_manual_df.drop_duplicates(\"pt_root_id\", keep=False, inplace=True)\nct_manual_df.head(5)\n\n\n\n\n\n\n\n\nid\ncreated\nvalid\nnucleus_id\nclassification_system\ncell_type\nid_ref\ncreated_ref\nvalid_ref\nvolume\n...\npt_position_y\npt_position_z\nbb_start_position_x\nbb_start_position_y\nbb_start_position_z\nbb_end_position_x\nbb_end_position_y\nbb_end_position_z\npt_supervoxel_id\npt_root_id\n\n\n\n\n0\n50\n2023-03-18 14:13:21.613360+00:00\nt\n258319\naibs_coarse_excitatory\n23P\n258319\n2020-09-28 22:40:42.476911+00:00\nt\n261.806162\n...\n572992.0\n849520.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n89309001002848425\n864691135927260174\n\n\n1\n1119\n2023-03-18 14:13:22.506660+00:00\nt\n276438\naibs_coarse_excitatory\n6P-CT\n276438\n2020-09-28 22:40:42.700226+00:00\nt\n277.317714\n...\n1035072.0\n943880.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n89465269428261699\n864691136487559186\n\n\n2\n35\n2023-03-18 14:13:21.602813+00:00\nt\n260552\naibs_coarse_excitatory\n23P\n260552\n2020-09-28 22:40:42.745779+00:00\nt\n230.111805\n...\n631872.0\n840080.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n89170256379033022\n864691135510274057\n\n\n3\n95\n2023-03-18 14:13:21.644304+00:00\nt\n260263\naibs_coarse_excitatory\n23P\n260263\n2020-09-28 22:40:42.746658+00:00\nt\n274.324193\n...\n632512.0\n810640.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n88044356338331571\n864691135694415551\n\n\n4\n81\n2023-03-18 14:13:21.634505+00:00\nt\n262898\naibs_coarse_inhibitory\nBPC\n262898\n2020-09-28 22:40:42.749245+00:00\nt\n230.092308\n...\n701120.0\n878560.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n88468836747612860\n864691135759892302\n\n\n\n\n5 rows × 21 columns\n\n\n\nThe reference table added two additional data columns: classification_system and cell_type. The classification_system divides the cells into excitatitory and inhibitory neurons as well as non-neuronal cells. cell_type provides lower level cell annotations.\nNext, we query the automatically classified cell type information. The query works the same way:\n\nct_auto_df = client.materialize.query_table(\"aibs_metamodel_celltypes_v661\", desired_resolution=[1, 1, 1], split_positions=True)\n\n# rename the reference column for clarity\nct_manual_df.rename(columns={'target_id': 'nucleus_id'}, inplace=True)\n\n# remove segments with multiple cell bodies\nct_auto_df.drop_duplicates(\"pt_root_id\", keep=False, inplace=True)\nct_auto_df.head(5)\n\n\n\n\n\n\n\n\nid\ncreated\nvalid\ntarget_id\nclassification_system\ncell_type\nid_ref\ncreated_ref\nvalid_ref\nvolume\n...\npt_position_y\npt_position_z\nbb_start_position_x\nbb_start_position_y\nbb_start_position_z\nbb_end_position_x\nbb_end_position_y\nbb_end_position_z\npt_supervoxel_id\npt_root_id\n\n\n\n\n0\n36916\n2023-12-19 22:47:18.659864+00:00\nt\n336365\nexcitatory_neuron\n5P-IT\n336365\n2020-09-28 22:42:48.966292+00:00\nt\n272.488202\n...\n723328.0\n1083040.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n93606511657924288\n864691136274724621\n\n\n1\n1070\n2023-12-19 22:38:00.472115+00:00\nt\n110648\nexcitatory_neuron\n23P\n110648\n2020-09-28 22:45:09.650639+00:00\nt\n328.533443\n...\n518528.0\n1016400.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n79385153184885329\n864691135489403194\n\n\n2\n1099\n2023-12-19 22:38:00.898837+00:00\nt\n112071\nexcitatory_neuron\n23P\n112071\n2020-09-28 22:43:34.088785+00:00\nt\n272.929423\n...\n597888.0\n623320.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n79035988248401958\n864691136147292311\n\n\n3\n13259\n2023-12-19 22:41:14.417986+00:00\nt\n197927\nnonneuron\noligo\n197927\n2020-09-28 22:43:10.652649+00:00\nt\n91.308851\n...\n744768.0\n1058840.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n84529699506051734\n864691136050858227\n\n\n4\n13271\n2023-12-19 22:41:14.685474+00:00\nt\n198087\nnonneuron\nastrocyte\n198087\n2020-09-28 22:41:36.677186+00:00\nt\n161.744978\n...\n763776.0\n1094440.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n83756261929388963\n864691135809440972\n\n\n\n\n5 rows × 21 columns\n\n\n\n\nct_auto_df[\"classification_system\"].value_counts()\n\nclassification_system\nexcitatory_neuron    63761\nnonneuron            18697\ninhibitory_neuron     7849\nName: count, dtype: int64\n\n\n\nct_auto_df[\"cell_type\"].value_counts()\n\ncell_type\n23P          19643\n4P           14722\n6P-IT        11637\n5P-IT         7889\nastrocyte     7108\noligo         6900\n6P-CT         6755\nBC            3310\nMC            2434\nmicroglia     2394\n5P-ET         2158\nBPC           1484\nOPC           1449\n5P-NP          957\npericyte       846\nNGC            621\nName: count, dtype: int64\n\n\nWe can merge the manual and automatic cell types together into a single cell type table for convenience.\nNote: the cells for which there is no manual cell type will appear as a NaN in the following dataframe\n\nct_all_df = pd.merge(ct_auto_df[['pt_root_id','classification_system','cell_type', 'id_ref']],\n                     ct_manual_df[['pt_root_id','classification_system','cell_type']],\n                     on='pt_root_id',\n                     how='outer',\n                     suffixes=['_auto','_manual'],\n                    )\nct_all_df['cell_type_auto'] = ct_all_df.cell_type_auto.fillna('unsure')\nct_all_df['cell_type_manual'] = ct_all_df.cell_type_manual.fillna('unsure')\nct_all_df.tail()\n\n\n\n\n\n\n\n\npt_root_id\nclassification_system_auto\ncell_type_auto\nid_ref\nclassification_system_manual\ncell_type_manual\n\n\n\n\n90332\n864691137198895425\nexcitatory_neuron\n4P\n298773.0\naibs_coarse_excitatory\n5P-IT\n\n\n90333\n864691137198900801\nexcitatory_neuron\n4P\n260651.0\naibs_coarse_excitatory\n4P\n\n\n90334\n864691137198933569\nexcitatory_neuron\n5P-IT\n196769.0\nNaN\nunsure\n\n\n90335\n864691137198939713\nexcitatory_neuron\n6P-CT\n234930.0\nNaN\nunsure\n\n\n90336\n864691137198943297\nnonneuron\nastrocyte\n468059.0\nNaN\nunsure\n\n\n\n\n\n\n\n\nct_proof_df = pd.merge(proof_df[['pt_root_id']], ct_all_df,\n                       on='pt_root_id',\n                       how='left')\n\n# ct_proof_df = ct_all_df[np.isin(ct_all_df[\"pt_root_id\"], proof_df[\"pt_root_id\"])]\nct_proof_df.set_index('pt_root_id', inplace=True)\nct_proof_df\n\n\n\n\n\n\n\n\nclassification_system_auto\ncell_type_auto\nid_ref\nclassification_system_manual\ncell_type_manual\n\n\npt_root_id\n\n\n\n\n\n\n\n\n\n864691135464714565\nexcitatory_neuron\n5P-ET\n527784.0\nNaN\nunsure\n\n\n864691136228491601\nexcitatory_neuron\n23P\n294589.0\naibs_coarse_excitatory\n23P\n\n\n864691136445280131\nexcitatory_neuron\n23P\n292674.0\naibs_coarse_excitatory\n23P\n\n\n864691135059817627\nexcitatory_neuron\n23P\n291175.0\naibs_coarse_excitatory\n23P\n\n\n864691136424171823\nexcitatory_neuron\n23P\n256510.0\naibs_coarse_excitatory\n23P\n\n\n...\n...\n...\n...\n...\n...\n\n\n864691135953632803\nexcitatory_neuron\n4P\n397167.0\nNaN\nunsure\n\n\n864691135741659499\nexcitatory_neuron\n4P\n493796.0\nNaN\nunsure\n\n\n864691135741684075\nexcitatory_neuron\n4P\n612794.0\nNaN\nunsure\n\n\n864691135741721707\nexcitatory_neuron\n23P\n361632.0\nNaN\nunsure\n\n\n864691136674219143\nexcitatory_neuron\n23P\n358984.0\nNaN\nunsure\n\n\n\n\n1436 rows × 5 columns",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "CAVE Query: Synaptic Connectivity"
    ]
  },
  {
    "objectID": "quickstart_notebooks/06-cloudvolume-download-mesh.html",
    "href": "quickstart_notebooks/06-cloudvolume-download-mesh.html",
    "title": "Download Meshes",
    "section": "",
    "text": "When trying to understand the fine 3d morphology of a neuron (e.g. features under 1 micron in scale), meshes are a particularly useful representation.More precisely, a mesh is a collection of vertices and faces that define a 3d surface.\nThe meshes that one sees in Neuroglancer are available to download through the python client cloud-volume, and can be loaded for analysis and visualization in other tools.\nThe MICrONS data can be representing and rendered in multiple formats, at different levels of abstraction from the original imagery.\n\n\n\n\n\n\n\n\n\nSegmentation: more expensive\n\n\n\n\n\n\n\nMesh\n\n\n\n\n\n\n\nSkeleton: less expensive",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Meshes"
    ]
  },
  {
    "objectID": "quickstart_notebooks/06-cloudvolume-download-mesh.html#connected-morphological-representations-meshes",
    "href": "quickstart_notebooks/06-cloudvolume-download-mesh.html#connected-morphological-representations-meshes",
    "title": "Download Meshes",
    "section": "",
    "text": "When trying to understand the fine 3d morphology of a neuron (e.g. features under 1 micron in scale), meshes are a particularly useful representation.More precisely, a mesh is a collection of vertices and faces that define a 3d surface.\nThe meshes that one sees in Neuroglancer are available to download through the python client cloud-volume, and can be loaded for analysis and visualization in other tools.\nThe MICrONS data can be representing and rendered in multiple formats, at different levels of abstraction from the original imagery.\n\n\n\n\n\n\n\n\n\nSegmentation: more expensive\n\n\n\n\n\n\n\nMesh\n\n\n\n\n\n\n\nSkeleton: less expensive",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Meshes"
    ]
  },
  {
    "objectID": "quickstart_notebooks/06-cloudvolume-download-mesh.html#what-is-a-mesh",
    "href": "quickstart_notebooks/06-cloudvolume-download-mesh.html#what-is-a-mesh",
    "title": "Download Meshes",
    "section": "What is a mesh?",
    "text": "What is a mesh?\nA mesh is a set of vertices connected via triangular faces to form a 3 dimensional representation of the outer membrane of a neuron, glia or nucleus.\n\nMeshes can either be static or dynamic:\n\nStatic:\n\npros: smaller files thus easier to work with, multiple levels of detail (lod) which can be accessed (example below)\ncons: may include false gaps and merges from self contacts, updated less frequently\n\nExample path: precomputed://gs://iarpa_microns/minnie/minnie65/seg_m1300\n\n\nDynamic:\n\npros: highly detailed thus more reflective of biological reality and backed by proofreading infrastructure CAVE (Connectome Annotation Versioning Engine)\ncons: much larger files, only one level of detail\n\nExample path: graphene://https://minnie.microns-daf.com/segmentation/table/minnie65_public",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Meshes"
    ]
  },
  {
    "objectID": "quickstart_notebooks/06-cloudvolume-download-mesh.html#cloud-volume-for-downloading-meshes",
    "href": "quickstart_notebooks/06-cloudvolume-download-mesh.html#cloud-volume-for-downloading-meshes",
    "title": "Download Meshes",
    "section": "Cloud-Volume for downloading meshes",
    "text": "Cloud-Volume for downloading meshes\nCloudVolume is a serverless Python client for random access reading and writing of Neuroglancer volumes in “Precomputed” format, a set of representations for arbitrarily large volumetric images, meshes, and skeletons.\nPrecomputed volumes are typically stored on AWS S3, Google Storage, or locally. CloudVolume can read and write to these object storage providers given a service account token with appropriate permissions. However, these volumes can be stored on any service, including an ordinary webserver or local filesystem, that supports key-value access.\ncloud-volume is the mechanism for many programmatic data queries, and integrates heavily with the CAVE ecosystem but is distinct. You can install cloud-volume with:\npip install cloud-volume\nThe Connectome Annotation Versioning Engine (CAVE) is a suite of tools developed at the Allen Institute and Seung Lab to manage large connectomics data.\n\n\n\n\n\n\nInitial Setup\n\n\n\nBefore using any programmatic access to the data, you first need to set up your CAVEclient token. When CAVEclient saves the token to your local machine, cloudvolume will access the same token.\n\n\n\nfrom caveclient import CAVEclient\nfrom cloudvolume import CloudVolume\n\ndatastack_name = 'minnie65_public'\nclient = CAVEclient(datastack_name)\n\nThe datastack includes information about the segmentation source, and can provide that information when prompted\n\nclient.info.segmentation_source()\n\n'graphene://https://minnie.microns-daf.com/segmentation/table/minnie65_public'\n\n\nNote that this is the same as the dynamic segmentation path above\n\n# this can be used to initialize a cloudvolume object\ncv = CloudVolume(client.info.segmentation_source(), progress=False, use_https=True)\n\n\nExamples from the cloudvolume README:\ncv.mesh.get(12345) # return the mesh as vertices and faces instead of writing to disk\ncv.mesh.get([ 12345, 12346 ]) # return these two segids fused into a single mesh\ncv.mesh.get([ 12345, 12346 ], fuse=False) # return { 12345: mesh, 12346: mesh }\n\n\nDownload dynamic mesh\nGiven a root_id, CloudVolume can be used to retrieve the mesh. cloudvolume.mesh.get() returns a dictionary with the neuron segment id as the key and the mesh as the value\n\n# specify the materialization version, for consistency across time\",\nclient.version = 1300\n\n# Example: pyramidal cell in v1300\nexample_cell_id = 864691135572530981\n%time mesh = cv.mesh.get(example_cell_id)[example_cell_id]\n\nWarning: deduplication not currently supported for this layer's variable layered draco meshes\nCPU times: total: 48 s\nWall time: 1min 2s\n\n\n\n\nDownload static mesh\nThe meshes that are available in the visualization on microns-explorer.org are fast to load because they are static and have been downsampled at multiple resolutions. However, this comes with the drawback of being less biologically accurate.\nLevel of detail is controlled with optional argument, where lod=0 is the highest level of detail\nwe can access one of these downsampled static meshes by setting the path here:\n\ncv = CloudVolume(\"precomputed://gs://iarpa_microns/minnie/minnie65/seg_m1300\", use_https=True)\n\n\n# the cloud volume interface is the same but it is a faster initial download \n%time mesh = cv.mesh.get(example_cell_id, lod=3)[example_cell_id]\n\nCPU times: total: 78.1 ms\nWall time: 1.19 s\n\n\n\n# as you can see the meshes aren't exactly the same as before. They because they have not been downsampled\nmesh.vertices.shape, mesh.faces.shape\n\n((56938, 3), (99936, 3))\n\n\nIn addition, the Static meshes are available in 3 levels of detail, this covers two orders of magnitude of detail which is what neuroglancer leverages to efficiently load the data at the resolution necessary to render the current scene.\n\nfor lod in range(4):\n    mesh = mesh = cv.mesh.get(example_cell_id, lod=lod)[example_cell_id]\n    print(f\"level of detail {lod}: n_verts: {mesh.vertices.shape[0]} n_faces: {mesh.faces.shape[0]}\")\n\nlevel of detail 0: n_verts: 5231141 n_faces: 10291430\n\n\nlevel of detail 1: n_verts: 1457607 n_faces: 2828410\n\n\nlevel of detail 2: n_verts: 253583 n_faces: 474769\n\n\nlevel of detail 3: n_verts: 56938 n_faces: 99936\n\n\nThe returned mesh includes: * vertices : Nx3 [x, y, z] positions in nm * faces: Kx3 [i, j, k] indices into vertices that describe the triangular meshes",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Meshes"
    ]
  },
  {
    "objectID": "quickstart_notebooks/06-cloudvolume-download-mesh.html#meshparty-for-processing-mesh-objects",
    "href": "quickstart_notebooks/06-cloudvolume-download-mesh.html#meshparty-for-processing-mesh-objects",
    "title": "Download Meshes",
    "section": "MeshParty for processing mesh objects",
    "text": "MeshParty for processing mesh objects\nThe Meshparty, which is a python package to work with meshes, designed around use cases for analyzing neuronal morphology.\npip install meshparty\nThe MeshParty object has more useful properties and attributes such as: * a scipy.csgraph sparse graph object (mesh.csgraph) * a networkx graph object (mesh.nxgraph)\nRead more about what you can do with MeshParty on its Documentation.\nIn particular it lets you associate skeletons, and annotations onto the mesh into a “meshwork” object.\n\nimport os\nfrom meshparty import trimesh_io\nfrom caveclient import CAVEclient\nclient = CAVEclient('minnie65_public')\n\nmm = trimesh_io.MeshMeta(\n  cv_path=client.info.segmentation_source(),\n  disk_cache_path=\"meshes\",\n)\n\nexample_cell_id = 864691135572530981\nmesh = mm.mesh(seg_id=example_cell_id)\n\nWarning: deduplication not currently supported for this layer's variable layered draco meshes\n\n\nOne convenience of using the MeshMeta approach is that if you have already downloaded a mesh for with a given root id, it will be loaded from disk rather than re-downloaded.\nIf you have to download many meshes, it is somewhat faster to use the bulk download_meshes function and use multiple threads via the n_threads argument. If you download them to the same folder used for the MeshMeta object, they can be loaded through the same interface.\nroot_ids = [864691135572530981, 864691135014128278, 864691134940133219]\nmm = trimesh_io.download_meshes(\n    seg_ids=root_ids,\n    target_dir='meshes',\n    cv_path=client.info.segmentation_source(),\n    n_threads=4, # Or whatever value you choose above one but less than the number of cores on your computer\n)\n\n\n\n\n\n\nFile size\n\n\n\nMeshes can be hundresds of megabytes in size, so be careful about downloading too many if the internet is not acting well or your computer doesn’t have much disk space!\n\n\n\nHealing Mesh Gaps\n\n\n\nExample of a continuous neuron whose mesh has a gap\n\n\nMany meshes are not actually fully continuous due to small gaps in the segmentation. However, information collected during proofreading allows one to partially repair these gaps by adding in links where the segmentation was merged across a small gap. If you are just visualizaing a mesh, these gaps are not a problem, but if you want to do analysis on the mesh, you will want to heal these gaps. Conveniently, there’s a function to do this:\nmesh.add_link_edges(\n    seg_id=example_cell_id, # This needs to be the same as the root id used to download the mesh\n    client=client.chunkedgraph,\n)\n\n\nProperties\nMeshes have a large number of properties, many of which come from being based on the Trimesh library’s mesh format, and others being specific to MeshParty.\nSeveral of the most important properties are:\n\nmesh.vertices : An N x 3 list of vertices and their 3d location in nanometers, where N is the number of vertices.\nmesh.faces : An P x 3 list of integers, with each row specifying a triangle of connected vertex indices.\nmesh.edges : An M x 2 list of integers, with each row specifying a pair of connected vertex indices based off of faces.\nmesh.edges : An M x 2 list of integers, with each row specifying a pair of connected vertex indices based off of faces.\nmesh.link_edges : An M_l x 2 list of integers, with each row specifying a pair of “link edges” that were used to heal gaps based on proofreading edits.\nmesh.graph_edges : An (M+M_l) x 2 list of integers, with each row specifying a pair of graph edges, which is the collection of both mesh.edges and mesh.link_edges.\nmesh.csgraph : A Scipy Compressed Sparse Graph representation of the mesh as an NxN graph of vertices connected to one another using graph edges and with edge weights being the distance between vertices. This is particularly useful for computing shortest paths between vertices.\n\n\n\n\n\n\n\nWarning\n\n\n\nMICrONs meshes are not generally “watertight”, a property that would enable a number of properties to be computed natively by Trimesh. Because of this, Trimesh-computed properties relating to solid forms or volumes like mesh.volume or mesh.center_mass do not have sensible values and other approaches should be taken. Unfortunately, because of the Trimesh implementation of these properties it is up to the user to be aware of this issue.\n\n\n\n\nVisualization\nThere are a variety of tools for visualizing meshes in python. MeshParty interfaces with VTK, a powerful but complex data visualization library that does not always work well in python. The basic pattern for MeshParty’s VTK integration is to create one or more “actors” from the data, and then pass those to a renderer that can be displayed in an interactive approach. The following code snippet shows how to visualize a mesh using this approach.\nmesh_actor = trimesh_vtk.mesh_actor(\n  mesh,\n  color=(1,0,0),\n  opacity=0.5,\n)\ntrimesh_vtk.render_actors([mesh_actor])\nNote that by default, neurons will appear upside down because the coordinate system of the dataset has the y-axis value increasing along the “downward” pia to white matter axis.\nMore documentation on the MeshParty VTK visualization can be found here.\nOther tools worth exploring are:\n\nPyVista\nPolyscope\nVedo\nMeshLab\nIf you have some existing experience, Blender (see Blender integration by our friends behind Navis, a fly-focused framework analyzing connectomics data).\n\n\n\nMasking\nOne of the most common operations on meshes is to mask them to a particular region of interest. This can be done by “masking” the mesh with a boolean array of length N where N is the number of vertices in the mesh, with True where the vertex should be kept and False where it should be omitted. There are several convenience functions to generate common masks in the Mesh Filters module.\nIn the following example, we will first mask out all vertices that aren’t part of the largest connected component of the mesh (i.e. get rid of floating vertices that might arise due to internal surfaces) and then mask out all vertices that are more than 20,000 nm away from the soma center.\nfrom meshparty import mesh_filters\n\nroot_id =864691134940133219 \nroot_point = client.materialize.tables.nucleus_detection_v0(pt_root_id=root_id).query()['pt_position'].values[0] * [4,4,40]  # Convert the nucleus location from voxels to nanometers via the data resolution.\n\nmesh = mm.mesh(seg_id=root_id)\n# Heal gaps in the mesh\nmesh.add_link_edges(\n    seg_id=864691134940133219,\n    client=client.chunkedgraph,\n)\n\n# Generate and use the largest component mask\ncomp_mask = mesh_filters.filter_largest_component(mesh)\nmask_filt = mesh.apply_mask(comp_mask)\n\nsoma_mask = mesh_filters.filter_spatial_distance_from_points(\n    mask_filt,\n    root_point,\n    20_000, # Note that this is in nanometers\n)\nmesh_soma = mesh.apply_mask(soma_mask)\nThis resulting mesh is just a small cutout around the soma.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Download Meshes"
    ]
  },
  {
    "objectID": "quickstart_notebooks/08-standard-transform.html",
    "href": "quickstart_notebooks/08-standard-transform.html",
    "title": "Standard Transform: Coordinate System",
    "section": "",
    "text": "Because of the many different data representations, dealing with coordinates in the MICrONs data is not entirely simple and it is easy to make mistakes by converting between coordinate systems incorrectly.\nThere are three main coordinate systems that wind up getting used:\n\nVoxel coordinates are the coordinates of a point in the original image volume. These are the coordinates that are used to index into the volumes you can see in Neuroglancer, but each number has a potentially different unit. In the MICrONs data, a voxel is 4 nm wide in the x and y directions, and 40 nm long in the z direction. This means that a 1x1x1 micron cube would be represented by a 250x250x25 voxel span. Annotations (such as synapses) are stored in voxel coordinates.\nNanometer coordinates are the coordinates of a point in the original image volume, but in nanometers. This is equivalent to the voxel coordinate multiplied by the voxel resolution, with no further transformation applied. Mesh and skeleton vertices are stored in nanometer coordinates.\nTransformed coordinates reflect a trasnsformation that has been applied to the original image volume. This transformation is a rotation to make the pia surface as flat as possible, a translation to move the pial surface to y=0, and a scaling to bring coordinates into microns. Transformed coordinates are convenient for more accurate computations of depth and the pia-to-white-matter axis, but are not stored by default. A python package standard_transform helps convert data to and from transformed coordinates.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that in all of these coordinate systems (including Neuroglancer), the y axis increases with depth. This is a standard definition when working with images, but is the opposite of what you usually think of with points. Because of that, when plotting annotations or neuroanatomy in matplotlib, you will usually have to invert the y axis with ax.invert_yaxis().",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Standard Transform: Coordinate System"
    ]
  },
  {
    "objectID": "quickstart_notebooks/08-standard-transform.html#introduction",
    "href": "quickstart_notebooks/08-standard-transform.html#introduction",
    "title": "Standard Transform: Coordinate System",
    "section": "",
    "text": "Because of the many different data representations, dealing with coordinates in the MICrONs data is not entirely simple and it is easy to make mistakes by converting between coordinate systems incorrectly.\nThere are three main coordinate systems that wind up getting used:\n\nVoxel coordinates are the coordinates of a point in the original image volume. These are the coordinates that are used to index into the volumes you can see in Neuroglancer, but each number has a potentially different unit. In the MICrONs data, a voxel is 4 nm wide in the x and y directions, and 40 nm long in the z direction. This means that a 1x1x1 micron cube would be represented by a 250x250x25 voxel span. Annotations (such as synapses) are stored in voxel coordinates.\nNanometer coordinates are the coordinates of a point in the original image volume, but in nanometers. This is equivalent to the voxel coordinate multiplied by the voxel resolution, with no further transformation applied. Mesh and skeleton vertices are stored in nanometer coordinates.\nTransformed coordinates reflect a trasnsformation that has been applied to the original image volume. This transformation is a rotation to make the pia surface as flat as possible, a translation to move the pial surface to y=0, and a scaling to bring coordinates into microns. Transformed coordinates are convenient for more accurate computations of depth and the pia-to-white-matter axis, but are not stored by default. A python package standard_transform helps convert data to and from transformed coordinates.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that in all of these coordinate systems (including Neuroglancer), the y axis increases with depth. This is a standard definition when working with images, but is the opposite of what you usually think of with points. Because of that, when plotting annotations or neuroanatomy in matplotlib, you will usually have to invert the y axis with ax.invert_yaxis().",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Standard Transform: Coordinate System"
    ]
  },
  {
    "objectID": "quickstart_notebooks/08-standard-transform.html#standard-transform",
    "href": "quickstart_notebooks/08-standard-transform.html#standard-transform",
    "title": "Standard Transform: Coordinate System",
    "section": "Standard Transform",
    "text": "Standard Transform\nStandard Transform is a python package designed to convert voxel and nanometer coordinates to transformed coordinates, with particular emphasis on the MICrONs data.\n\nInstallation\nStandard Transform can be installed from pip: pip install standard_transform\n\n\nWhy use Standard Transform?\nLet’s look at the coordinates of every excitatory neuron in the MICrONs data to see why we might want to use transformed coordinates.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom caveclient import CAVEclient\n\nclient = CAVEclient('minnie65_public')\n\nct_df = client.materialize.query_table('aibs_metamodel_celltypes_v661', split_positions=True)\n\n# convert to nanometers\nct_df['pt_position_x_nm'] = ct_df['pt_position_x'] * 4\nct_df['pt_position_y_nm'] = ct_df['pt_position_y'] * 4\nct_df['pt_position_z_nm'] = ct_df['pt_position_z'] * 40\n\nfig, ax = plt.subplots(figsize=(5,4))\nsns.scatterplot(\n    x='pt_position_x_nm',\n    y='pt_position_y_nm',\n    s=3,\n    data=ct_df.sample(10_000), # Pick a random sample of 10,000 points, enough to see the shape of the data.\n    ax=ax,\n)\n\n# Important to flip the y axis to have y increase with depth!\nax.invert_yaxis()\nsns.despine(ax=ax)\n\n\n\n\n\n\n\n\nThis is just the raw positions, converted to nanometers. You can see a few aspects that might make this hard to work with.\nFirst, if you look along the top of the data, you can see that the pia surface is not flat. In fact, there’s a roughly 5 degree slope from the left to the right. Second, if you look at the location of the pia surface, it’s at around y = 0.4 * 10^6.\nNot only are these units large, the offset is arbitrary and it would make much more sense to anchor y=0 to the pial surface.\nLet’s see how we can do this with standard_transform.\n\nfrom standard_transform import minnie_ds\nimport numpy as np\n\nX_transformed = minnie_ds.transform_vx.apply_dataframe('pt_position', ct_df)\nX_transformed = np.array(X_transformed)\nct_df['pt_xt'] = X_transformed[:,0]\nct_df['pt_yt'] = X_transformed[:,1]\nct_df['pt_zt'] = X_transformed[:,2] \n\nfig, ax = plt.subplots(figsize=(5,4))\n\nsns.scatterplot(\n    x='pt_xt',\n    y='pt_yt',\n    s=3,\n    data=ct_df.sample(10_000),\n    ax=ax,\n)\n\nax.invert_yaxis()\nsns.despine(ax=ax)\n\n\n\n\n\n\n\n\nNow you can see that the surface is much more aligned with the x-axis, and the cells in layer 1 start just below y=0. In addition, the units have been converted to microns, which is much more readable and more consistent with measurements in other modalities.\n\n\nHow to use Standard Transform.\nA number of examples are available in the standard_transform readme.\n\nTransforming points\nThe main functions when working with point data are transform_vx and transform_nm, which transform from voxel coordinates and from nanometer coordinates respectively. In the above example, you will note that because we were working with annotations, we used the voxel coordinate transform.\nEach of the two transforms has the same functions that can be used:\n\nminnie_ds.{transform}.apply(x): This converts an Nx3 array of points from the original space to an Nx3 array of transformed coordinates.\nminnie_ds.{transform}.apply_project(projection_axis, x): This converts an Nx3 array of points from the original space to an N-length array of transformed coordinates, taking only values along the given axis (x, y, or z). This is typically done along the y axis to get the depth of each point.\nminnie_ds.{transform}.apply_dataframe(column_name, df, Optional[projection_axis]): This takes points from a dataframe column (or collection of three dataframe columns named {column_name}_x, {column_name}_y, and {column_name}_z) and converts them to transformed coordinates. If projection_axis is given, it will only return the values along that axis.\n\nWhere {transform} is either transform_vx or transform_nm.\nYou can also invert a transformation, for example if you want to convert transformed coordinates back to voxel coordinates to view in Neuroglancer.\n\nminnie_ds.{transform}.invert(X): This maps from an Nx3 array in the transformed space back to the original space (voxel or nanometer coordiantes, depending on which transform you used).\n\n\n\nTransforming meshes and skeletons\nStandard transform has the capability to transform MeshParty skeletons, MeshWorks, and MeshWork annotations using these same transforms as above.\nFor example, to transform all the vertices of meshwork object:\n\nnrn_transformed = minnie_ds.transform_nm.apply_meshwork_vertices(nrn)\n\nHowever, Meshwork objects also have annotations in dataframes as well as vertices. In order to transform these points, we need to specify both the name of the dataframe table and the columns to transform as a dictionary. For example, if you want to also remap the ctr_pt_position values from the pre_syn table and post_syn tables (which are in voxels by default), you would use:\n\nanno_dict={'pre_syn': 'ctr_pt_position', 'post_syn': 'ctr_pt_position'}\nnrn_transformed = minnie_ds.transform_vx.apply_meshwork_annotations(nrn_transformed, anno_dict, inplace=True)\n\nNote that without the inplace=True parameter, these meshwork transformations will return a new object, while ifinplace=True the original object is modified.",
    "crumbs": [
      "Introduction",
      "Python Tools",
      "Standard Transform: Coordinate System"
    ]
  },
  {
    "objectID": "release_manifests/version-1078.html",
    "href": "release_manifests/version-1078.html",
    "title": "1078 (Jun 2024)",
    "section": "",
    "text": "Data Release v1078 (June 5, 2024) is the second quarterly release of 2024.\nThis version ADDS the following tables\nIt includes all other tables as of v661, with the following exceptions (removed tables still available in version 1181):",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1078 (Jun 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-1078.html#vortex-efforts",
    "href": "release_manifests/version-1078.html#vortex-efforts",
    "title": "1078 (Jun 2024)",
    "section": "VORTEX Efforts",
    "text": "VORTEX Efforts\n\nProofreading Status and Strategy\nTable name: proofreading_status_and_strategy\nThe table proofreading_status_and_strategy describes the status of cells that have undergone manual proofreading.\nBecause of the inherent difference in the difficulty and time required for different kinds of proofreading, we describe the status of axons and dendrites separately.\nEach compartment status may be either:\n\nFALSE: indicates no comprehensive proofreading has been performed, or is not applicable.\n\nTRUE: indicates that false merges have been comprehensively removed, and the compartment is at least ‘clean’. Consult the strategy column if completeness of the compartment is relevant to your research.\n\nAn axon or dendrite labeled as status=TRUE can be trusted to be correct, but may not be complete. The degree of completion can be read from the strategy column. For more information, please see Proofreading and Data Quality; or also the microns-explorer page on proofreading strategies.\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made. NOTE: if this does not match the pt_root_id then the cell has undergone further changes. This is usually and improvement in proofreading, but proceed with caution.\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. May be TRUE or FALSE\n\n\nstatus_axon\nThe status of the axon proofreading. May be TRUE or FALSE\n\n\nstrategy_dendrite\nThe strategy employed to proofread the dendrite. See strategy table below for details\n\n\nstrategy_axon\nThe strategy employed to proofread the axon. See strategy table below for details\n\n\n\nThe specific strategies are as follows (and will update over time):\n\nProofreading Strategies\n\nProofreading Strategy Table\n\n\n\n\n\n\nStrategy\nDescription\n\n\n\n\nnone\nNo cleaning, and no extension. Indicates an entry in proofreading_status that is FALSE for that compartment\n\n\ndendrite_clean\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. The dendrite may be incorrectly truncated by segmentation error. Not all dendrite tips have been checked for extension. No comprehensive attempt was made to re-attach spine heads.\n\n\ndendrite_extended\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. Every tip was identified, manually inspected, and extended if possible. No comprehensive attempt was made to re-attach spine heads.\n\n\naxon_column_truncated\nAThe axon was extended within the V1 cortical column, with a preference for local connections. In some cases the axon was cut at the column boundary and/or the layer boundary, especially the boundary between layers 2/3 and layer 4. Output synapses represent a sampling of potential partners\n\n\naxon_interareal\nThe axon was extended with a preference for branches that projected to other brain areas. Some axon branches were fully extended, but local connections may be incomplete. Output synapses represent a sampling of potential partners.\n\n\naxon_partially_extended\nThe axon was extended outward from the soma, following each branch to its termination. Output synapses represent a sampling of potential partners.\n\n\naxon_fully_extended\nAxon was extended outward from the soma, following each branch to its termination. After initial extension, every endpoint was identified, manually inspected, and extended again if possible. Output synapses represent a largely complete sampling of partners.\n\n\n\nThis table, proofreading_status_and_strategy, supercedes proofreading_status_public_release.\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()\n\n\n\nVORTEX proofread astrocytes\nTable name: vortex_astrocyte_proofreading_status\nManual screening and proofreading of astrocytes as part of the as part of the VORTEX project.\nThe key columns are:\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nstatus\nOne of ‘non’ indicating no cleaning or extension; ‘clean’ indicating neurite and non-astrocytic elements have been manually removed; ‘extended’ meaning astrocytic processes within the volume of the astrocyte have been comprehensively reviewed and assigned to the most likely cell\n\n\nvalid_id\nThe root id of the astrocyte when it the proofreading assessment was made\n\n\n\n# Standard query\nclient.materialize.query_table('vortex_astrocyte_proofreading_status')\n\n# Content-aware query\nclient.materialize.tables.vortex_astrocyte_proofreading_status(status='clean').query()\n\n\nVORTEX synapse compartments\nTable name: vortex_compartment_targets\nManual labeling of synaptic targets of putative parvalbumin basket cells, and inputs to pyramidal cells as part of the VORTEX project. This is a reference table on the synapses table synapses_pni_2, and provided ground truth for the automatic classifier behind synapse_target_predictions_ssa.\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nreference id of the synapse in synapses_pni_2\n\n\ntag\nManual label of synapse type. One of: ‘spine’, ‘shaft’, ‘soma’, ‘soma_spine’, ‘orphan’, ‘other’. Orphan refers to orphan neurites; other is a catch-all for synapses that do not match other labels\n\n\npre_pt_position  pre_pt_supervoxel_id  pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse\n\n\npost_pt_position  post_pt_supervoxel_id  post_pt_root_id\nThe bound spatial point data for the postsynaptic side of the synapse\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on synapses_pni_2, and can be indexed by the same synapse id.\n\n\n# Standard query\nclient.materialize.query_table('vortex_compartment_targets', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_compartment_targets(pre_pt_root_id=example_root_id).query()\n\n\nVORTEX manual myelin\nTable name: vortex_manual_myelination_v0\nManual labeling of myelin at 1 um resolution along axons, as part of the VORTEX project. This table was created in parallel to the node of Ranvier labeling, vortex_manual_nodes_of_ranvier, and used for ground truth of the myelin_auto_tags myelination classifier.\n\nManual Myelination\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npt_position\nThe position along the axon that was evaluated for myelin. Note that this position is not always on the segmentation of the axon\n\n\nvalid_id\nThe segmentation root_id associated with this assessment is the valid_id, NOT necessarily the pt_root_id associated with the point\n\n\ntag\nMyelination status, either True or False\n\n\npt_supervoxel_id  pt_root_id\nThe bound spatial point data for the evaluated point. This may not match the valid_id due to proofreading, or spatial downsampling when evaluating the points.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe myelination was evaluated for the valid_id and not necessarily the pt_root_id.\n\n\n# Standard query\nclient.materialize.query_table('vortex_manual_myelination_v0', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_manual_myelination_v0(valid_id=example_root_id).query()\n\n\nVORTEX nodes of Ranvier\nTable name: vortex_manual_nodes_of_ranvier\nThis table annotates manually identified nodes of Ranvier on the axon of a subset of cells. Each point listed was inspected and manually labeled for the presence of myelin; nodes were suggested where myelination disappeared and reappeared within 5 um. Labeling performed as part of the VORTEX project, and created in parallel to the myelin labeling, listed below as vortex_manual_myelination_v0.\n\nManual Myelination\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npt_position\nThe position along the axon that was evaluated for myelin. Note that this position is not always on the segmentation of the axon\n\n\nvalid_id\nThe segmentation root_id associated with this assessment is the valid_id, NOT necessarily the pt_root_id associated with the point\n\n\ntag\npresence of a node of Ranvier, labeled as node\n\n\npt_supervoxel_id  pt_root_id\nThe bound spatial point data for the evaluated point. This may not match the valid_id due to proofreading, or spatial downsampling when evaluating the points.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe myelination was evaluated for the valid_id and not necessarily the pt_root_id.\n\n\n# Standard query\nclient.materialize.query_table('vortex_manual_nodes_of_ranvier')\n\n# Content-aware query\nclient.materialize.tables.vortex_manual_nodes_of_ranvier(valid_id=example_root_id).query()\nFor more on how to interpret the table, see Annotation Tables",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1078 (Jun 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-1078.html#functional-area-nucleus-look-up",
    "href": "release_manifests/version-1078.html#functional-area-nucleus-look-up",
    "title": "1078 (Jun 2024)",
    "section": "Functional area nucleus look-up",
    "text": "Functional area nucleus look-up\nTable name: nucleus_functional_area_assignment\nGiven the nucleus detection table nucleus_detection_v0 and the transformation of 2-photon in vivo imaging to the EM structural space (see functional-coregistration-tables ), each cell in the volume has been assigned to one of four visual cortical areas.\nThe inferred functional brain area based on the position of the nucleus in the EM volume. Area boundaries estimated from the area-membership assignments of the 2P recorded cells, after transformation to EM space.\nThe table nucleus_functional_area_assignment is a reference table on nucleus_detection_v0 and adds the following columns:\n\nFunctional area assignment\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the nucleus\n\n\ntag\nthe brain area label (one of V1, AL, RL, LM)\n\n\nvalue\nthe distance to the area boundary (in um), calculated as the mean-distance of the 10 nearest neighbors in a non-matching brain area. A larger value is further from the area boundaries, and can be interpretted as higher confidence in area assignment.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('nucleus_functional_area_assignment')\n\n# Content-aware query\nclient.materialize.tables.nucleus_functional_area_assignment(tag='V1').query()\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1078 (Jun 2024)"
    ]
  },
  {
    "objectID": "release_manifests/version-1300.html",
    "href": "release_manifests/version-1300.html",
    "title": "1300 (Jan 2025)",
    "section": "",
    "text": "Data Release v1300 (January 13, 2025) is the first quarterly release of 2025.\nThis version ADDS the following tables\nIt includes all other tables as of v1181, with the following exceptions (removed tables still available in version 1181):\nThe following tables have updated status or entries, from VORTEX proofreading and annotations",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1300 (Jan 2025)"
    ]
  },
  {
    "objectID": "release_manifests/version-1300.html#synapse-target-classifier",
    "href": "release_manifests/version-1300.html#synapse-target-classifier",
    "title": "1300 (Jan 2025)",
    "section": "Synapse-target classifier",
    "text": "Synapse-target classifier\nTable name: synapse_target_predictions_ssa\nThis table contains synapse target structure predictions (‘soma’, ‘shaft’, ‘spine’) for synapses in synapses_pni_2. Predictions are currently available for the majority of cells predicted to be neurons by aibs_metamodel_mtypes_v661_v2. The predictions come from a model trained on synapse target annotations from vortex_compartment_targets as well as some additional manual annotations where the model was failing.\nThe model is a random forest trained on spectral shape analysis features from the mesh. Current failure modes include some stubby spines being predicted as shafts, and some very thin dendrites being classified as spines.\nIt contains the following columns:\n\nSynapse target predictions column definitions\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npre_pt_position / pre_pt_supervoxel_id / pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse.\n\n\npost_pt_position / post_pt_supervoxel_id / post_pt_root_id\nIThe bound spatial point data for the postsynaptic side of the synapse.\n\n\nsize\nThe size of the synapse in voxels. This correlates well, but not perfectly, with the surface area of synapse.\n\n\nctr_pt_position\nA position in the center of the detected synaptic junction. Of all points in the synapse table, this is usually the closest point to the surface (and thus mesh) of both neurons. Because it is at the edge of cells, it is not associated with a root id.\n\n\ntag\nlabel of the post-synaptic structure, one of: spine, shaft, soma\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on synapses_pni_2, and can be indexed by the same synapse id.\n\n\n# Standard query\nclient.materialize.query_table('synapse_target_predictions_ssa', limit=10)\n\n# Content-aware query\nclient.materialize.tables.synapse_target_predictions_ssa(post_pt_root_id=example_root_id).query()\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1300 (Jan 2025)"
    ]
  },
  {
    "objectID": "release_manifests/version-1300.html#automated-coregistration",
    "href": "release_manifests/version-1300.html#automated-coregistration",
    "title": "1300 (Jan 2025)",
    "section": "Automated Coregistration",
    "text": "Automated Coregistration\nTable name: coregistration_auto_phase3_fwd_apl_vess_combined_v2\nA table of EM nucleus centroids automatically matched to Baylor functional units. This table reconciles the following two tables that both make a best match of the of registration using different techniques: coregistration_auto_phase3_fwd_v2 and apl_functional_coreg_vess_fwd. A unique functional unit is identified by its session, scan_idx and unit_id. An EM nucleus centroid may have matched to more than one functional unit if it was scanned on more than one imaging field.\nThe key columns are:\n\nCoregistration table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nsession\nThe session index from functional imaging\n\n\nscan_idx\nThe scan index from functional imaging\n\n\nunit_id\nThe functional unit index from imaging. Only unique within scan and session\n\n\nfield\nThe field index from functional imaging\n\n\nresidual\nThe residual distance between the functional and the assigned structural points after transformation, in microns\n\n\nscore\nA separation score, measuring the difference between the residual distance to the assigned neuron and the distance to the nearest non-assigned neuron, in microns. This can be negative if the non-assigned neuron is closer than the assigned neuron. Larger values indicate fewer nearby neurons that could be confused with the assigned neuron.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis table includes duplicate entries for the same pt_root_id and nucleus id if the coregistered cell has multiple unit recordings\n\n\n# Standard query\nclient.materialize.query_table('coregistration_auto_phase3_fwd_apl_vess_combined_v2')\n\n# Content-aware query\nclient.materialize.tables.coregistration_auto_phase3_fwd_apl_vess_combined_v2(id=example_nucleus_id).query()\nThis table coregistration_auto_phase3_fwd_apl_vess_combined_v2 supercedes previous iterations of this table:\n\ncoregistration_auto_phase3_fwd_apl_vess_combined_v2\napl_functional_coreg_forward_v5\n\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1300 (Jan 2025)"
    ]
  },
  {
    "objectID": "release_manifests/version-1300.html#publication-gamlin-et-al.",
    "href": "release_manifests/version-1300.html#publication-gamlin-et-al.",
    "title": "1300 (Jan 2025)",
    "section": "Publication: Gamlin et al.",
    "text": "Publication: Gamlin et al.\n\nMartinotti cells\nTable name: gamlin_2023_mcs\nA table containing the manually selected Martinotti cells (MC) included in (Gamlin et al. 2025).\nThe key columns are:\n\nGamlin et al. Martinotti cells\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nclassification-system\nallen_cortex_inhibitory.\n\n\ncell_type\nMartinotti\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('gamlin_2023_mcs')\n\n# Content-aware query\nclient.materialize.tables.gamlin_2023_mcs(id=example_nucleus_id).query()\n\n\nMartinotti cell synaptic targets\nTable name: cg_cell_type_calls\nManual cell type calls on the targets of L5 Martinotti cells (MC) in gamlin_2023_mcs, annotated for (Gamlin et al. 2025).\nIt contains the following columns:\n\nGamlin et al. Martinotti targets\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npre_pt_position / pre_pt_supervoxel_id / pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse.\n\n\npost_pt_position / post_pt_supervoxel_id / post_pt_root_id\nIThe bound spatial point data for the postsynaptic side of the synapse.\n\n\nsize\nThe size of the synapse in voxels. This correlates well, but not perfectly, with the surface area of synapse.\n\n\nctr_pt_position\nA position in the center of the detected synaptic junction. Of all points in the synapse table, this is usually the closest point to the surface (and thus mesh) of both neurons. Because it is at the edge of cells, it is not associated with a root id.\n\n\nclassification_system\ncg_calls\n\n\ncell_type\nThe excitatory subtype or inhibitory type of the target. One of: ‘1P’, ‘23P’, ‘4P’, ‘5P_IT’, ‘5P_PT’, ‘5P_NP’, ‘6P’, ‘INH’\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on synapses_pni_2, and can be indexed by the same synapse id.\n\n\n# Standard query\nclient.materialize.query_table('cg_cell_type_calls')\n\n# Content-aware query\nclient.materialize.tables.cg_cell_type_calls(pre_pt_root_id=example_root_id).query()\n\n\nMartinotti cell MET-types\nTable name: gamlin_2023_mcs_met_types\nA table matching Martinotti cells (MC) included in (Gamlin et al. 2025) to link the EM and MET-types (morphology, electrophysiology, transcriptomic cell types).\nThe key columns are:\n\nGamlin et al. Martinotti MET types\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\ntag\npredicted MET-type label\n\n\nvalue\nprediction reliability from the MET-type classifier, specifically the fraction of times that the cell was classified under that MET-type label in all the folds of the cross-validation\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on nucleus_detection_v0, and can be indexed by the same nucleus id.\n\n\n# Standard query\nclient.materialize.query_table('gamlin_2023_mcs_met_types')\n\n# Content-aware query\nclient.materialize.tables.gamlin_2023_mcs_met_types(id=example_nucleus_id).query()\nFor more on how to interpret the table, see Annotation Tables.",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1300 (Jan 2025)"
    ]
  },
  {
    "objectID": "release_manifests/version-1300.html#vortex-efforts",
    "href": "release_manifests/version-1300.html#vortex-efforts",
    "title": "1300 (Jan 2025)",
    "section": "VORTEX Efforts",
    "text": "VORTEX Efforts\n\nProofreading Status and Strategy\nTable name: proofreading_status_and_strategy\nThe table proofreading_status_and_strategy describes the status of cells that have undergone manual proofreading.\nBecause of the inherent difference in the difficulty and time required for different kinds of proofreading, we describe the status of axons and dendrites separately.\nEach compartment status may be either:\n\nFALSE: indicates no comprehensive proofreading has been performed, or is not applicable.\n\nTRUE: indicates that false merges have been comprehensively removed, and the compartment is at least ‘clean’. Consult the strategy column if completeness of the compartment is relevant to your research.\n\nAn axon or dendrite labeled as status=TRUE can be trusted to be correct, but may not be complete. The degree of completion can be read from the strategy column. For more information, please see Proofreading and Data Quality; or also the microns-explorer page on proofreading strategies.\nThe key columns are:\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nSoma ID for the cell\n\n\npt_position  pt_supervoxel_id  pt_root_id\nBound spatial point columns associated with the centroid of the cell nucleus\n\n\nvalid_id\nThe root id of the neuron when it the proofreading assessment was made. NOTE: if this does not match the pt_root_id then the cell has undergone further changes. This is usually and improvement in proofreading, but proceed with caution.\n\n\nstatus_dendrite\nThe status of the dendrite proofreading. May be TRUE or FALSE\n\n\nstatus_axon\nThe status of the axon proofreading. May be TRUE or FALSE\n\n\nstrategy_dendrite\nThe strategy employed to proofread the dendrite. See strategy table below for details\n\n\nstrategy_axon\nThe strategy employed to proofread the axon. See strategy table below for details\n\n\n\nThe specific strategies are as follows (and will update over time):\n\nProofreading Strategies\n\nProofreading Strategy Table\n\n\n\n\n\n\nStrategy\nDescription\n\n\n\n\nnone\nNo cleaning, and no extension. Indicates an entry in proofreading_status that is FALSE for that compartment\n\n\ndendrite_clean\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. The dendrite may be incorrectly truncated by segmentation error. Not all dendrite tips have been checked for extension. No comprehensive attempt was made to re-attach spine heads.\n\n\ndendrite_extended\nThe dendrite had incorrectly-merged axon and dendritic segments comprehensively removed, meaning the input synapses are accurate. Every tip was identified, manually inspected, and extended if possible. No comprehensive attempt was made to re-attach spine heads.\n\n\naxon_column_truncated\nAThe axon was extended within the V1 cortical column, with a preference for local connections. In some cases the axon was cut at the column boundary and/or the layer boundary, especially the boundary between layers 2/3 and layer 4. Output synapses represent a sampling of potential partners\n\n\naxon_interareal\nThe axon was extended with a preference for branches that projected to other brain areas. Some axon branches were fully extended, but local connections may be incomplete. Output synapses represent a sampling of potential partners.\n\n\naxon_partially_extended\nThe axon was extended outward from the soma, following each branch to its termination. Output synapses represent a sampling of potential partners.\n\n\naxon_fully_extended\nAxon was extended outward from the soma, following each branch to its termination. After initial extension, every endpoint was identified, manually inspected, and extended again if possible. Output synapses represent a largely complete sampling of partners.\n\n\n\nThis table, proofreading_status_and_strategy, supercedes proofreading_status_public_release.\n# Standard query\nclient.materialize.query_table('proofreading_status_and_strategy')\n\n# Content-aware query\nclient.materialize.tables.proofreading_status_and_strategy(status_axon='t').query()\n\n\n\nVORTEX synapse compartments\nTable name: vortex_compartment_targets\nManual labeling of synaptic targets of putative parvalbumin basket cells, and inputs to pyramidal cells as part of the VORTEX project. This is a reference table on the synapses table synapses_pni_2, and provided ground truth for the automatic classifier behind synapse_target_predictions_ssa.\n\nProofreading Status Table\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntarget_id\nreference id of the synapse in synapses_pni_2\n\n\ntag\nManual label of synapse type. One of: ‘spine’, ‘shaft’, ‘soma’, ‘soma_spine’, ‘orphan’, ‘other’. Orphan refers to orphan neurites; other is a catch-all for synapses that do not match other labels\n\n\npre_pt_position  pre_pt_supervoxel_id  pre_pt_root_id\nThe bound spatial point data for the presynaptic side of the synapse\n\n\npost_pt_position  post_pt_supervoxel_id  post_pt_root_id\nThe bound spatial point data for the postsynaptic side of the synapse\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a reference table on synapses_pni_2, and can be indexed by the same synapse id.\n\n\n# Standard query\nclient.materialize.query_table('vortex_compartment_targets', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_compartment_targets(pre_pt_root_id=example_root_id).query()\n\n\nVORTEX manual myelin\nTable name: vortex_manual_myelination_v0\nManual labeling of myelin at 1 um resolution along axons, as part of the VORTEX project. This table was created in parallel to the node of Ranvier labeling, vortex_manual_nodes_of_ranvier, and used for ground truth of the myelin_auto_tags myelination classifier.\n\nManual Myelination\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npt_position\nThe position along the axon that was evaluated for myelin. Note that this position is not always on the segmentation of the axon\n\n\nvalid_id\nThe segmentation root_id associated with this assessment is the valid_id, NOT necessarily the pt_root_id associated with the point\n\n\ntag\nMyelination status, either True or False\n\n\npt_supervoxel_id  pt_root_id\nThe bound spatial point data for the evaluated point. This may not match the valid_id due to proofreading, or spatial downsampling when evaluating the points.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe myelination was evaluated for the valid_id and not necessarily the pt_root_id.\n\n\n# Standard query\nclient.materialize.query_table('vortex_manual_myelination_v0', limit=10)\n\n# Content-aware query\nclient.materialize.tables.vortex_manual_myelination_v0(valid_id=example_root_id).query()\nFor more on how to interpret the table, see Annotation Tables",
    "crumbs": [
      "Introduction",
      "Data Inventory",
      "Release Manifests",
      "1300 (Jan 2025)"
    ]
  }
]